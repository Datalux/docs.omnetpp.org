{
    "docs": [
        {
            "location": "/", 
            "text": "OMNeT++ Documentation\n\n\nOMNeT++ Tutorials\n\n\n\n\nTicToc\n. An introductory tutorial that guides you through building and working with\n  an example simulation model.\n\n\nResult Analysis with Python\n. This tutorial will walk you through the initial steps of\n  using Python for analysing simulation results, and shows how to do some of the most common tasks.\n\n\nRunning Simulation Campaigns in the Cloud\n. Concepts, ideas and a solution draft\n  for harnessing the power of computing clouds for running simulation campaigns.\n\n\nRunning INET Simulation Campaigns on AWS\n. Presents a minimal but powerful toolset \n  for running INET simulations on Amazon's cloud platform.", 
            "title": "Overview"
        }, 
        {
            "location": "/#omnet-documentation", 
            "text": "", 
            "title": "OMNeT++ Documentation"
        }, 
        {
            "location": "/#omnet-tutorials", 
            "text": "TicToc . An introductory tutorial that guides you through building and working with\n  an example simulation model.  Result Analysis with Python . This tutorial will walk you through the initial steps of\n  using Python for analysing simulation results, and shows how to do some of the most common tasks.  Running Simulation Campaigns in the Cloud . Concepts, ideas and a solution draft\n  for harnessing the power of computing clouds for running simulation campaigns.  Running INET Simulation Campaigns on AWS . Presents a minimal but powerful toolset \n  for running INET simulations on Amazon's cloud platform.", 
            "title": "OMNeT++ Tutorials"
        }, 
        {
            "location": "/tutorials/tictoc/", 
            "text": "Introduction\n\n\nThis tutorial guides you through building and working\nwith an example simulation model, showing you along the way some\nof the commonly used OMNeT++ features.\n\n\nThe tutorial is based on the Tictoc example simulation, which you\ncan find in the \nsamples/tictoc\n directory of your\nOMNeT++ installation, so you can try out immediately how\nthe examples work. However, you'll find the tutorial much more useful\nif you actually carry out the steps described here.\nWe assume that you have a good C++ knowledge, and you are in general\nfamiliar with C/C++ development (editing source files, compiling, debugging etc.)\nTo make the examples easier to follow, all source code in here is\ncross-linked to the OMNeT++ API documentation.\n\n\nThis document and the TicToc model are an expanded version of\nthe original TicToc tutorial from Ahmet Sekercioglu (Monash University).", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/tictoc/#introduction", 
            "text": "This tutorial guides you through building and working\nwith an example simulation model, showing you along the way some\nof the commonly used OMNeT++ features.  The tutorial is based on the Tictoc example simulation, which you\ncan find in the  samples/tictoc  directory of your\nOMNeT++ installation, so you can try out immediately how\nthe examples work. However, you'll find the tutorial much more useful\nif you actually carry out the steps described here.\nWe assume that you have a good C++ knowledge, and you are in general\nfamiliar with C/C++ development (editing source files, compiling, debugging etc.)\nTo make the examples easier to follow, all source code in here is\ncross-linked to the OMNeT++ API documentation.  This document and the TicToc model are an expanded version of\nthe original TicToc tutorial from Ahmet Sekercioglu (Monash University).", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/tictoc/part1/", 
            "text": "Part 1 - Getting Started\n\n\n1.1 The model\n\n\nFor a start, let us begin with a \"network\" that consists of two nodes.\nThe nodes will do something simple: one of the nodes will create a packet,\nand the two nodes will keep passing the same packet back and forth.\nWe'll call the nodes \ntic\n and \ntoc\n. Later we'll gradually\nimprove this model, introducing OMNeT++ features at each step.\n\n\nHere are the steps you take to implement your first simulation from scratch.\n\n\n1.2 Setting up the project\n\n\nStart the OMNeT++ IDE by typing \nomnetpp\n in your terminal. (We assume\nthat you already have a working OMNeT++ installation. If not, please install the latest\nversion, consulting the \nInstallation Guide\n as needed.)\nOnce in the IDE, choose \nNew -\n OMNeT++ Project\n from the menu.\n\n\n\n\nA wizard dialog will appear. Enter \ntictoc\n as project name,\nchoose \nEmpty project\n when asked about the initial content of the project,\nthen click \nFinish\n. An empty project will be created, as you can see\nin the \nProject Explorer\n.\n(Note: Some OMNeT++ versions will generate a \npackage.ned\n file into the project.\nWe don't need it now: delete the file by selecting it and hitting Delete.)\n\n\nThe project will hold all files that belong to our simulation. In our example,\nthe project consists of a single directory. For larger simulations, the project's\ncontents are usually sorted into \nsrc/\n and \nsimulations/\n folders,\nand possibly subfolders underneath them.\n\n\n\n\nNote\n\n\nUsing the IDE is entirely optional. Almost all functionality of OMNeT++\n(except for some very graphics-intensive and interactive features\nlike sequence chart browsing and result plotting) is available on\nthe command line. Model source files can be edited with any text editor,\nand OMNeT++ provides command-line tools for special tasks such as makefile\ncreation, message file to C++ translation, result file querying and data export,\nand so on. To proceed without the IDE, simply create a directory and create the\nfollowing NED, C++ and ini files in it with your favorite text editor.\n\n\n\n\n1.3 Adding the NED file\n\n\nOMNeT++ uses NED files to define components and to assemble them into larger units\nlike networks. We start implementing our model by adding a NED file.\nTo add the file to the project, right-click the project directory in the\n\nProject Explorer\n panel on the left, and choose \nNew -\n Network Description File (NED)\n\nfrom the menu. Enter \ntictoc1.ned\n when prompted for the file name.\n\n\nOnce created, the file can be edited in the \nEditor area\n of the OMNeT++ IDE.\nThe OMNeT++ IDE's NED editor has two modes, \nDesign\n and \nSource\n; one can switch\nbetween them using the tabs at the bottom of the editor. In \nDesign\n mode,\nthe topology can be edited graphically, using the mouse and the palette on the right.\nIn \nSource\n mode, the NED source code can be directly edited as text.\nChanges done in one mode will be immediately reflected in the other, so you can\nfreely switch between modes during editing, and do each change in whichever mode\nit is more convenient. (Since NED files are plain text files, you can even use\nan external text editor to edit them, although you'll miss syntax highlighting,\ncontent assist, cross-references and other IDE features.)\n\n\nSwitch into \nSource\n mode, and enter the following:\n\n\n\n\n\nWhen you're done, switch back to \nDesign\n mode. You should see something like this:\n\n\n\n\nThe first block in the file declares \nTxc1\n as a simple module type.\nSimple modules are atomic on NED level. They are also active components,\nand their behavior is implemented in C++. The declaration also says that\n\nTxc1\n has an input gate named \nin\n, and an output gate named \nout\n.\n\n\nThe second block declares \nTictoc1\n as a network. \nTictoc1\n is assembled from two\nsubmodules, \ntic\n and \ntoc\n, both instances of the module type \nTxc1\n.\n\ntic\n's output gate is connected to \ntoc\n's input gate, and vica versa.\nThere will be a 100ms propagation delay both ways.\n\n\n\n\nNote\n\n\nYou can find a detailed description of the NED language in the\n\nOMNeT++ Simulation Manual\n.\n(The manual can also be found in the \ndoc\n  directory of your OMNeT++ installation.)\n\n\n\n\n1.4 Adding the C++ files\n\n\nWe now need to implement the functionality of the Txc1 simple module in C++.\nCreate a file named \ntxc1.cc\n by choosing \nNew -\n Source File\n from the\nproject's context menu (or \nFile -\n New -\n File\n from the IDE's main menu),\nand enter the following content:\n\n\n\n\n\nThe \nTxc1\n simple module type is represented by the C++ class \nTxc1\n. The \nTxc1\n\nclass needs to subclass from OMNeT++'s \ncSimpleModule\n class, and needs to be\nregistered in OMNeT++ with the \nDefine_Module()\n macro.\n\n\n\n\nNote\n\n\nIt is a common mistake to forget the \nDefine_Module()\n line. If it is missing,\nyou'll get an error message similar to this one: \nError: Class \nTxc1\n not found -- perhapsits code was not linked in, or the class wasn\nt registered with Register_Class(), or inthe case of modules and channels, with Define_Module()/Define_Channel()\n.\n\n\n\n\nWe redefine two methods from \ncSimpleModule\n: \ninitialize()\n and \nhandleMessage()\n.\nThey are invoked from the simulation kernel: the first one only once, and\nthe second one whenever a message arrives at the module.\n\n\nIn \ninitialize()\n we create a message object (\ncMessage\n), and send it out\non gate \nout\n. Since this gate is connected to the other module's\ninput gate, the simulation kernel will deliver this message to the other module\nin the argument to \nhandleMessage()\n -- after a 100ms propagation delay\nassigned to the link in the NED file. The other module just sends it back\n(another 100ms delay), so it will result in a continuous ping-pong.\n\n\nMessages (packets, frames, jobs, etc) and events (timers, timeouts) are\nall represented by cMessage objects (or its subclasses) in OMNeT++.\nAfter you send or schedule them, they will be held by the simulation\nkernel in the \"scheduled events\" or \"future events\" list until\ntheir time comes and they are delivered to the modules via \nhandleMessage()\n.\n\n\nNote that there is no stopping condition built into this simulation:\nit would continue forever. You will be able to stop it from the GUI.\n(You could also specify a simulation time limit or CPU time limit\nin the configuration file, but we don't do that in the tutorial.)\n\n\n1.5  Adding omnetpp.ini\n\n\nTo be able to run the simulation, we need to create an \nomnetpp.ini\n file.\n\nomnetpp.ini\n tells the simulation program which network you want to simulate\n(as NED files may contain several networks), you can pass parameters\nto the model, explicitly specify seeds for the random number generators, etc.\n\n\nCreate an \nomnetpp.ini\n file using the \nFile -\n New -\n Initialization file (INI)\n\nmenu item. The new file will open in an \nInifile Editor\n.\nAs the NED Editor, the Inifile Editor also has two modes, \nForm\n and \nSource\n,\nwhich edit the same content. The former is more suitable for configuring the\nsimulation kernel, and the latter for entering module parameters.\n\n\nFor now, just switch to \nSource\n mode and enter the following:\n\n\n\n[General]\nnetwork = Tictoc1\n\n\n\n\nYou can verify the result in \nForm\n mode:\n\n\n\n\ntictoc2\n and further steps will all share a common \n file.\n\n\nWe are now done with creating the first model, and ready to compile and run it.\n\n\nSources: \n, \n,", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#part-1-getting-started", 
            "text": "", 
            "title": "Part 1 - Getting Started"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#11-the-model", 
            "text": "For a start, let us begin with a \"network\" that consists of two nodes.\nThe nodes will do something simple: one of the nodes will create a packet,\nand the two nodes will keep passing the same packet back and forth.\nWe'll call the nodes  tic  and  toc . Later we'll gradually\nimprove this model, introducing OMNeT++ features at each step.  Here are the steps you take to implement your first simulation from scratch.", 
            "title": "1.1 The model"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#12-setting-up-the-project", 
            "text": "Start the OMNeT++ IDE by typing  omnetpp  in your terminal. (We assume\nthat you already have a working OMNeT++ installation. If not, please install the latest\nversion, consulting the  Installation Guide  as needed.)\nOnce in the IDE, choose  New -  OMNeT++ Project  from the menu.   A wizard dialog will appear. Enter  tictoc  as project name,\nchoose  Empty project  when asked about the initial content of the project,\nthen click  Finish . An empty project will be created, as you can see\nin the  Project Explorer .\n(Note: Some OMNeT++ versions will generate a  package.ned  file into the project.\nWe don't need it now: delete the file by selecting it and hitting Delete.)  The project will hold all files that belong to our simulation. In our example,\nthe project consists of a single directory. For larger simulations, the project's\ncontents are usually sorted into  src/  and  simulations/  folders,\nand possibly subfolders underneath them.   Note  Using the IDE is entirely optional. Almost all functionality of OMNeT++\n(except for some very graphics-intensive and interactive features\nlike sequence chart browsing and result plotting) is available on\nthe command line. Model source files can be edited with any text editor,\nand OMNeT++ provides command-line tools for special tasks such as makefile\ncreation, message file to C++ translation, result file querying and data export,\nand so on. To proceed without the IDE, simply create a directory and create the\nfollowing NED, C++ and ini files in it with your favorite text editor.", 
            "title": "1.2 Setting up the project"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#13-adding-the-ned-file", 
            "text": "OMNeT++ uses NED files to define components and to assemble them into larger units\nlike networks. We start implementing our model by adding a NED file.\nTo add the file to the project, right-click the project directory in the Project Explorer  panel on the left, and choose  New -  Network Description File (NED) \nfrom the menu. Enter  tictoc1.ned  when prompted for the file name.  Once created, the file can be edited in the  Editor area  of the OMNeT++ IDE.\nThe OMNeT++ IDE's NED editor has two modes,  Design  and  Source ; one can switch\nbetween them using the tabs at the bottom of the editor. In  Design  mode,\nthe topology can be edited graphically, using the mouse and the palette on the right.\nIn  Source  mode, the NED source code can be directly edited as text.\nChanges done in one mode will be immediately reflected in the other, so you can\nfreely switch between modes during editing, and do each change in whichever mode\nit is more convenient. (Since NED files are plain text files, you can even use\nan external text editor to edit them, although you'll miss syntax highlighting,\ncontent assist, cross-references and other IDE features.)  Switch into  Source  mode, and enter the following:   When you're done, switch back to  Design  mode. You should see something like this:   The first block in the file declares  Txc1  as a simple module type.\nSimple modules are atomic on NED level. They are also active components,\nand their behavior is implemented in C++. The declaration also says that Txc1  has an input gate named  in , and an output gate named  out .  The second block declares  Tictoc1  as a network.  Tictoc1  is assembled from two\nsubmodules,  tic  and  toc , both instances of the module type  Txc1 . tic 's output gate is connected to  toc 's input gate, and vica versa.\nThere will be a 100ms propagation delay both ways.   Note  You can find a detailed description of the NED language in the OMNeT++ Simulation Manual .\n(The manual can also be found in the  doc   directory of your OMNeT++ installation.)", 
            "title": "1.3 Adding the NED file"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#14-adding-the-c-files", 
            "text": "We now need to implement the functionality of the Txc1 simple module in C++.\nCreate a file named  txc1.cc  by choosing  New -  Source File  from the\nproject's context menu (or  File -  New -  File  from the IDE's main menu),\nand enter the following content:   The  Txc1  simple module type is represented by the C++ class  Txc1 . The  Txc1 \nclass needs to subclass from OMNeT++'s  cSimpleModule  class, and needs to be\nregistered in OMNeT++ with the  Define_Module()  macro.   Note  It is a common mistake to forget the  Define_Module()  line. If it is missing,\nyou'll get an error message similar to this one:  Error: Class  Txc1  not found -- perhapsits code was not linked in, or the class wasn t registered with Register_Class(), or inthe case of modules and channels, with Define_Module()/Define_Channel() .   We redefine two methods from  cSimpleModule :  initialize()  and  handleMessage() .\nThey are invoked from the simulation kernel: the first one only once, and\nthe second one whenever a message arrives at the module.  In  initialize()  we create a message object ( cMessage ), and send it out\non gate  out . Since this gate is connected to the other module's\ninput gate, the simulation kernel will deliver this message to the other module\nin the argument to  handleMessage()  -- after a 100ms propagation delay\nassigned to the link in the NED file. The other module just sends it back\n(another 100ms delay), so it will result in a continuous ping-pong.  Messages (packets, frames, jobs, etc) and events (timers, timeouts) are\nall represented by cMessage objects (or its subclasses) in OMNeT++.\nAfter you send or schedule them, they will be held by the simulation\nkernel in the \"scheduled events\" or \"future events\" list until\ntheir time comes and they are delivered to the modules via  handleMessage() .  Note that there is no stopping condition built into this simulation:\nit would continue forever. You will be able to stop it from the GUI.\n(You could also specify a simulation time limit or CPU time limit\nin the configuration file, but we don't do that in the tutorial.)", 
            "title": "1.4 Adding the C++ files"
        }, 
        {
            "location": "/tutorials/tictoc/part1/#15-adding-omnetppini", 
            "text": "To be able to run the simulation, we need to create an  omnetpp.ini  file. omnetpp.ini  tells the simulation program which network you want to simulate\n(as NED files may contain several networks), you can pass parameters\nto the model, explicitly specify seeds for the random number generators, etc.  Create an  omnetpp.ini  file using the  File -  New -  Initialization file (INI) \nmenu item. The new file will open in an  Inifile Editor .\nAs the NED Editor, the Inifile Editor also has two modes,  Form  and  Source ,\nwhich edit the same content. The former is more suitable for configuring the\nsimulation kernel, and the latter for entering module parameters.  For now, just switch to  Source  mode and enter the following:  \n[General]\nnetwork = Tictoc1  You can verify the result in  Form  mode:   tictoc2  and further steps will all share a common   file.  We are now done with creating the first model, and ready to compile and run it.  Sources:  ,  ,", 
            "title": "1.5  Adding omnetpp.ini"
        }, 
        {
            "location": "/tutorials/tictoc/part2/", 
            "text": "Part 2 - Running the Simulation\n\n\n2.1 Launching the simulation program\n\n\nOnce you complete the above steps, you can launch the simulation by selecting\n%omnetpp.ini (in either the editor area or the \nProject Explorer\n),\nand pressing the \nRun\n button.\n\n\n\n\nThe IDE will build your project automatically. If there are compilation errors,\nyou need to rectify those until you get an error-free compilation and linking.\nYou can manually trigger a build by hitting choosing \nProject -\n Build All\n\nfrom the menu, or hitting \nCtrl+B\n.\n\n\n\n\nNote\n\n\nIf you want to build the simulation executable on the command-line,\ncreate a \nMakefile\n using the \nopp_makemake\n\ncommand, then enter \nmake\n to build the project. It will produce\nan executable that can be run by entering \n./tictoc\n.\n\n\n\n\n2.2 Running the simulation\n\n\nAfter successfully building and launching your simulation, you should see\na new GUI window appear, similar to the one in the screenshot below.\nThe window belongs to \nQtenv\n, the main OMNeT++ simulation runtime GUI.\nYou should also see the network containing \ntic\n and \ntoc\n displayed\ngraphically in the main area.\n\n\nPress the \nRun\n button on the toolbar to start the simulation. What you should\nsee is that \ntic\n and \ntoc\n are exchanging messages with each other.\n\n\n\n\nThe main window toolbar displays the current simulation time. This is virtual time,\nit has nothing to do with the actual (or wall-clock) time that the program takes to\nexecute. Actually, how many seconds you can simulate in one real-world second\ndepends highly on the speed of your hardware and even more on the nature and\ncomplexity of the simulation model itself.\n\n\nNote that it takes zero simulation time for a node to process the message.\nThe only thing that makes the simulation time pass in this model is\nthe propagation delay on the connections.\n\n\nYou can play with slowing down the animation or making it faster with\nthe slider at the top of the graphics window. You can stop the simulation\nby hitting F8 (equivalent to the STOP button on the toolbar), single-step\nthrough it (F4), run it with (F5) or without (F6) animation.\nF7 (express mode) completely turns off tracing features for maximum speed.\nNote the event/sec and simsec/sec gauges on the status bar of the\nmain window (only visible when the simulation is running in fast or express mode).\n\n\n\n\nExercise\n\n\nExplore the GUI by running the simulation several times. Try\n\nRun\n, \nRun Until\n, \nRebuild Network\n, and other functions.\n\n\n\n\nYou can exit the simulation program by clicking its \nClose\n icon or\nchoosing \nFile -\n Exit\n.\n\n\n2.3 Debugging\n\n\nThe simulation is just a C++ program, and as such, it often needs to be\ndebugged while it is being developed. In this section we'll look at the\nbasics of debugging to help you acquire this vital task.\n\n\nThe simulation can be started in debug mode by clicking the \nDebug\n\nbutton on the IDE's main toolbar.\n\n\n\n\nThis will cause the simulation program to be launched under a debugger\n(usually \ngdb\n). The IDE will also switch into \"Debug perspective\",\ni.e. rearrange its various panes and views to a layout which is better\nsuited to debugging. You can end the debugging session with the\n\nTerminate\n button (a red square) on the toolbar.\n\n\nRuntime errors\n\n\nDebugging is most often needed to track down runtime errors. Let's try it!\nFirst, deliberately introduce an error into the program. In\n\n, duplicate the \nsend()\n line inside\n \nhandleMessage()\n, so that the code looks like this:\n\n\nvoid Txc1::handleMessage(cMessage *msg)\n{\n    //...\n    send(msg, \nout\n); // send out the message\n    send(msg, \nout\n); // THIS SHOULD CAUSE AN ERROR\n}\n\n\n\n\nWhen you launch the simulation in normal mode (\nRun\n button) and try to run it,\nyou'll get an error message like this:\n\n\n\n\nNow, run the simulation in \nDebug\n mode. Due to a \ndebug-on-errors\n option\nbeing enabled by default, the simulation program will stop in the debugger.\nYou can locate the error by examining the stack trace (the list of nested\nfunction calls) in the \nDebug\n view:\n\n\n\n\nYou can see that it was OMNeT++'s \nbreakIntoDebuggerIfRequested()\n method that\nactivated the debugger. From then on, you need to search for a function that\nlooks familiar, i.e. for one that is part of the model. In our case, that is\nthe \"Txc1::handleMessage() at txc1.cc:54\" line. Selecting that line will\nshow you the corresponding source code in the editor area, and lets you\nexamine the values of variables in the \nVariables\n view. This information\nwill help you determine the cause of the error and fix it.\n\n\nCrashes\n\n\nTracking down crashes i.e. segfaults is similar, let's try that as well.\nUndo the previous source code edit (remove the duplicate \nsend()\n line),\nand introduce another error. Let's pretend we forgot to create the message\nbefore sending it, and change the following lines in \ninitialize()\n\n\n        cMessage *msg = new cMessage(\ntictocMsg\n);\n        send(msg, \nout\n);\n\n\n\n\nto simply\n\n\n        cMessage *msg; // no initialization!\n        send(msg, \nout\n);\n\n\n\n\nWhen you run the simulation, it will crash. (You will get an error message\nsimilar to \"Simulation terminated with exit code: 139\"). If you launch the simulation\nagain, this time in \nDebug\n mode, the crash will bring you into the debugger.\nOnce there, you'll be able to locate the error in the \nDebug\n view and examine\nvariables, which will help you identify and fix the bug.\n\n\nBreakpoints\n\n\nYou can also manually place breakpoints into the code. Breakpoints will stop\nexecution, and let you examine variables, execute the code line-by-line,\nor resume execution (until the next breakpint).\n\n\nA breakpoint can be placed at a specific line in the source code by double-clicking\non the left gutter in the editor, or choosing \nToggle Breakpoint\n from\nthe context menu. The list of active (and inactive) breakpoints can be examined\nin the \nBreakpoints\n view.\n\n\n\n\nExercise\n\n\nExperiment with breakpoints! Place a breakpoint at the beginning of\nthe \nhandleMessage()\n method function, and run the simulation. Use appropriate\nbuttons on the toolbar to single-step, continue execution until next time the\nbreakpoint is hit, and so on.\n\n\n\n\nDebug next event\n\n\nIf you did the previous exercise, you must have noticed that the breakpoint\nwas triggered at each and every event in the Txc1 simple module. In real life\nit often occurs that an error only surfaces at, say, the 357\nth\n event in that module,\nso ideally that's when you'd want to start debugging. It is not very convenient\nto have to hit \nResume\n 356 times just to get to the place of the error.\nA possible solution is to add a \ncondition\n or an \nignore\n-count to the\nbreakpoint (see \nBreakpoint Properties\n in its context menu). However,\nthere is a potentially more convenient solution.\n\n\nIn \nQtenv\n, use \nRun Until\n to get to the event to be debugged. Then,\nchoose \nSimulation -\n Debug Next Event\n from the menu. This will trigger\na breakpoint in the debugger at the beginning of \nhandleMessage()\n of the\nnext event, and you can start debugging that event.\n\n\n\n\n2.4 The Debug/Run dialog\n\n\nLet us return to launching simulations once more.\n\n\nWhen you launch the simulation program with the \nRun\n or \nDebug\n\nbuttons on the IDE toolbar, settings associated with the launch\nare saved in a \nlaunch configuration\n. Launch configurations\ncan be viewed in the \nRun/Debug Configurations\n dialog which\ncan be opened e.g. by clicking the little \ndown\n arrow next to the\n\nRun\n (\nDebug\n) toolbar button to open a menu, and choosing\n\nRun (Debug) Configurations...\n in it. In the same menu, you can also\nclick the name of a launch configuration (e.g. \ntictoc\n) while\nholding down the Ctrl key to open the dialog with the corresponding\nconfiguration.\n\n\nThe dialog allows you activate various settings for the launch.\n\n\n\n\n2.5 Visualizing on a Sequence Chart\n\n\nThe OMNeT++ simulation kernel can record the message exchanges during the\nsimulation into an \nevent log file\n. To enable recording the event log,\ncheck the \nRecord eventlog\n checkbox in the launch configuration dialog.\nAlternatively, you can specify \nrecord-eventlog = true\n in omnetpp.ini,\nor even, use the \nRecord\n button in the Qtenv graphical runtime environment\nafter launching,\n\n\nThe log file can be analyzed later with the \nSequence Chart\n tool in the IDE.\nThe \nresults\n directory in the project folder contains the \n.elog\n file.\nDouble-clicking on it in the OMNeT++ IDE opens the Sequence Chart tool,\nand the event log tab at the bottom of the window.\n\n\n\n\nNote\n\n\nThe resulting log file can be quite large, so enable this feature only\nif you really need it.\n\n\n\n\nThe following figure has been created with the \nSequence Chart\n tool, and shows\nhow the message is routed between the different nodes in the network.\nIn this instance the chart is very simple, but when you have a complex model,\nsequence charts can be very valuable in debugging, exploring or documenting\nthe model's behaviour.\n\n\n\n\nSources: \n, \n,", 
            "title": "Running the Simulation"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#part-2-running-the-simulation", 
            "text": "", 
            "title": "Part 2 - Running the Simulation"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#21-launching-the-simulation-program", 
            "text": "Once you complete the above steps, you can launch the simulation by selecting\n%omnetpp.ini (in either the editor area or the  Project Explorer ),\nand pressing the  Run  button.   The IDE will build your project automatically. If there are compilation errors,\nyou need to rectify those until you get an error-free compilation and linking.\nYou can manually trigger a build by hitting choosing  Project -  Build All \nfrom the menu, or hitting  Ctrl+B .   Note  If you want to build the simulation executable on the command-line,\ncreate a  Makefile  using the  opp_makemake \ncommand, then enter  make  to build the project. It will produce\nan executable that can be run by entering  ./tictoc .", 
            "title": "2.1 Launching the simulation program"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#22-running-the-simulation", 
            "text": "After successfully building and launching your simulation, you should see\na new GUI window appear, similar to the one in the screenshot below.\nThe window belongs to  Qtenv , the main OMNeT++ simulation runtime GUI.\nYou should also see the network containing  tic  and  toc  displayed\ngraphically in the main area.  Press the  Run  button on the toolbar to start the simulation. What you should\nsee is that  tic  and  toc  are exchanging messages with each other.   The main window toolbar displays the current simulation time. This is virtual time,\nit has nothing to do with the actual (or wall-clock) time that the program takes to\nexecute. Actually, how many seconds you can simulate in one real-world second\ndepends highly on the speed of your hardware and even more on the nature and\ncomplexity of the simulation model itself.  Note that it takes zero simulation time for a node to process the message.\nThe only thing that makes the simulation time pass in this model is\nthe propagation delay on the connections.  You can play with slowing down the animation or making it faster with\nthe slider at the top of the graphics window. You can stop the simulation\nby hitting F8 (equivalent to the STOP button on the toolbar), single-step\nthrough it (F4), run it with (F5) or without (F6) animation.\nF7 (express mode) completely turns off tracing features for maximum speed.\nNote the event/sec and simsec/sec gauges on the status bar of the\nmain window (only visible when the simulation is running in fast or express mode).   Exercise  Explore the GUI by running the simulation several times. Try Run ,  Run Until ,  Rebuild Network , and other functions.   You can exit the simulation program by clicking its  Close  icon or\nchoosing  File -  Exit .", 
            "title": "2.2 Running the simulation"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#23-debugging", 
            "text": "The simulation is just a C++ program, and as such, it often needs to be\ndebugged while it is being developed. In this section we'll look at the\nbasics of debugging to help you acquire this vital task.  The simulation can be started in debug mode by clicking the  Debug \nbutton on the IDE's main toolbar.   This will cause the simulation program to be launched under a debugger\n(usually  gdb ). The IDE will also switch into \"Debug perspective\",\ni.e. rearrange its various panes and views to a layout which is better\nsuited to debugging. You can end the debugging session with the Terminate  button (a red square) on the toolbar.", 
            "title": "2.3 Debugging"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#runtime-errors", 
            "text": "Debugging is most often needed to track down runtime errors. Let's try it!\nFirst, deliberately introduce an error into the program. In , duplicate the  send()  line inside\n  handleMessage() , so that the code looks like this:  void Txc1::handleMessage(cMessage *msg)\n{\n    //...\n    send(msg,  out ); // send out the message\n    send(msg,  out ); // THIS SHOULD CAUSE AN ERROR\n}  When you launch the simulation in normal mode ( Run  button) and try to run it,\nyou'll get an error message like this:   Now, run the simulation in  Debug  mode. Due to a  debug-on-errors  option\nbeing enabled by default, the simulation program will stop in the debugger.\nYou can locate the error by examining the stack trace (the list of nested\nfunction calls) in the  Debug  view:   You can see that it was OMNeT++'s  breakIntoDebuggerIfRequested()  method that\nactivated the debugger. From then on, you need to search for a function that\nlooks familiar, i.e. for one that is part of the model. In our case, that is\nthe \"Txc1::handleMessage() at txc1.cc:54\" line. Selecting that line will\nshow you the corresponding source code in the editor area, and lets you\nexamine the values of variables in the  Variables  view. This information\nwill help you determine the cause of the error and fix it.", 
            "title": "Runtime errors"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#crashes", 
            "text": "Tracking down crashes i.e. segfaults is similar, let's try that as well.\nUndo the previous source code edit (remove the duplicate  send()  line),\nand introduce another error. Let's pretend we forgot to create the message\nbefore sending it, and change the following lines in  initialize()          cMessage *msg = new cMessage( tictocMsg );\n        send(msg,  out );  to simply          cMessage *msg; // no initialization!\n        send(msg,  out );  When you run the simulation, it will crash. (You will get an error message\nsimilar to \"Simulation terminated with exit code: 139\"). If you launch the simulation\nagain, this time in  Debug  mode, the crash will bring you into the debugger.\nOnce there, you'll be able to locate the error in the  Debug  view and examine\nvariables, which will help you identify and fix the bug.", 
            "title": "Crashes"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#breakpoints", 
            "text": "You can also manually place breakpoints into the code. Breakpoints will stop\nexecution, and let you examine variables, execute the code line-by-line,\nor resume execution (until the next breakpint).  A breakpoint can be placed at a specific line in the source code by double-clicking\non the left gutter in the editor, or choosing  Toggle Breakpoint  from\nthe context menu. The list of active (and inactive) breakpoints can be examined\nin the  Breakpoints  view.   Exercise  Experiment with breakpoints! Place a breakpoint at the beginning of\nthe  handleMessage()  method function, and run the simulation. Use appropriate\nbuttons on the toolbar to single-step, continue execution until next time the\nbreakpoint is hit, and so on.", 
            "title": "Breakpoints"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#debug-next-event", 
            "text": "If you did the previous exercise, you must have noticed that the breakpoint\nwas triggered at each and every event in the Txc1 simple module. In real life\nit often occurs that an error only surfaces at, say, the 357 th  event in that module,\nso ideally that's when you'd want to start debugging. It is not very convenient\nto have to hit  Resume  356 times just to get to the place of the error.\nA possible solution is to add a  condition  or an  ignore -count to the\nbreakpoint (see  Breakpoint Properties  in its context menu). However,\nthere is a potentially more convenient solution.  In  Qtenv , use  Run Until  to get to the event to be debugged. Then,\nchoose  Simulation -  Debug Next Event  from the menu. This will trigger\na breakpoint in the debugger at the beginning of  handleMessage()  of the\nnext event, and you can start debugging that event.", 
            "title": "Debug next event"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#24-the-debugrun-dialog", 
            "text": "Let us return to launching simulations once more.  When you launch the simulation program with the  Run  or  Debug \nbuttons on the IDE toolbar, settings associated with the launch\nare saved in a  launch configuration . Launch configurations\ncan be viewed in the  Run/Debug Configurations  dialog which\ncan be opened e.g. by clicking the little  down  arrow next to the Run  ( Debug ) toolbar button to open a menu, and choosing Run (Debug) Configurations...  in it. In the same menu, you can also\nclick the name of a launch configuration (e.g.  tictoc ) while\nholding down the Ctrl key to open the dialog with the corresponding\nconfiguration.  The dialog allows you activate various settings for the launch.", 
            "title": "2.4 The Debug/Run dialog"
        }, 
        {
            "location": "/tutorials/tictoc/part2/#25-visualizing-on-a-sequence-chart", 
            "text": "The OMNeT++ simulation kernel can record the message exchanges during the\nsimulation into an  event log file . To enable recording the event log,\ncheck the  Record eventlog  checkbox in the launch configuration dialog.\nAlternatively, you can specify  record-eventlog = true  in omnetpp.ini,\nor even, use the  Record  button in the Qtenv graphical runtime environment\nafter launching,  The log file can be analyzed later with the  Sequence Chart  tool in the IDE.\nThe  results  directory in the project folder contains the  .elog  file.\nDouble-clicking on it in the OMNeT++ IDE opens the Sequence Chart tool,\nand the event log tab at the bottom of the window.   Note  The resulting log file can be quite large, so enable this feature only\nif you really need it.   The following figure has been created with the  Sequence Chart  tool, and shows\nhow the message is routed between the different nodes in the network.\nIn this instance the chart is very simple, but when you have a complex model,\nsequence charts can be very valuable in debugging, exploring or documenting\nthe model's behaviour.   Sources:  ,  ,", 
            "title": "2.5 Visualizing on a Sequence Chart"
        }, 
        {
            "location": "/tutorials/tictoc/part3/", 
            "text": "Part 3 - Enhancing the 2-node TicToc\n\n\n3.1 Adding icons\n\n\nHere we make the model look a bit prettier in the GUI. We assign\nthe \nblock/routing\n icon (the file \nimages/block/routing.png\n), and paint it cyan for \ntic\n\nand yellow for \ntoc\n. This is achieved by adding display strings to the\nNED file. The \ni=\n tag in the display string specifies the icon.\n\n\n\n\n\nYou can see the result here:\n\n\n\n\n3.2 Adding logging\n\n\nWe also modify the C++ code. We add log statements to \nTxc1\n so that it\nprints what it is doing. OMNeT++ provides a sophisticated logging facility\nwith log levels, log channels, filtering, etc. that are useful for large\nand complex models, but in this model we'll use its simplest form \nEV\n:\n\n\n\n\n\nand\n\n\n\n\n\nWhen you run the simulation in the OMNeT++ runtime environment, the following output\nwill appear in the log window:\n\n\n\n\nYou can also open separate output windows for \ntic\n and \ntoc\n by right-clicking\non their icons and choosing \nComponent log\n from the menu. This feature\nwill be useful when you have a large model (\"fast scrolling logs syndrome\")\nand you're interested only in the log messages of specific module.\n\n\n\n\nSources: \n, \n, \n\n\n3.3 Adding state variables\n\n\nIn this step we add a counter to the module, and delete the message\nafter ten exchanges.\n\n\nWe add the counter as a class member:\n\n\n\n\n\nWe set the variable to 10 in \ninitialize()\n and decrement in \nhandleMessage()\n,\nthat is, on every message arrival. After it reaches zero, the simulation\nwill run out of events and terminate.\n\n\nNote the\n\n\n\n\n\nline in the source: this makes it possible to see the counter value\nin the graphical runtime environment.\n\n\nIf you click on \ntic\n's icon, the inspector window in the bottom left corner of the main window will display\ndetails about \ntic\n. Make sure that \nChildren\n mode is selected from the toolbar at the top.\nThe inspector now displays the counter variable.\n\n\n\n\nAs you continue running the simulation, you can follow as the counter\nkeeps decrementing until it reaches zero.\n\n\nSources: \n, \n, \n\n\n3.4 Adding parameters\n\n\nIn this step you'll learn how to add input parameters to the simulation:\nwe'll turn the \"magic number\" 10 into a parameter and add a boolean parameter\nto decide whether the module should send out the first message in its\ninitialization code (whether this is a \ntic\n or a \ntoc\n module).\n\n\nModule parameters have to be declared in the NED file. The data type can\nbe numeric, string, bool, or xml (the latter is for easy access to\nXML config files), among others.\n\n\n\n\n\nWe also have to modify the C++ code to read the parameter in\n\ninitialize()\n, and assign it to the counter.\n\n\n\n\n\nWe can use the second parameter to decide whether to send initial message:\n\n\n\n\n\nNow, we can assign the parameters in the NED file or from \nomnetpp.ini\n.\nAssignments in the NED file take precedence. You can define default\nvalues for parameters if you use the \ndefault(...)\n syntax\nin the NED file. In this case you can either set the value of the\nparameter in omnetpp.ini or use the default value provided by the NED file.\n\n\nHere, we assign one parameter in the NED file:\n\n\n\n\n\nand the other in \nomnetpp.ini\n:\n\n\n\n\n\nNote that because omnetpp.ini supports wildcards, and parameters\nassigned from NED files take precedence over the ones in omnetpp.ini,\nwe could have used\n\n\nTictoc4.t*c.limit=5\n\n\n\n\nor\n\n\nTictoc4.*.limit=5\n\n\n\n\nor even\n\n\n**.limit=5\n\n\n\n\nwith the same effect. (The difference between \n*\n and \n**\n is that \n*\n will not match\na dot and \n**\n will.)\n\n\nIn the graphical runtime environment, you can inspect module parameters either in the object tree\non the left-hand side of the main window, or in the Parameters page of\nthe module inspector (information is shown in the bottom left corner of the main window after\nclicking on a module).\n\n\nThe module with the smaller limit will delete the message and thereby\nconclude the simulation.\n\n\nSources: \n, \n, \n\n\n3.5 Using NED inheritance\n\n\nIf we take a closer look at the NED file we will realize that \ntic\n\nand \ntoc\n differs only in their parameter values and their display string.\nWe can create a new simple module type by inheriting from an other one and specifying\nor overriding some of its parameters. In our case we will derive two simple\nmodule types (\nTic\n and \nToc\n). Later we can use these types when defining\nthe submodules in the network.\n\n\nDeriving from an existing simple module is easy. Here is the base module:\n\n\n\n\n\nAnd here is the derived module. We just simply specify the parameter values and add some\ndisplay properties.\n\n\n\n\n\nThe \nToc\n module looks similar, but with different parameter values.\n\n\n\n\n\n\n\nNote\n\n\nThe C++ implementation is inherited from the base simple module (\nTxc5\n).\n\n\n\n\nOnce we created the new simple modules, we can use them as submodule types in our network:\n\n\n\n\n\nAs you can see, the network definition is much shorter and simpler now.\nInheritance allows you to use common types in your network and avoid\nredundant definitions and parameter settings.\n\n\n3.6 Modeling processing delay\n\n\nIn the previous models, \ntic\n and \ntoc\n immediately sent back the\nreceived message. Here we'll add some timing: \ntic\n and \ntoc\n will hold the\nmessage for 1 simulated second before sending it back. In OMNeT++\nsuch timing is achieved by the module sending a message to itself.\nSuch messages are called self-messages (but only because of the way they\nare used, otherwise they are ordinary message objects).\n\n\nWe added two cMessage * variables, \nevent\n and \ntictocMsg\n\nto the class, to remember the message we use for timing and message whose\nprocessing delay we are simulating.\n\n\n\n\n\nWe \"send\" the self-messages with the scheduleAt() function, specifying\nwhen it should be delivered back to the module.\n\n\n\n\n\nIn \nhandleMessage()\n now we have to differentiate whether a new message\nhas arrived via the input gate or the self-message came back\n(timer expired). Here we are using\n\n\n\n\n\nbut we could have written\n\n\n    if (msg-\nisSelfMessage())\n\n\n\n\nas well.\n\n\nWe have left out the counter, to keep the source code small.\n\n\nWhile running the simulation you will see the following log output:\n\n\n\n\nSources: \n, \n, \n\n\n3.7 Random numbers and parameters\n\n\nIn this step we'll introduce random numbers. We change the delay from 1s\nto a random value which can be set from the NED file or from omnetpp.ini.\nModule parameters are able to return random variables; however, to make\nuse of this feature we have to read the parameter in \nhandleMessage()\n\nevery time we use it.\n\n\n\n\n\nIn addition, we'll \"lose\" (delete) the packet with a small (hardcoded) probability.\n\n\n\n\n\nWe'll assign the parameters in omnetpp.ini:\n\n\n\n\n\nYou can try that no matter how many times you re-run the simulation (or\nrestart it, \nSimulate -\n Rebuild network\n menu item), you'll get exactly the\nsame results. This is because OMNeT++ uses a deterministic algorithm\n(by default the Mersenne Twister RNG) to generate random numbers, and\ninitializes it to the same seed. This is important for reproducible\nsimulations. You can experiment with different seeds if you add the\nfollowing lines to omnetpp.ini:\n\n\n[General]\n\n\nseed-0-mt\n=\n532569  # or any other 32-bit value\n\n\n\n\n\nFrom the syntax you have probably guessed that OMNeT++ supports\nmore than one RNGs. That's right, however, all models in this tutorial\nuse RNG 0.\n\n\n\n\nExercise\n\n\nTry other distributions as well.\n\n\n\n\nSources: \n, \n, \n\n\n3.8 Timeout, cancelling timers\n\n\nIn order to get one step closer to modelling networking protocols,\nlet us transform our model into a stop-and-wait simulation.\nThis time we'll have separate classes for \ntic\n and \ntoc\n. The basic\nscenario is similar to the previous ones: \ntic\n and \ntoc\n will be tossing a\nmessage to one another. However, \ntoc\n will \"lose\" the message with some\nnonzero probability, and in that case \ntic\n will have to resend it.\n\n\nHere's \ntoc\n's code:\n\n\n\n\n\nThanks to the \nbubble()\n call in the code, \ntoc\n will display a callout whenever\nit drops the message.\n\n\n\n\nSo, \ntic\n will start a timer whenever it sends the message. When\nthe timer expires, we'll assume the message was lost and send another\none. If \ntoc\n's reply arrives, the timer has to be cancelled.\nThe timer will be (what else?) a self-message.\n\n\n\n\n\nCancelling the timer will be done with the \ncancelEvent()\n call. Note that\nthis does not prevent us from being able to reuse the same\ntimeout message over and over.\n\n\n\n\n\nYou can read Tic's full source in \n\n\nSources: \n, \n, \n\n\n3.9 Retransmitting the same message\n\n\nIn this step we refine the previous model.\nThere we just created another packet if we needed to\nretransmit. This is OK because the packet didn't contain much, but\nin real life it's usually more practical to keep a copy of the original\npacket so that we can re-send it without the need to build it again.\nKeeping a pointer to the sent message - so we can send it again - might seem easier,\nbut when the message is destroyed at the other node the pointer becomes invalid.\n\n\nWhat we do here is keep the original packet and send only copies of it.\nWe delete the original when \ntoc\n's acknowledgement arrives.\nTo make it easier to visually verify the model, we'll include a message\nsequence number in the message names.\n\n\nIn order to avoid \nhandleMessage()\n growing too large, we'll put the\ncorresponding code into two new functions, \ngenerateNewMessage()\n\nand \nsendCopyOf()\n and call them from \nhandleMessage()\n.\n\n\nThe functions:\n\n\n\n\n\n\n\n\nSources: \n, \n,", 
            "title": "Enhancing the 2-node TicToc"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#part-3-enhancing-the-2-node-tictoc", 
            "text": "", 
            "title": "Part 3 - Enhancing the 2-node TicToc"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#31-adding-icons", 
            "text": "Here we make the model look a bit prettier in the GUI. We assign\nthe  block/routing  icon (the file  images/block/routing.png ), and paint it cyan for  tic \nand yellow for  toc . This is achieved by adding display strings to the\nNED file. The  i=  tag in the display string specifies the icon.   You can see the result here:", 
            "title": "3.1 Adding icons"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#32-adding-logging", 
            "text": "We also modify the C++ code. We add log statements to  Txc1  so that it\nprints what it is doing. OMNeT++ provides a sophisticated logging facility\nwith log levels, log channels, filtering, etc. that are useful for large\nand complex models, but in this model we'll use its simplest form  EV :   and   When you run the simulation in the OMNeT++ runtime environment, the following output\nwill appear in the log window:   You can also open separate output windows for  tic  and  toc  by right-clicking\non their icons and choosing  Component log  from the menu. This feature\nwill be useful when you have a large model (\"fast scrolling logs syndrome\")\nand you're interested only in the log messages of specific module.   Sources:  ,  ,", 
            "title": "3.2 Adding logging"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#33-adding-state-variables", 
            "text": "In this step we add a counter to the module, and delete the message\nafter ten exchanges.  We add the counter as a class member:   We set the variable to 10 in  initialize()  and decrement in  handleMessage() ,\nthat is, on every message arrival. After it reaches zero, the simulation\nwill run out of events and terminate.  Note the   line in the source: this makes it possible to see the counter value\nin the graphical runtime environment.  If you click on  tic 's icon, the inspector window in the bottom left corner of the main window will display\ndetails about  tic . Make sure that  Children  mode is selected from the toolbar at the top.\nThe inspector now displays the counter variable.   As you continue running the simulation, you can follow as the counter\nkeeps decrementing until it reaches zero.  Sources:  ,  ,", 
            "title": "3.3 Adding state variables"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#34-adding-parameters", 
            "text": "In this step you'll learn how to add input parameters to the simulation:\nwe'll turn the \"magic number\" 10 into a parameter and add a boolean parameter\nto decide whether the module should send out the first message in its\ninitialization code (whether this is a  tic  or a  toc  module).  Module parameters have to be declared in the NED file. The data type can\nbe numeric, string, bool, or xml (the latter is for easy access to\nXML config files), among others.   We also have to modify the C++ code to read the parameter in initialize() , and assign it to the counter.   We can use the second parameter to decide whether to send initial message:   Now, we can assign the parameters in the NED file or from  omnetpp.ini .\nAssignments in the NED file take precedence. You can define default\nvalues for parameters if you use the  default(...)  syntax\nin the NED file. In this case you can either set the value of the\nparameter in omnetpp.ini or use the default value provided by the NED file.  Here, we assign one parameter in the NED file:   and the other in  omnetpp.ini :   Note that because omnetpp.ini supports wildcards, and parameters\nassigned from NED files take precedence over the ones in omnetpp.ini,\nwe could have used  Tictoc4.t*c.limit=5  or  Tictoc4.*.limit=5  or even  **.limit=5  with the same effect. (The difference between  *  and  **  is that  *  will not match\na dot and  **  will.)  In the graphical runtime environment, you can inspect module parameters either in the object tree\non the left-hand side of the main window, or in the Parameters page of\nthe module inspector (information is shown in the bottom left corner of the main window after\nclicking on a module).  The module with the smaller limit will delete the message and thereby\nconclude the simulation.  Sources:  ,  ,", 
            "title": "3.4 Adding parameters"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#35-using-ned-inheritance", 
            "text": "If we take a closer look at the NED file we will realize that  tic \nand  toc  differs only in their parameter values and their display string.\nWe can create a new simple module type by inheriting from an other one and specifying\nor overriding some of its parameters. In our case we will derive two simple\nmodule types ( Tic  and  Toc ). Later we can use these types when defining\nthe submodules in the network.  Deriving from an existing simple module is easy. Here is the base module:   And here is the derived module. We just simply specify the parameter values and add some\ndisplay properties.   The  Toc  module looks similar, but with different parameter values.    Note  The C++ implementation is inherited from the base simple module ( Txc5 ).   Once we created the new simple modules, we can use them as submodule types in our network:   As you can see, the network definition is much shorter and simpler now.\nInheritance allows you to use common types in your network and avoid\nredundant definitions and parameter settings.", 
            "title": "3.5 Using NED inheritance"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#36-modeling-processing-delay", 
            "text": "In the previous models,  tic  and  toc  immediately sent back the\nreceived message. Here we'll add some timing:  tic  and  toc  will hold the\nmessage for 1 simulated second before sending it back. In OMNeT++\nsuch timing is achieved by the module sending a message to itself.\nSuch messages are called self-messages (but only because of the way they\nare used, otherwise they are ordinary message objects).  We added two cMessage * variables,  event  and  tictocMsg \nto the class, to remember the message we use for timing and message whose\nprocessing delay we are simulating.   We \"send\" the self-messages with the scheduleAt() function, specifying\nwhen it should be delivered back to the module.   In  handleMessage()  now we have to differentiate whether a new message\nhas arrived via the input gate or the self-message came back\n(timer expired). Here we are using   but we could have written      if (msg- isSelfMessage())  as well.  We have left out the counter, to keep the source code small.  While running the simulation you will see the following log output:   Sources:  ,  ,", 
            "title": "3.6 Modeling processing delay"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#37-random-numbers-and-parameters", 
            "text": "In this step we'll introduce random numbers. We change the delay from 1s\nto a random value which can be set from the NED file or from omnetpp.ini.\nModule parameters are able to return random variables; however, to make\nuse of this feature we have to read the parameter in  handleMessage() \nevery time we use it.   In addition, we'll \"lose\" (delete) the packet with a small (hardcoded) probability.   We'll assign the parameters in omnetpp.ini:   You can try that no matter how many times you re-run the simulation (or\nrestart it,  Simulate -  Rebuild network  menu item), you'll get exactly the\nsame results. This is because OMNeT++ uses a deterministic algorithm\n(by default the Mersenne Twister RNG) to generate random numbers, and\ninitializes it to the same seed. This is important for reproducible\nsimulations. You can experiment with different seeds if you add the\nfollowing lines to omnetpp.ini:  [General]  seed-0-mt = 532569  # or any other 32-bit value   From the syntax you have probably guessed that OMNeT++ supports\nmore than one RNGs. That's right, however, all models in this tutorial\nuse RNG 0.   Exercise  Try other distributions as well.   Sources:  ,  ,", 
            "title": "3.7 Random numbers and parameters"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#38-timeout-cancelling-timers", 
            "text": "In order to get one step closer to modelling networking protocols,\nlet us transform our model into a stop-and-wait simulation.\nThis time we'll have separate classes for  tic  and  toc . The basic\nscenario is similar to the previous ones:  tic  and  toc  will be tossing a\nmessage to one another. However,  toc  will \"lose\" the message with some\nnonzero probability, and in that case  tic  will have to resend it.  Here's  toc 's code:   Thanks to the  bubble()  call in the code,  toc  will display a callout whenever\nit drops the message.   So,  tic  will start a timer whenever it sends the message. When\nthe timer expires, we'll assume the message was lost and send another\none. If  toc 's reply arrives, the timer has to be cancelled.\nThe timer will be (what else?) a self-message.   Cancelling the timer will be done with the  cancelEvent()  call. Note that\nthis does not prevent us from being able to reuse the same\ntimeout message over and over.   You can read Tic's full source in   Sources:  ,  ,", 
            "title": "3.8 Timeout, cancelling timers"
        }, 
        {
            "location": "/tutorials/tictoc/part3/#39-retransmitting-the-same-message", 
            "text": "In this step we refine the previous model.\nThere we just created another packet if we needed to\nretransmit. This is OK because the packet didn't contain much, but\nin real life it's usually more practical to keep a copy of the original\npacket so that we can re-send it without the need to build it again.\nKeeping a pointer to the sent message - so we can send it again - might seem easier,\nbut when the message is destroyed at the other node the pointer becomes invalid.  What we do here is keep the original packet and send only copies of it.\nWe delete the original when  toc 's acknowledgement arrives.\nTo make it easier to visually verify the model, we'll include a message\nsequence number in the message names.  In order to avoid  handleMessage()  growing too large, we'll put the\ncorresponding code into two new functions,  generateNewMessage() \nand  sendCopyOf()  and call them from  handleMessage() .  The functions:    Sources:  ,  ,", 
            "title": "3.9 Retransmitting the same message"
        }, 
        {
            "location": "/tutorials/tictoc/part4/", 
            "text": "Part 4 - Turning it Into a Real Network\n\n\n4.1 More than two nodes\n\n\nNow we'll make a big step: create several \ntic\n modules and connect\nthem into a network. For now, we'll keep it simple what they do:\none of the nodes generates a message, and the others keep tossing\nit around in random directions until it arrives at\na predetermined destination node.\n\n\nThe NED file will need a few changes. First of all, the \nTxc\n module will\nneed to have multiple input and output gates:\n\n\n\n\n\nThe \n[ ]\n turns the gates into gate vectors. The size of the vector\n(the number of gates) will be determined where we use Txc to build\nthe network.\n\n\n\n\n\nHere we created 6 modules as a module vector, and connected them.\n\n\nThe resulting topology looks like this:\n\n\n\n\nIn this version, \ntic[0]\n will generate the message to be sent around.\nThis is done in \ninitialize()\n, with the help of the \ngetIndex()\n function which\nreturns the index of the module in the vector.\n\n\nThe meat of the code is the \nforwardMessage()\n function which we invoke\nfrom \nhandleMessage()\n whenever a message arrives at the node. It draws\na random gate number, and sends out message on that gate.\n\n\n\n\n\nWhen the message arrives at \ntic[3]\n, its \nhandleMessage()\n will delete the message.\n\n\nSee the full code in \n\n\n\n\nExercise\n\n\nYou'll notice that this simple \"routing\" is not very efficient:\noften the packet keeps bouncing between two nodes for a while before it is sent\nto a different direction. This can be improved somewhat if nodes don't send\nthe packet back to the sender. Implement this. Hints: \ncMessage::getArrivalGate()\n,\n\ncGate::getIndex()\n. Note that if the message didn't arrive via a gate but was\na self-message, then \ngetArrivalGate()\n returns \nNULL\n.\n\n\n\n\nSources: \n, \n, \n\n\n4.2 Channels and inner type definitions\n\n\nOur new network definition is getting quite complex and long, especially\nthe connections section. Let's try to simplify it. The first thing we\nnotice is that the connections always use the same \ndelay\n parameter.\nIt is possible to create types for the connections (they are called channels)\nsimilarly to simple modules. We should create a channel type which specifies the\ndelay parameter and we will use that type for all connections in the network.\n\n\n\n\n\nAs you have noticed we have defined the new channel type inside the network definition\nby adding a \ntypes\n section. This type definition is only visible inside the\nnetwork. It is called as a local or inner type. You can use simple modules as inner types\ntoo, if you wish.\n\n\n\n\nNote\n\n\nWe have created the channel by specializing the built-in \nDelayChannel\n.\n(built-in channels can be found inside the \nned\n package. Thats why we used\nthe full type name \nned.DelayChannel\n) after the \nextends\n keyword.\n\n\n\n\nNow let's check how the \nconnections\n section changed.\n\n\n\n\n\nAs you see we just specify the channel name inside the connection definition.\nThis allows to easily change the delay parameter for the whole network.\n\n\nSources: \n, \n, \n\n\n4.3 Using two-way connections\n\n\nIf we check the \nconnections\n section a little more, we will realize that\neach node pair is connected with two connections. One for each direction.\nOMNeT++ 4 supports two way connections, so let's use them.\n\n\nFirst of all, we have to define two-way (or so called \ninout\n) gates instead of the\nseparate \ninput\n and \noutput\n gates we used previously.\n\n\n\n\n\nThe new \nconnections\n section would look like this:\n\n\n\n\n\nWe have modified the gate names so we have to make some modifications to the\nC++ code.\n\n\n\n\n\n\n\nNote\n\n\nThe special $i and $o suffix after the gate name allows us to use the\nconnection's two direction separately.\n\n\n\n\nSources: \n, \n, \n\n\n4.4 Defining our message class\n\n\nIn this step the destination address is no longer hardcoded \ntic[3]\n -- we draw a\nrandom destination, and we'll add the destination address to the message.\n\n\nThe best way is to subclass cMessage and add destination as a data member.\nHand-coding the message class is usually tedious because it contains\na lot of boilerplate code, so we let OMNeT++ generate the class for us.\nThe message class specification is in \ntictoc13.msg\n:\n\n\n\n\n\n\n\nNote\n\n\nSee \nSection 6\n of the OMNeT++ manual for more details on messages.\n\n\n\n\nThe makefile is set up so that the message compiler, opp_msgc is invoked\nand it generates \ntictoc13_m.h\n and \ntictoc13_m.cc\n from the message declaration\n(The file names are generated from the \ntictoc13.msg\n file name, not the message type name).\nThey will contain a generated \nTicTocMsg13\n class subclassed from [\ncMessage\n];\nthe class will have getter and setter methods for every field.\n\n\nWe'll include \ntictoc13_m.h\n into our C++ code, and we can use \nTicTocMsg13\n as\nany other class.\n\n\n\n\n\nFor example, we use the following lines in \ngenerateMessage()\n to create the\nmessage and fill its fields.\n\n\n\n\n\nThen, \nhandleMessage()\n begins like this:\n\n\n\n\n\nIn the argument to handleMessage(), we get the message as a \ncMessage*\n pointer.\nHowever, we can only access its fields defined in \nTicTocMsg13\n if we cast\nmsg to \nTicTocMsg13*\n. Plain C-style cast (\n(TicTocMsg13 *)msg\n)\nis not safe because if the message is \nnot\n a \nTicTocMsg13\n after all\nthe program will just crash, causing an error which is difficult to explore.\n\n\nC++ offers a solution which is called \ndynamic_cast\n. Here we use \ncheck_and_cast\n()\n\nwhich is provided by OMNeT++: it tries to cast the pointer via \ndynamic_cast\n,\nand if it fails it stops the simulation with an error message, similar to the\nfollowing:\n\n\n\n\nIn the next line, we check if the destination address is the same as the\nnode's address. The \ngetIndex()\n member function returns the index\nof the module in the submodule vector (remember, in the NED file we\ndeclarared it as \ntic\n:\n \nTxc13\n[\n6\n]\n, so our nodes have addresses 0..5).\n\n\nTo make the model execute longer, after a message arrives to its destination\nthe destination node will generate another message with a random destination\naddress, and so forth. Read the full code: \n\n\nWhen you run the model, it'll look like this:\n\n\n\n\nYou can click on the messages to see their content in the inspector window.\nDouble-clicking will open the inspector in a new window.\n(You'll either have to temporarily stop the simulation for that,\nor to be very fast in handling the mouse). The inspector window\ndisplays lots of useful information; the message fields can be seen\non the \nContents\n page.\n\n\n\n\nSources: \n, \n, \n, \n\n\n\n\nExercise\n\n\nIn this model, there is only one message underway at any\ngiven moment: nodes only generate a message when another message arrives\nat them. We did it this way to make it easier to follow the simulation.\nChange the module class so that instead, it generates messages periodically.\nThe interval between messages should be a module parameter, returning\nexponentially distributed random numbers.", 
            "title": "Turning it Into a Real Network"
        }, 
        {
            "location": "/tutorials/tictoc/part4/#part-4-turning-it-into-a-real-network", 
            "text": "", 
            "title": "Part 4 - Turning it Into a Real Network"
        }, 
        {
            "location": "/tutorials/tictoc/part4/#41-more-than-two-nodes", 
            "text": "Now we'll make a big step: create several  tic  modules and connect\nthem into a network. For now, we'll keep it simple what they do:\none of the nodes generates a message, and the others keep tossing\nit around in random directions until it arrives at\na predetermined destination node.  The NED file will need a few changes. First of all, the  Txc  module will\nneed to have multiple input and output gates:   The  [ ]  turns the gates into gate vectors. The size of the vector\n(the number of gates) will be determined where we use Txc to build\nthe network.   Here we created 6 modules as a module vector, and connected them.  The resulting topology looks like this:   In this version,  tic[0]  will generate the message to be sent around.\nThis is done in  initialize() , with the help of the  getIndex()  function which\nreturns the index of the module in the vector.  The meat of the code is the  forwardMessage()  function which we invoke\nfrom  handleMessage()  whenever a message arrives at the node. It draws\na random gate number, and sends out message on that gate.   When the message arrives at  tic[3] , its  handleMessage()  will delete the message.  See the full code in    Exercise  You'll notice that this simple \"routing\" is not very efficient:\noften the packet keeps bouncing between two nodes for a while before it is sent\nto a different direction. This can be improved somewhat if nodes don't send\nthe packet back to the sender. Implement this. Hints:  cMessage::getArrivalGate() , cGate::getIndex() . Note that if the message didn't arrive via a gate but was\na self-message, then  getArrivalGate()  returns  NULL .   Sources:  ,  ,", 
            "title": "4.1 More than two nodes"
        }, 
        {
            "location": "/tutorials/tictoc/part4/#42-channels-and-inner-type-definitions", 
            "text": "Our new network definition is getting quite complex and long, especially\nthe connections section. Let's try to simplify it. The first thing we\nnotice is that the connections always use the same  delay  parameter.\nIt is possible to create types for the connections (they are called channels)\nsimilarly to simple modules. We should create a channel type which specifies the\ndelay parameter and we will use that type for all connections in the network.   As you have noticed we have defined the new channel type inside the network definition\nby adding a  types  section. This type definition is only visible inside the\nnetwork. It is called as a local or inner type. You can use simple modules as inner types\ntoo, if you wish.   Note  We have created the channel by specializing the built-in  DelayChannel .\n(built-in channels can be found inside the  ned  package. Thats why we used\nthe full type name  ned.DelayChannel ) after the  extends  keyword.   Now let's check how the  connections  section changed.   As you see we just specify the channel name inside the connection definition.\nThis allows to easily change the delay parameter for the whole network.  Sources:  ,  ,", 
            "title": "4.2 Channels and inner type definitions"
        }, 
        {
            "location": "/tutorials/tictoc/part4/#43-using-two-way-connections", 
            "text": "If we check the  connections  section a little more, we will realize that\neach node pair is connected with two connections. One for each direction.\nOMNeT++ 4 supports two way connections, so let's use them.  First of all, we have to define two-way (or so called  inout ) gates instead of the\nseparate  input  and  output  gates we used previously.   The new  connections  section would look like this:   We have modified the gate names so we have to make some modifications to the\nC++ code.    Note  The special $i and $o suffix after the gate name allows us to use the\nconnection's two direction separately.   Sources:  ,  ,", 
            "title": "4.3 Using two-way connections"
        }, 
        {
            "location": "/tutorials/tictoc/part4/#44-defining-our-message-class", 
            "text": "In this step the destination address is no longer hardcoded  tic[3]  -- we draw a\nrandom destination, and we'll add the destination address to the message.  The best way is to subclass cMessage and add destination as a data member.\nHand-coding the message class is usually tedious because it contains\na lot of boilerplate code, so we let OMNeT++ generate the class for us.\nThe message class specification is in  tictoc13.msg :    Note  See  Section 6  of the OMNeT++ manual for more details on messages.   The makefile is set up so that the message compiler, opp_msgc is invoked\nand it generates  tictoc13_m.h  and  tictoc13_m.cc  from the message declaration\n(The file names are generated from the  tictoc13.msg  file name, not the message type name).\nThey will contain a generated  TicTocMsg13  class subclassed from [ cMessage ];\nthe class will have getter and setter methods for every field.  We'll include  tictoc13_m.h  into our C++ code, and we can use  TicTocMsg13  as\nany other class.   For example, we use the following lines in  generateMessage()  to create the\nmessage and fill its fields.   Then,  handleMessage()  begins like this:   In the argument to handleMessage(), we get the message as a  cMessage*  pointer.\nHowever, we can only access its fields defined in  TicTocMsg13  if we cast\nmsg to  TicTocMsg13* . Plain C-style cast ( (TicTocMsg13 *)msg )\nis not safe because if the message is  not  a  TicTocMsg13  after all\nthe program will just crash, causing an error which is difficult to explore.  C++ offers a solution which is called  dynamic_cast . Here we use  check_and_cast () \nwhich is provided by OMNeT++: it tries to cast the pointer via  dynamic_cast ,\nand if it fails it stops the simulation with an error message, similar to the\nfollowing:   In the next line, we check if the destination address is the same as the\nnode's address. The  getIndex()  member function returns the index\nof the module in the submodule vector (remember, in the NED file we\ndeclarared it as  tic :   Txc13 [ 6 ] , so our nodes have addresses 0..5).  To make the model execute longer, after a message arrives to its destination\nthe destination node will generate another message with a random destination\naddress, and so forth. Read the full code:   When you run the model, it'll look like this:   You can click on the messages to see their content in the inspector window.\nDouble-clicking will open the inspector in a new window.\n(You'll either have to temporarily stop the simulation for that,\nor to be very fast in handling the mouse). The inspector window\ndisplays lots of useful information; the message fields can be seen\non the  Contents  page.   Sources:  ,  ,  ,    Exercise  In this model, there is only one message underway at any\ngiven moment: nodes only generate a message when another message arrives\nat them. We did it this way to make it easier to follow the simulation.\nChange the module class so that instead, it generates messages periodically.\nThe interval between messages should be a module parameter, returning\nexponentially distributed random numbers.", 
            "title": "4.4 Defining our message class"
        }, 
        {
            "location": "/tutorials/tictoc/part5/", 
            "text": "Part 5 - Adding Statistics Collection\n\n\n5.1 Displaying the number of packets sent/received\n\n\nTo get an overview at runtime how many messages each node sent or\nreceived, we've added two counters to the module class: numSent and numReceived.\n\n\n\n\n\nThey are set to zero and \nWATCH\n'ed in the \ninitialize()\n method. Now we\ncan use the \nFind/inspect\n objects dialog (\nInspect\n menu; it is also on\nthe toolbar) to learn how many packets were sent or received by the\nvarious nodes.\n\n\n\n\nIt's true that in this concrete simulation model the numbers will be\nroughly the same, so you can only learn from them that \nintuniform()\n\nworks properly. But in real-life simulations it can be very useful that\nyou can quickly get an overview about the state of various nodes in the\nmodel.\n\n\nIt can be also arranged that this info appears above the module\nicons. The \nt=\n display string tag specifies the text;\nwe only need to modify the displays string during runtime.\nThe following code does the job:\n\n\n\n\n\nAnd the result looks like this:\n\n\n\n\nSources: \n, \n, \n, \n\n\n5.2 Adding statistics collection\n\n\nThe previous simulation model does something interesting enough\nso that we can collect some statistics. For example, you may be interested\nin the average hop count a message has to travel before reaching\nits destination.\n\n\nWe'll record in the hop count of every message upon arrival into\nan output vector (a sequence of (time,value) pairs, sort of a time series).\nWe also calculate mean, standard deviation, minimum, maximum values per node, and\nwrite them into a file at the end of the simulation. Then we'll use\ntools from the OMNeT++ IDE to analyse the output files.\n\n\nFor that, we add an output vector object (which will record the data into\n\nTictoc15-#0.vec\n) and a histogram object (which also calculates mean, etc)\nto the class.\n\n\n\n\n\nWhen a message arrives at the destination node, we update the statistics.\nThe following code has been added to \nhandleMessage()\n:\n\n\n\n\n\nThe \nhopCountVector.record()\n call writes the data into \nTictoc15-#0.vec\n.\nWith a large simulation model or long execution time, the \nTictoc15-#0.vec\n file\nmay grow very large. To handle this situation, you can specifically\ndisable/enable vector in omnetpp.ini, and you can also specify\na simulation time interval in which you're interested\n(data recorded outside this interval will be discarded.)\n\n\nWhen you begin a new simulation, the existing \nTictoc15-#0.vec/sca\n\nfiles get deleted.\n\n\nScalar data (collected by the histogram object in this simulation)\nhave to be recorded manually, in the \nfinish()\n function.\n\nfinish()\n is invoked on successful completion of the simulation,\ni.e. not when it's stopped with an error. The \nrecordScalar()\n calls\nin the code below write into the \nTictoc15-#0.sca\n file.\n\n\n\n\n\nThe files are stored in the \nresults/\n subdirectory.\n\n\nYou can also view the data during simulation. To do that, right click on a module, and\nchoose \nOpen Details\n. In the module inspector's \nContents\n page you'll find the \nhopCountStats\n\nand \nhopCountVector\n objects. To open their inspectors, right click on \ncLongHistogram hopCountStats\n or\n\ncOutVector HopCount\n, and click \nOpen Graphical View\n.\n\n\n\n\nThe inspector:\n\n\n\n\nThey will be initially empty -- run the simulation in \nFast\n (or even \nExpress\n)\nmode to get enough data to be displayed. After a while you'll get something like this:\n\n\n\n\n\n\nWhen you think enough data has been collected, you can stop the simulation\nand then we'll analyse the result files (\nTictoc15-#0.vec\n and\n\nTictoc15-#0.sca\n) off-line. You'll need to choose \nSimulate -\n Call finish()\n\nfrom the menu (or click the corresponding toolbar button) before exiting --\nthis will cause the \nfinish()\n functions to run and data to be written into\n\nTictoc15-#0.sca\n.\n\n\nSources: \n, \n, \n, \n\n\n5.3 Statistic collection without modifying your model\n\n\nIn the previous step we have added statistic collection to our model.\nWhile we can compute and save any value we wish, usually it is not known\nat the time of writing the model, what data the enduser will need.\n\n\nOMNeT++ provides an additional mechanism to record values and events.\nAny model can emit \nsignals\n that can carry a value or an object. The\nmodel writer just have to decide what signals to emit, what data to attach\nto them and when to emit them. The enduser can attach 'listeners' to these\nsignals that can process or record these data items. This way the model\ncode does not have to contain any code that is specific to the statistics\ncollection and the enduser can freely add additional statistics without\neven looking into the C++ code.\n\n\nWe will re-write the statistic collection introduced in the last step to\nuse signals. First of all, we can safely remove all statistic related variables\nfrom our module. There is no need for the \ncOutVector\n and\n\ncLongHistogram\n classes either. We will need only a single signal\nthat carries the \nhopCount\n of the message at the time of message\narrival at the destination.\n\n\nFirst we need to define our signal. The \narrivalSignal\n is just an\nidentifier that can be used later to easily refer to our signal.\n\n\n\n\n\nWe must register all signals before using them. The best place to do this\nis the \ninitialize()\n method of the module.\n\n\n\n\n\nNow we can emit our signal, when the message has arrived to the destination node.\n\n\n\n\n\nAs we do not have to save or store anything manually, the \nfinish()\n method\ncan be deleted. We no longer need it.\n\n\nThe last step is that we have to define the emitted signal also in the NED file.\nDeclaring signals in the NED file allows you to have all information about your\nmodule in one place. You will see the parameters it takes, its input and output\ngates, and also the signals and statistics it provides.\n\n\n\n\n\nNow we can define also a statistic that should be collected by default. Our previous example\nhas collected statistics (max, min, mean, count, etc.) about the hop count of the\narriving messages, so let's collect the same data here, too.\n\n\nThe \nsource\n key specifies the signal we want our statistic to attach to.\nThe \nrecord\n key can be used to tell what should be done with the received\ndata. In our case we specify that each value must be saved in a vector file (vector)\nand also we need to calculate min,max,mean,count etc. (stats). (NOTE: \nstats\n is\njust a shorthand for min, max, mean, sum, count, etc.) With this step we have finished\nour model.\n\n\nNow we have just realized that we would like to see a histogram of the \nhopCount\n on the\n\ntic[1]\n module. On the other hand we are short on disk storage and we are not interested\nhaving the vector data for the first three module \ntic\n 0,1,2. No problem. We can add our\nhistogram and remove the unneeded vector recording without even touching the C++ or NED\nfiles. Just open the INI file and modify the statistic recording:\n\n\n\n\n\nWe can configure a wide range of statistics without even looking into the C++ code,\nprovided that the original model emits the necessary signals for us.\n\n\nSources: \n, \n, \n, \n\n\n5.4 Adding figures\n\n\nOMNeT++ can display figures on the canvas, such as text, geometric shapes or images.\nThese figures can be static, or change dynamically according to what happens in the simulation.\nIn this case, we will display a static descriptive text, and a dynamic text showing the hop count of the last message that arrived at its destination.\n\n\nWe create figures in \n, with the \n@figure\n property.\n\n\n\n\n\nThis creates two text figures named \ndescription\n and \nlasthopcount\n, and sets their positions on the canvas (we place them in the top right corner).\nThe \nfont\n attribute sets the figure text's font. It has three parameters: \ntypeface, size, style\n. Any one of them\ncan be omitted to leave the parameter at default. Here we set the description figure's font to bold.\n\n\nBy default the text in \nlasthopcount\n is static, but we'll\nchange it when a message arrives. This is done in \n, in the \nhandleMessage()\n function.\n\n\n\n\n\nThe figure is represented by the \ncTextFigure\n C++ class. There are several figure types,\nall of them are subclassed from the \ncFigure\n base class.\nWe insert the code responsible for updating the figure text after we retreive the \nhopcount\n variable.\n\n\nWe want to draw the figures on the network's canvas. The \ngetParentModule()\n function returns the parent of the node, ie. the network.\nThen the \ngetCanvas()\n function returns the network's canvas, and \ngetFigure()\n gets the figure by name.\nThen, we update the figure's text with the \nsetText()\n function.\n\n\n\n\nTip\n\n\nFor more information on figures and the canvas, see \nThe Canvas\n section of the OMNeT++ manual\n\n\n\n\nWhen you run the simulation, the figure displays 'last hopCount: N/A' before the arrival of the first message.\nThen, it is updated whenever a message arrives at its destination.\n\n\n\n\n\n\nTip\n\n\nIf the figure text and nodes overlap, press 're-layout'.\n\n\n\n\n\nIn the last few steps, we have collected and displayed statistics. In the next part,\nwe'll see and analyze them in the IDE.\n\n\nSources: \n, \n, \n,", 
            "title": "Adding Statistics Collection"
        }, 
        {
            "location": "/tutorials/tictoc/part5/#part-5-adding-statistics-collection", 
            "text": "", 
            "title": "Part 5 - Adding Statistics Collection"
        }, 
        {
            "location": "/tutorials/tictoc/part5/#51-displaying-the-number-of-packets-sentreceived", 
            "text": "To get an overview at runtime how many messages each node sent or\nreceived, we've added two counters to the module class: numSent and numReceived.   They are set to zero and  WATCH 'ed in the  initialize()  method. Now we\ncan use the  Find/inspect  objects dialog ( Inspect  menu; it is also on\nthe toolbar) to learn how many packets were sent or received by the\nvarious nodes.   It's true that in this concrete simulation model the numbers will be\nroughly the same, so you can only learn from them that  intuniform() \nworks properly. But in real-life simulations it can be very useful that\nyou can quickly get an overview about the state of various nodes in the\nmodel.  It can be also arranged that this info appears above the module\nicons. The  t=  display string tag specifies the text;\nwe only need to modify the displays string during runtime.\nThe following code does the job:   And the result looks like this:   Sources:  ,  ,  ,", 
            "title": "5.1 Displaying the number of packets sent/received"
        }, 
        {
            "location": "/tutorials/tictoc/part5/#52-adding-statistics-collection", 
            "text": "The previous simulation model does something interesting enough\nso that we can collect some statistics. For example, you may be interested\nin the average hop count a message has to travel before reaching\nits destination.  We'll record in the hop count of every message upon arrival into\nan output vector (a sequence of (time,value) pairs, sort of a time series).\nWe also calculate mean, standard deviation, minimum, maximum values per node, and\nwrite them into a file at the end of the simulation. Then we'll use\ntools from the OMNeT++ IDE to analyse the output files.  For that, we add an output vector object (which will record the data into Tictoc15-#0.vec ) and a histogram object (which also calculates mean, etc)\nto the class.   When a message arrives at the destination node, we update the statistics.\nThe following code has been added to  handleMessage() :   The  hopCountVector.record()  call writes the data into  Tictoc15-#0.vec .\nWith a large simulation model or long execution time, the  Tictoc15-#0.vec  file\nmay grow very large. To handle this situation, you can specifically\ndisable/enable vector in omnetpp.ini, and you can also specify\na simulation time interval in which you're interested\n(data recorded outside this interval will be discarded.)  When you begin a new simulation, the existing  Tictoc15-#0.vec/sca \nfiles get deleted.  Scalar data (collected by the histogram object in this simulation)\nhave to be recorded manually, in the  finish()  function. finish()  is invoked on successful completion of the simulation,\ni.e. not when it's stopped with an error. The  recordScalar()  calls\nin the code below write into the  Tictoc15-#0.sca  file.   The files are stored in the  results/  subdirectory.  You can also view the data during simulation. To do that, right click on a module, and\nchoose  Open Details . In the module inspector's  Contents  page you'll find the  hopCountStats \nand  hopCountVector  objects. To open their inspectors, right click on  cLongHistogram hopCountStats  or cOutVector HopCount , and click  Open Graphical View .   The inspector:   They will be initially empty -- run the simulation in  Fast  (or even  Express )\nmode to get enough data to be displayed. After a while you'll get something like this:    When you think enough data has been collected, you can stop the simulation\nand then we'll analyse the result files ( Tictoc15-#0.vec  and Tictoc15-#0.sca ) off-line. You'll need to choose  Simulate -  Call finish() \nfrom the menu (or click the corresponding toolbar button) before exiting --\nthis will cause the  finish()  functions to run and data to be written into Tictoc15-#0.sca .  Sources:  ,  ,  ,", 
            "title": "5.2 Adding statistics collection"
        }, 
        {
            "location": "/tutorials/tictoc/part5/#53-statistic-collection-without-modifying-your-model", 
            "text": "In the previous step we have added statistic collection to our model.\nWhile we can compute and save any value we wish, usually it is not known\nat the time of writing the model, what data the enduser will need.  OMNeT++ provides an additional mechanism to record values and events.\nAny model can emit  signals  that can carry a value or an object. The\nmodel writer just have to decide what signals to emit, what data to attach\nto them and when to emit them. The enduser can attach 'listeners' to these\nsignals that can process or record these data items. This way the model\ncode does not have to contain any code that is specific to the statistics\ncollection and the enduser can freely add additional statistics without\neven looking into the C++ code.  We will re-write the statistic collection introduced in the last step to\nuse signals. First of all, we can safely remove all statistic related variables\nfrom our module. There is no need for the  cOutVector  and cLongHistogram  classes either. We will need only a single signal\nthat carries the  hopCount  of the message at the time of message\narrival at the destination.  First we need to define our signal. The  arrivalSignal  is just an\nidentifier that can be used later to easily refer to our signal.   We must register all signals before using them. The best place to do this\nis the  initialize()  method of the module.   Now we can emit our signal, when the message has arrived to the destination node.   As we do not have to save or store anything manually, the  finish()  method\ncan be deleted. We no longer need it.  The last step is that we have to define the emitted signal also in the NED file.\nDeclaring signals in the NED file allows you to have all information about your\nmodule in one place. You will see the parameters it takes, its input and output\ngates, and also the signals and statistics it provides.   Now we can define also a statistic that should be collected by default. Our previous example\nhas collected statistics (max, min, mean, count, etc.) about the hop count of the\narriving messages, so let's collect the same data here, too.  The  source  key specifies the signal we want our statistic to attach to.\nThe  record  key can be used to tell what should be done with the received\ndata. In our case we specify that each value must be saved in a vector file (vector)\nand also we need to calculate min,max,mean,count etc. (stats). (NOTE:  stats  is\njust a shorthand for min, max, mean, sum, count, etc.) With this step we have finished\nour model.  Now we have just realized that we would like to see a histogram of the  hopCount  on the tic[1]  module. On the other hand we are short on disk storage and we are not interested\nhaving the vector data for the first three module  tic  0,1,2. No problem. We can add our\nhistogram and remove the unneeded vector recording without even touching the C++ or NED\nfiles. Just open the INI file and modify the statistic recording:   We can configure a wide range of statistics without even looking into the C++ code,\nprovided that the original model emits the necessary signals for us.  Sources:  ,  ,  ,", 
            "title": "5.3 Statistic collection without modifying your model"
        }, 
        {
            "location": "/tutorials/tictoc/part5/#54-adding-figures", 
            "text": "OMNeT++ can display figures on the canvas, such as text, geometric shapes or images.\nThese figures can be static, or change dynamically according to what happens in the simulation.\nIn this case, we will display a static descriptive text, and a dynamic text showing the hop count of the last message that arrived at its destination.  We create figures in  , with the  @figure  property.   This creates two text figures named  description  and  lasthopcount , and sets their positions on the canvas (we place them in the top right corner).\nThe  font  attribute sets the figure text's font. It has three parameters:  typeface, size, style . Any one of them\ncan be omitted to leave the parameter at default. Here we set the description figure's font to bold.  By default the text in  lasthopcount  is static, but we'll\nchange it when a message arrives. This is done in  , in the  handleMessage()  function.   The figure is represented by the  cTextFigure  C++ class. There are several figure types,\nall of them are subclassed from the  cFigure  base class.\nWe insert the code responsible for updating the figure text after we retreive the  hopcount  variable.  We want to draw the figures on the network's canvas. The  getParentModule()  function returns the parent of the node, ie. the network.\nThen the  getCanvas()  function returns the network's canvas, and  getFigure()  gets the figure by name.\nThen, we update the figure's text with the  setText()  function.   Tip  For more information on figures and the canvas, see  The Canvas  section of the OMNeT++ manual   When you run the simulation, the figure displays 'last hopCount: N/A' before the arrival of the first message.\nThen, it is updated whenever a message arrives at its destination.    Tip  If the figure text and nodes overlap, press 're-layout'.   In the last few steps, we have collected and displayed statistics. In the next part,\nwe'll see and analyze them in the IDE.  Sources:  ,  ,  ,", 
            "title": "5.4 Adding figures"
        }, 
        {
            "location": "/tutorials/tictoc/part6/", 
            "text": "Part 6 - Visualizing the Results With the IDE\n\n\n6.1 Visualizing output scalars and vectors\n\n\nThe OMNeT++ IDE can help you to analyze your results. It supports filtering,\nprocessing and displaying vector and scalar data, and can display histograms, too.\nThe following diagrams have been created with the Result Analysis tool of the IDE.\n\n\nThe \nresults\n directory in the project folder contains .vec and .sca files, which are the files that store the results in vector and scalar form, respectively.\nVectors record data values as a function of time, while scalars typically record aggregate values at the end of the simulation.\nTo open the Result Analysis tool, double click on either the .vec or the .sca files in the OMNeT++ IDE. Both files will be loaded by the Result Analysis tool.\nYou can find the \nBrowse Data\n tab at the bottom of the Result Analysis tool panel. Here you can browse results by type by switching the various tabs\nat the top of the tool panel, ie. Scalars, Vectors, or Histograms. By default, all results of a result type are displayed. You can filter them by the module filter\nto view all or some of the individual modules, or the statistic name filter to display different types of statistics, ie. mean, max, min, standard deviation, etc.\nYou can select some or all of the individual results by highlighting them. If you select multiple results, they will be plotted on one chart. Right click and select Plot to display the figures.\n\n\n\n\n\n\nTip\n\n\nFor further information about the charting and processing capabilities,\nplease refer to the \nOMNeT++ Users Guide\n (you can find it in the \ndoc\n directory of the OMNeT++ installation).\n\n\n\n\nOur last model records the \nhopCount\n of a message each time the message\nreaches its destination.\nTo plot these vectors for all nodes, select the 6 nodes in the browse data tab.\nRight click and select Plot.\n\n\n\n\nWe can change various options about how the data on the chart is displayed.\nRight click on the chart background, and select Properties.\nThis opens the \nEdit LineChart\n window.\nIn the \nLines\n tab, set \nLine type\n to \nDots\n, and \nSymbol Type\n to \nDot\n.\n\n\n\n\nTo add a legend to the chart, select \nDisplay legend\n on the \nLegend\n tab.\n\n\n\n\nThe chart looks like the following:\n\n\n\n\nIf we apply a \nmean\n operation we can see how the \nhopCount\n in the different\nnodes converge to an average.\nRight-click the chart, and select \nApply -\n Mean\n.\nAgain, right-click on the chart background, and select \nProperties\n.\nIn the \nLines\n tab, set \nLine type\n to Linear, and \nSymbol Type\n to None.\nThe mean is displayed on the following chart. The lines are easier to see this way because they are thinner.\n\n\n\n\nScalar data can be plotted on bar charts.\nThe next chart displays the mean and the maximum of the \nhopCount\n of the messages\nfor each destination node, based on the scalar data recorded at the end of the simulation.\nIn the \nBrowse data\n tab, select \nScalars\n. Now select \nhop count:max\n and \nhop count:mean\n\nfor all 6 nodes.\n\n\n\n\nTo create a histogram that shows \nhopCount\n's distribution, select \nHistograms\n\non the \nBrowse data\n tab. Select all nodes, and right click \nPlot\n.", 
            "title": "Visualizing the Results"
        }, 
        {
            "location": "/tutorials/tictoc/part6/#part-6-visualizing-the-results-with-the-ide", 
            "text": "", 
            "title": "Part 6 - Visualizing the Results With the IDE"
        }, 
        {
            "location": "/tutorials/tictoc/part6/#61-visualizing-output-scalars-and-vectors", 
            "text": "The OMNeT++ IDE can help you to analyze your results. It supports filtering,\nprocessing and displaying vector and scalar data, and can display histograms, too.\nThe following diagrams have been created with the Result Analysis tool of the IDE.  The  results  directory in the project folder contains .vec and .sca files, which are the files that store the results in vector and scalar form, respectively.\nVectors record data values as a function of time, while scalars typically record aggregate values at the end of the simulation.\nTo open the Result Analysis tool, double click on either the .vec or the .sca files in the OMNeT++ IDE. Both files will be loaded by the Result Analysis tool.\nYou can find the  Browse Data  tab at the bottom of the Result Analysis tool panel. Here you can browse results by type by switching the various tabs\nat the top of the tool panel, ie. Scalars, Vectors, or Histograms. By default, all results of a result type are displayed. You can filter them by the module filter\nto view all or some of the individual modules, or the statistic name filter to display different types of statistics, ie. mean, max, min, standard deviation, etc.\nYou can select some or all of the individual results by highlighting them. If you select multiple results, they will be plotted on one chart. Right click and select Plot to display the figures.    Tip  For further information about the charting and processing capabilities,\nplease refer to the  OMNeT++ Users Guide  (you can find it in the  doc  directory of the OMNeT++ installation).   Our last model records the  hopCount  of a message each time the message\nreaches its destination.\nTo plot these vectors for all nodes, select the 6 nodes in the browse data tab.\nRight click and select Plot.   We can change various options about how the data on the chart is displayed.\nRight click on the chart background, and select Properties.\nThis opens the  Edit LineChart  window.\nIn the  Lines  tab, set  Line type  to  Dots , and  Symbol Type  to  Dot .   To add a legend to the chart, select  Display legend  on the  Legend  tab.   The chart looks like the following:   If we apply a  mean  operation we can see how the  hopCount  in the different\nnodes converge to an average.\nRight-click the chart, and select  Apply -  Mean .\nAgain, right-click on the chart background, and select  Properties .\nIn the  Lines  tab, set  Line type  to Linear, and  Symbol Type  to None.\nThe mean is displayed on the following chart. The lines are easier to see this way because they are thinner.   Scalar data can be plotted on bar charts.\nThe next chart displays the mean and the maximum of the  hopCount  of the messages\nfor each destination node, based on the scalar data recorded at the end of the simulation.\nIn the  Browse data  tab, select  Scalars . Now select  hop count:max  and  hop count:mean \nfor all 6 nodes.   To create a histogram that shows  hopCount 's distribution, select  Histograms \non the  Browse data  tab. Select all nodes, and right click  Plot .", 
            "title": "6.1 Visualizing output scalars and vectors"
        }, 
        {
            "location": "/tutorials/tictoc/part7/", 
            "text": "Part 7 - Parameter Studies\n\n\n7.1 The goal\n\n\nWe want to run the simulation with a different number of nodes, and see how\nthe behavior of the network changes. With OMNeT++ you can do parameter studies,\nwhich are multiple simulation runs with different parameter values.\n\n\nWe'll make the number of central nodes (the \"handle\" in the dumbbell shape) a parameter, and\nuse the same random routing protocol as before. We're interested in how the average\nhop count depends on the number of nodes.\n\n\n7.2 Making the network topology parametric\n\n\nTo parameterize the network, the number of nodes is given as a NED parameter,\n\nnumCentralNodes\n. This parameter specifies how many nodes are in the central\nsection of the network, but doesn't cover the two nodes at each side.\n\n\n\n\nThe total number of nodes including the four nodes on the sides is \nnumCentralNodes+4\n.\nThe default of the \nnumCentralNodes\n parameter is 2, this corresponds\nto the network in the previous step.\n\n\n\n\n\nNow, we must specify that the variable number of nodes should be connected into the dumbbell shape.\nFirst, the two nodes on one side is connected to the third one. Then the the last two nodes on the other side is\nconnected to the third last. The nodes in the center of the dumbbell can be connected with a for loop.\nStarting from the third, each *i*th node is connected to the *i+1*th.\n\n\n\n\n\nHere is how the network looks like with \nnumCentralNodes = 4\n:\n\n\n\n\nTo run the simulation with multiple different values of \nnumCentralNodes\n, we specify\nthe variable \nN\n in the ini file:\n\n\n\n\n\n7.3 Setting up a parameter study\n\n\nWe specify that \nN\n should go from 2 to 100, in steps of 2.\nThis produces about 50 simulation runs. Each can be explored in the graphical user interface, but\nsimulation batches are often run from the command line interface using the \nCmdenv\n runtime environment.\n\n\n\n\nTip\n\n\nYou can find more information on variables and parameter studies in the \nParameter Studies\n section of the OMNeT++ manual.\n\n\n\n\nTo increase the accuracy of the simulation we may need to run the same simulation several times\nusing different random numbers. These runs are called \nRepetitions\n and are specified in \nomnetpp.ini\n:\n\n\n\n\n\nThis means that each simulation run will be executed four times, each time with a different seed for the RNGs.\nThis produces more samples, which can be averaged. With more repetitions, the results will increasingly converge\nto the expected values.\n\n\n7.4 Running the parameter study\n\n\nNow, we can run the simulations. In the dropdown menu of the \nRun\n icon, select \nRun Configurations\n.\n\n\n\n\nIn the \nRun Configurations\n dialog, select the config name, make sure \nCmdenv\n is selected as the user interface.\n\n\n\n\nIf you have a multicore CPU, you can specify how many simulations to run concurrently.\n\n\n\n\nNote\n\n\nAlternatively, you can run the simulation batches from the command line with \nopp_runall\n tool with the following command:\n\n\n\n\n\nopp_runall -j4 ./tictoc -u Cmdenv -c TicToc18\n\n\n\n\nThe \n-j\n parameter specifies the number of CPU cores, the \n-u\n parameter the user interface, and \n-c\n the config to run.\n\n\n7.5 Analyzing the results\n\n\nNow, we can visualize and analyze the data we've collected from the simulation runs.\nWe'll display the average hop count for messages that reach their destinations vs \nN\n, the number of central nodes.\nAdditionally, we will display the average number of packets that reached their destinations vs \nN\n.\nThe analysis file \nTictoc18.anf\n contains the dataset we will use for the visualization.\n\n\n\n\nThese two average scalars are not recorded during the simulation, we will have to compute them from the available data.\n\n\nThe hop count is recorded at each node when a message arrives, so the mean of hop count will be available as a statistic.\nBut this is recorded per node, and we're interested in the average of the mean hop count for all nodes.\nThe 'Compute Scalar' dataset node can be used to compute scalar statistics from other scalars.\nWe compute \nAvgHopCount\n as \nmean(**.\nhopCount:stats:mean\n)\n.\n\n\nWe're also interested in the average number of packets that arrive at their destination.\nThe count of the arrived packets is available at each node. We can compute their average,\n\nAvgNumPackets\n as \nmean(\nhopCount:stats:count\n)\n.\n\n\n\n\nTip\n\n\nRefer to the chapter \"Using the Analysis Editor\" in the \nUser Guide\n for more information on datasets. You can find it in the \ndoc/\n directory of your OMNeT++ installation.\n\n\n\n\nThen, we plot these two computed scalars against \nN\n in two scatter charts. The data for different repetitions is automatically averaged.\nHere is the average hop count vs \nN\n:\n\n\n\n\nThe average hop count increases as the network gets larger, as packets travel more to reach their destination.\nThe increase is polynomial. Notice that there are missing values at the far right of the chart.\nThis is because in such a large network, some packets might not reach their destination in the simulation time limit.\nWhen no packets arrive at a node, the hop count statistic will be \nNaN\n (not a number) for that node.\nWhen there is a \nNaN\n in any mathematical expression, its result will be also \nNaN\n.\nThus it takes just one node in all the simulation runs to have a \nNaN\n statistic, and the average will be \nNaN\n, and there'll be no data to display.\nThis can be remedied by increasing the simulation time limit, so more packets have a chance to arrive.\n\n\nBelow is the average number of packets that arrived vs \nN\n:\n\n\n\n\nNotice that the Y axis is logarithmic. The average number of packets that arrive decreases polynomially\nas \nN\n increases, and the network gets larger.", 
            "title": "Parameter Studies"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#part-7-parameter-studies", 
            "text": "", 
            "title": "Part 7 - Parameter Studies"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#71-the-goal", 
            "text": "We want to run the simulation with a different number of nodes, and see how\nthe behavior of the network changes. With OMNeT++ you can do parameter studies,\nwhich are multiple simulation runs with different parameter values.  We'll make the number of central nodes (the \"handle\" in the dumbbell shape) a parameter, and\nuse the same random routing protocol as before. We're interested in how the average\nhop count depends on the number of nodes.", 
            "title": "7.1 The goal"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#72-making-the-network-topology-parametric", 
            "text": "To parameterize the network, the number of nodes is given as a NED parameter, numCentralNodes . This parameter specifies how many nodes are in the central\nsection of the network, but doesn't cover the two nodes at each side.   The total number of nodes including the four nodes on the sides is  numCentralNodes+4 .\nThe default of the  numCentralNodes  parameter is 2, this corresponds\nto the network in the previous step.   Now, we must specify that the variable number of nodes should be connected into the dumbbell shape.\nFirst, the two nodes on one side is connected to the third one. Then the the last two nodes on the other side is\nconnected to the third last. The nodes in the center of the dumbbell can be connected with a for loop.\nStarting from the third, each *i*th node is connected to the *i+1*th.   Here is how the network looks like with  numCentralNodes = 4 :   To run the simulation with multiple different values of  numCentralNodes , we specify\nthe variable  N  in the ini file:", 
            "title": "7.2 Making the network topology parametric"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#73-setting-up-a-parameter-study", 
            "text": "We specify that  N  should go from 2 to 100, in steps of 2.\nThis produces about 50 simulation runs. Each can be explored in the graphical user interface, but\nsimulation batches are often run from the command line interface using the  Cmdenv  runtime environment.   Tip  You can find more information on variables and parameter studies in the  Parameter Studies  section of the OMNeT++ manual.   To increase the accuracy of the simulation we may need to run the same simulation several times\nusing different random numbers. These runs are called  Repetitions  and are specified in  omnetpp.ini :   This means that each simulation run will be executed four times, each time with a different seed for the RNGs.\nThis produces more samples, which can be averaged. With more repetitions, the results will increasingly converge\nto the expected values.", 
            "title": "7.3 Setting up a parameter study"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#74-running-the-parameter-study", 
            "text": "Now, we can run the simulations. In the dropdown menu of the  Run  icon, select  Run Configurations .   In the  Run Configurations  dialog, select the config name, make sure  Cmdenv  is selected as the user interface.   If you have a multicore CPU, you can specify how many simulations to run concurrently.   Note  Alternatively, you can run the simulation batches from the command line with  opp_runall  tool with the following command:   \nopp_runall -j4 ./tictoc -u Cmdenv -c TicToc18  The  -j  parameter specifies the number of CPU cores, the  -u  parameter the user interface, and  -c  the config to run.", 
            "title": "7.4 Running the parameter study"
        }, 
        {
            "location": "/tutorials/tictoc/part7/#75-analyzing-the-results", 
            "text": "Now, we can visualize and analyze the data we've collected from the simulation runs.\nWe'll display the average hop count for messages that reach their destinations vs  N , the number of central nodes.\nAdditionally, we will display the average number of packets that reached their destinations vs  N .\nThe analysis file  Tictoc18.anf  contains the dataset we will use for the visualization.   These two average scalars are not recorded during the simulation, we will have to compute them from the available data.  The hop count is recorded at each node when a message arrives, so the mean of hop count will be available as a statistic.\nBut this is recorded per node, and we're interested in the average of the mean hop count for all nodes.\nThe 'Compute Scalar' dataset node can be used to compute scalar statistics from other scalars.\nWe compute  AvgHopCount  as  mean(**. hopCount:stats:mean ) .  We're also interested in the average number of packets that arrive at their destination.\nThe count of the arrived packets is available at each node. We can compute their average, AvgNumPackets  as  mean( hopCount:stats:count ) .   Tip  Refer to the chapter \"Using the Analysis Editor\" in the  User Guide  for more information on datasets. You can find it in the  doc/  directory of your OMNeT++ installation.   Then, we plot these two computed scalars against  N  in two scatter charts. The data for different repetitions is automatically averaged.\nHere is the average hop count vs  N :   The average hop count increases as the network gets larger, as packets travel more to reach their destination.\nThe increase is polynomial. Notice that there are missing values at the far right of the chart.\nThis is because in such a large network, some packets might not reach their destination in the simulation time limit.\nWhen no packets arrive at a node, the hop count statistic will be  NaN  (not a number) for that node.\nWhen there is a  NaN  in any mathematical expression, its result will be also  NaN .\nThus it takes just one node in all the simulation runs to have a  NaN  statistic, and the average will be  NaN , and there'll be no data to display.\nThis can be remedied by increasing the simulation time limit, so more packets have a chance to arrive.  Below is the average number of packets that arrived vs  N :   Notice that the Y axis is logarithmic. The average number of packets that arrive decreases polynomially\nas  N  increases, and the network gets larger.", 
            "title": "7.5 Analyzing the results"
        }, 
        {
            "location": "/tutorials/tictoc/conclusion/", 
            "text": "Closing words\n\n\nCongratulations!\n\n\nYou have successfully completed this tutorial! You have gained a good overview\nand the basic skills to work with OMNeT++, from writing simulations to analyzing\nresults. To go to the next level, we recommend you to read the \nSimulation Manual\n\nand skim through the \nUser Guide\n.\n\n\nComments and suggestions regarding this tutorial will be very much appreciated.", 
            "title": "Closing words"
        }, 
        {
            "location": "/tutorials/tictoc/conclusion/#closing-words", 
            "text": "", 
            "title": "Closing words"
        }, 
        {
            "location": "/tutorials/tictoc/conclusion/#congratulations", 
            "text": "You have successfully completed this tutorial! You have gained a good overview\nand the basic skills to work with OMNeT++, from writing simulations to analyzing\nresults. To go to the next level, we recommend you to read the  Simulation Manual \nand skim through the  User Guide .  Comments and suggestions regarding this tutorial will be very much appreciated.", 
            "title": "Congratulations!"
        }, 
        {
            "location": "/tutorials/pandas/", 
            "text": "1. When to use Python?\n\n\nThe Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of\nsimulation results. If you are doing sophisticated result analysis, you will\nnotice after a while that you have outgrown the IDE. The need for customized\ncharts, the necessity of multi-step computations to produce chart input, or the\nsheer volume of raw simulation results might all be causes to make you look for\nsomething else.\n\n\nIf you are an R or Matlab expert, you'll probably reach for those tools, but for\neveryone else, Python with the right libraries is pretty much the best choice.\nPython has a big momentum for data science, and in addition to having excellent\nlibraries for data analysis and visualization, it is also a great general-purpose\nprogramming language. Python is used for diverse problems ranging from building\ndesktop GUIs to machine learning and AI, so the knowledge you gain by learning\nit will be convertible to other areas.\n\n\nThis tutorial will walk you through the initial steps of using Python for\nanalysing simulation results, and shows how to do some of the most common tasks.\nThe tutorial assumes that you have a working knowledge of OMNeT++ with regard\nto result recording, and basic familiarity with Python.\n\n\n2. Setting up\n\n\nBefore we can start, you need to install the necessary software.\nFirst, make sure you have Python, either version 2.x or 3.x (they are\nslightly incompatible.) If you have both versions available on your system,\nwe recommend version 3.x. You also need OMNeT++ version 5.2 or later.\n\n\nWe will heavily rely on three Python packages: \nNumPy\n,\n\nPandas\n, and \nMatplotlib\n.\nThere are also optional packages that will be useful for certain tasks:\n\nSciPy\n,\n\nPivotTable.js\n.\nWe also recommend that you install \nIPython\n and\n\nJupyter\n, because they let you work much more comfortably\nthan the bare Python shell.\n\n\nOn most systems, these packages can be installed with \npip\n, the Python package\nmanager (if you go for Python 3, replace \npip\n with \npip3\n in the commands\nbelow):\n\n\nsudo pip install ipython jupyter\nsudo pip install numpy pandas matplotlib\nsudo pip install scipy pivottablejs\n\n\n\n\n\nAs packages continually evolve, there might be incompatibilities between\nversions. We used the following versions when writing this tutorial:\nPandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0.\nAn easy way to determine which versions you have installed is using the \npip list\n\ncommand. (Note that the last one is the version of the Python interface library,\nthe PivotTable.js main Javascript library uses different version numbers, e.g.\n2.7.0.)\n\n\n3. Getting your simulation results into Python\n\n\nOMNeT++ result files have their own file format which is not directly\ndigestible by Python. There are a number of ways to get your data\ninside Python:\n\n\n\n\n\n\nExport from the IDE. The Analysis Tool can export data in a number of\n  formats, the ones that are useful here are CSV and Python-flavoured JSON.\n  In this tutorial we'll use the CSV export, and read the result into Pandas\n  using its \nread_csv()\n function.\n\n\n\n\n\n\nExport using scavetool. Exporting from the IDE may become tedious\n  after a while, because you have to go through the GUI every time your\n  simulations are re-run. Luckily, you can automate the exporting with\n  OMNeT++'s scavetool program. scavetool exposes the same export\n  functionality as the IDE, and also allows filtering of the data.\n\n\n\n\n\n\nRead the OMNeT++ result files directly from Python. Development\n  of a Python package to read these files into Pandas data frames is\n  underway, but given that these files are line-oriented text files\n  with a straightforward and well-documented structure, writing your\n  own custom reader is also a perfectly feasible option.\n\n\n\n\n\n\nSQLite. Since version 5.1, OMNeT++ has the ability to record simulation\n  results int SQLite3 database files, which can be opened directly from\n  Python using the \nsqlite\n\n  package. This lets you use SQL queries to select the input data for your\n  charts or computations, which is kind of cool! You can even use GUIs like\n  \nSQLiteBrowser\n to browse the database and\n  craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3\n  output, you'll still get \n.vec\n and \n.sca\n files as before, only their\n  format will change from textual to SQLite's binary format. When querying\n  the contents of the files, one issue  to deal with is that SQLite does not\n  allow cross-database queries, so you either need to configure OMNeT++\n  to record everything into one file (i.e. each run should append instead\n  of creating a new file), or use scavetool's export functionality to\n  merge the files into one.\n\n\n\n\n\n\nCustom result recording. There is also the option to instrument\n  the simulation (via C++ code) or OMNeT++ (via custom result recorders)\n  to produce files that Python can directly digest, e.g. CSV.\n  However, in the light of the above options, it is rarely necessary\n  to go this far.\n\n\n\n\n\n\nWith large-scale simulation studies, it can easily happen that the\nfull set of simulation results do not fit into the memory at once.\nThere are also multiple approaches to deal with this problem:\n\n\n\n\nIf you don't need all simulation results for the analysis, you can\n  configure OMNeT++ to record only a subset of them. Fine-grained control\n  is available.\n\n\nPerform filtering and aggregation steps before analysis. The IDE and\n  scavetool are both capable of filtering the results before export.\n\n\nWhen the above approaches are not enough, it can help to move\n  part of the result processing (typically, filtering and aggregation)\n  into the simulation model as dedicated result collection modules.\n  However, this solution requires significantly more work than the previous\n  two, so use with care.\n\n\n\n\nIn this tutorial, we'll work with the contents of the \nsamples/resultfiles\n\ndirectory distributed with OMNeT++. The directory contains result\nfiles produced by the Aloha and Routing sample simulations, both\nof which are parameter studies. We'll start by looking at the Aloha results.\n\n\nAs the first step, we use OMNeT++'s \nscavetool\n to convert Aloha's scalar files\nto CSV. Run the following commands in the terminal (replace \n~/omnetpp\n with\nthe location of your OMNeT++ installation):\n\n\ncd ~/omnetpp/samples/resultfiles/aloha\nscavetool x *.sca -o aloha.csv\n\n\n\n\n\nIn the scavetool command line, \nx\n means export, and the export format is\ninferred from the output file's extension. (Note that scavetool supports\ntwo different CSV output formats. We need \nCSV Records\n, or CSV-R for short,\nwhich is the default for the \n.csv\n extension.)\n\n\nLet us spend a minute on what the export has created. The CSV file\nhas a fixed number of columns named \nrun\n, \ntype\n, \nmodule\n, \nname\n,\n\nvalue\n, etc. Each result item, i.e. scalar, statistic, histogram\nand vector, produces one row of output in the CSV. Other items such\nas run attributes, iteration variables of the parameter study and result\nattributes also generate their own rows. The content of the \ntype\n column\ndetermines what type of information a given row contains. The \ntype\n\ncolumn also determines which other columns are in use. For example,\nthe \nbinedges\n and \nbinvalues\n columns are only filled in for histogram\nitems. The colums are:\n\n\n\n\nrun\n: Identifies the simulation run\n\n\ntype\n: Row type, one of the following: \nscalar\n, \nvector\n, \nstatistics\n,\n  \nhistogram\n, \nrunattr\n, \nitervar\n, \nparam\n, \nattr\n\n\nmodule\n: Hierarchical name (a.k.a. full path) of the module that recorded the\n  result item\n\n\nname\n: Name of the result item (scalar, statistic, histogram or vector)\n\n\nattrname\n: Name of the run attribute or result item attribute (in the latter\n  case, the \nmodule\n and \nname\n columns identify the result item the attribute\n  belongs to)\n\n\nattrvalue\n: Value of run and result item attributes, iteration variables,\n  saved ini param settings (\nrunattr\n, \nattr\n, \nitervar\n, \nparam\n)\n\n\nvalue\n: Output scalar value\n\n\ncount\n, \nsumweights\n, \nmean\n, \nmin\n, \nmax\n, \nstddev\n: Fields of the statistics\n  or histogram\n\n\nbinedges\n, \nbinvalues\n: Histogram bin edges and bin values, as space-separated\n  lists. \nlen(binedges)==len(binvalues)+1\n\n\nvectime\n, \nvecvalue\n: Output vector time and value arrays, as space-separated\n  lists\n\n\n\n\nWhen the export is done, you can start Jupyter server with the following command:\n\n\njupyter notebook\n\n\n\n\n\nOpen a web browser with the displayed URL to access the Jupyter GUI. Once there,\nchoose \nNew\n -\n \nPython3\n in the top right corner to open a blank notebook.\nThe notebook allows you to enter Python commands or sequences of commands,\nrun them, and view the output. Note that \nEnter\n simply inserts a newline;\nhit \nCtrl+Enter\n to execute the commands in the current cell, or \nAlt+Enter\n\nto execute them and also insert a new cell below.\n\n\nIf you cannot use Jupyter for some reason, a terminal-based Python shell\n(\npython\n or \nipython\n) will also allow you to follow the tutorial.\n\n\nOn the Python prompt, enter the following lines to make the functionality of\nPandas, NumpPy and Matplotlib available in the session. The last, \n%matplotlib\n\nline is only needed for Jupyter. (It is a \"magic command\" that arranges plots\nto be displayed within the notebook.)\n\n\nIn[1]:\n\n\n\nimport\n \npandas\n \nas\n \npd\n\n\nimport\n \nnumpy\n \nas\n \nnp\n\n\nimport\n \nmatplotlib.pyplot\n \nas\n \nplt\n\n\n%\nmatplotlib\n \ninline\n\n\n\n\n\nWe utilize the \nread_csv()\n function to import the contents of the\nCSV file into a data frame. The data frame is the central concept of\nPandas. We will continue to work with this data frame throughout\nthe whole tutorial.\n\n\nIn[2]:\n\n\n\naloha\n \n=\n \npd\n.\nread_csv\n(\naloha.csv\n)\n\n\n\n\n\n4. Exploring the data frame\n\n\nYou can view the contents of the data frame by simply entering the name\nof the variable (\naloha\n). Alternatively, you can use the \nhead()\n method\nof the data frame to view just the first few lines.\n\n\nIn[3]:\n\n\n\naloha\n.\nhead\n()\n\n\n\n\n\nOut[3]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nconfigname\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ndatetime\n\n      \n20170627-20:42:20\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n2\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nexperiment\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n3\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ninifile\n\n      \nomnetpp.ini\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n4\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \niterationvars\n\n      \nnumHosts=10, iaMean=3\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nYou can see that the structure of the data frame, i.e. rows and columns,\ndirectly corresponds to the contents of the CSV file. Column names have\nbeen taken from the first line of the CSV file. Missing values are\nrepresented with NaNs (not-a-number).\n\n\nThe complementary \ntail()\n method shows the last few lines. There is also\nan \niloc\n method that we use at places in this tutorial to show rows\nfrom the middle of the data frame. It accepts a range: \naloha.iloc[20:30]\n\nselects 10 lines from line 20, \naloha.iloc[:5]\n is like \nhead()\n, and\n\naloha.iloc[-5:]\n is like \ntail()\n.\n\n\nIn[4]:\n\n\n\naloha\n.\niloc\n[\n1200\n:\n1205\n]\n\n\n\n\n\nOut[4]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n    \n\n  \n\n  \n\n    \n\n      \n1200\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \nNaN\n\n      \nNaN\n\n      \n40692.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1201\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nattr\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \nsource\n\n      \nsum(collision)\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1202\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nattr\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \ntitle\n\n      \ncollided frames, last\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1203\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.156176\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1204\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nattr\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \ninterpolationmode\n\n      \nlinear\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nHint: If you are in the terminal and you find that the data frame printout does\nnot make use of the whole width of the terminal, you can increase the display\nwidth for better readability with the following commands:\n\n\nIn[5]:\n\n\n\npd\n.\nset_option\n(\ndisplay.width\n,\n \n180\n)\n\n\npd\n.\nset_option\n(\ndisplay.max_colwidth\n,\n \n100\n)\n\n\n\n\n\nIf you have not looked at any Pandas tutorial yet, now is a very good\ntime to read one. (See References at the bottom of this page for hints.)\nUntil you finish, here are some basics for your short-term survival.\n\n\nYou can refer to a column as a whole with the array index syntax: \naloha[\nrun\n]\n.\nAlternatively, the more convenient member access syntax (\naloha.run\n) can\nalso be used, with restrictions. (E.g. the column name must be valid as a Python\nidentifier, and should not collide with existing methods of the data frame.\nNames that are known to cause trouble include \nname\n, \nmin\n, \nmax\n, \nmean\n).\n\n\nIn[6]:\n\n\n\naloha\n.\nrun\n.\nhead\n()\n  \n# .head() is for limiting the output to 5 lines here\n\n\n\n\n\nOut[6]:\n\n\n\n0    PureAlohaExperiment-4-20170627-20:42:20-22739\n1    PureAlohaExperiment-4-20170627-20:42:20-22739\n2    PureAlohaExperiment-4-20170627-20:42:20-22739\n3    PureAlohaExperiment-4-20170627-20:42:20-22739\n4    PureAlohaExperiment-4-20170627-20:42:20-22739\nName: run, dtype: object\n\n\n\n\n\nSelecting multiple columns is also possible, one just needs to use a list of\ncolumn names as index. The result will be another data frame. (The double\nbrackets in the command are due to the fact that both the array indexing and\nthe list syntax use square brackets.)\n\n\nIn[7]:\n\n\n\ntmp\n \n=\n \naloha\n[[\nrun\n,\n \nattrname\n,\n \nattrvalue\n]]\n\n\ntmp\n.\nhead\n()\n\n\n\n\n\nOut[7]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \nattrname\n\n      \nattrvalue\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nconfigname\n\n      \nPureAlohaExperiment\n\n    \n\n    \n\n      \n1\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \ndatetime\n\n      \n20170627-20:42:20\n\n    \n\n    \n\n      \n2\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nexperiment\n\n      \nPureAlohaExperiment\n\n    \n\n    \n\n      \n3\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \ninifile\n\n      \nomnetpp.ini\n\n    \n\n    \n\n      \n4\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \niterationvars\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nThe \ndescribe()\n method can be used to get an idea about the contents of a\ncolumn. When applied to a non-numeric column, it prints the number of\nnon-null elements in it (\ncount\n), the number of unique values (\nunique\n),\nthe most frequently occurring value (\ntop\n) and its multiplicity (\nfreq\n),\nand the inferred data type (more about that later.)\n\n\nIn[8]:\n\n\n\naloha\n.\nmodule\n.\ndescribe\n()\n\n\n\n\n\nOut[8]:\n\n\n\ncount             1012\nunique              11\ntop       Aloha.server\nfreq               932\nName: module, dtype: object\n\n\n\n\n\nYou can get a list of the unique values using the \nunique()\n method. For example,\nthe following command lists the names of modules that have recorded any statistics:\n\n\nIn[9]:\n\n\n\naloha\n.\nmodule\n.\nunique\n()\n\n\n\n\n\nOut[9]:\n\n\n\narray([nan, \nAloha.server\n, \nAloha.host[0]\n, \nAloha.host[1]\n,\n       \nAloha.host[2]\n, \nAloha.host[3]\n, \nAloha.host[4]\n, \nAloha.host[5]\n,\n       \nAloha.host[6]\n, \nAloha.host[7]\n, \nAloha.host[8]\n, \nAloha.host[9]\n], dtype=object)\n\n\n\n\n\nWhen you apply \ndescribe()\n to a numeric column, you get a statistical summary\nwith things like mean, standard deviation, minimum, maximum, and various\nquantiles.\n\n\nIn[10]:\n\n\n\naloha\n.\nvalue\n.\ndescribe\n()\n\n\n\n\n\nOut[10]:\n\n\n\ncount      294.000000\nmean      4900.038749\nstd      11284.077075\nmin          0.045582\n25%          0.192537\n50%        668.925298\n75%       5400.000000\nmax      95630.000000\nName: value, dtype: float64\n\n\n\n\n\nApplying \ndescribe()\n to the whole data frame creates a similar report about\nall numeric columns.\n\n\nIn[11]:\n\n\n\naloha\n.\ndescribe\n()\n\n\n\n\n\nOut[11]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n    \n\n  \n\n  \n\n    \n\n      \ncount\n\n      \n294.000000\n\n      \n84.000000\n\n      \n0.0\n\n      \n84.000000\n\n      \n84.000000\n\n      \n84.000000\n\n      \n84.000000\n\n    \n\n    \n\n      \nmean\n\n      \n4900.038749\n\n      \n5591.380952\n\n      \nNaN\n\n      \n1.489369\n\n      \n0.599396\n\n      \n1.049606\n\n      \n6.560987\n\n    \n\n    \n\n      \nstd\n\n      \n11284.077075\n\n      \n4528.796760\n\n      \nNaN\n\n      \n1.530455\n\n      \n0.962515\n\n      \n0.956102\n\n      \n9.774404\n\n    \n\n    \n\n      \nmin\n\n      \n0.045582\n\n      \n470.000000\n\n      \nNaN\n\n      \n0.152142\n\n      \n0.031326\n\n      \n0.099167\n\n      \n0.272013\n\n    \n\n    \n\n      \n25%\n\n      \n0.192537\n\n      \n1803.000000\n\n      \nNaN\n\n      \n0.164796\n\n      \n0.049552\n\n      \n0.099186\n\n      \n0.498441\n\n    \n\n    \n\n      \n50%\n\n      \n668.925298\n\n      \n4065.500000\n\n      \nNaN\n\n      \n1.197140\n\n      \n0.243035\n\n      \n1.049776\n\n      \n3.084077\n\n    \n\n    \n\n      \n75%\n\n      \n5400.000000\n\n      \n8815.000000\n\n      \nNaN\n\n      \n2.384397\n\n      \n0.741081\n\n      \n2.000000\n\n      \n9.000000\n\n    \n\n    \n\n      \nmax\n\n      \n95630.000000\n\n      \n14769.000000\n\n      \nNaN\n\n      \n6.936747\n\n      \n5.323887\n\n      \n2.000000\n\n      \n54.000000\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nLet's spend a minute on data types and column data types. Every column has a\ndata type (abbreviated \ndtype\n) that determines what type of values it may\ncontain. Column dtypes can be printed with \ndtypes\n:\n\n\nIn[12]:\n\n\n\naloha\n.\ndtypes\n\n\n\n\n\nOut[12]:\n\n\n\nrun            object\ntype           object\nmodule         object\nname           object\nattrname       object\nattrvalue      object\nvalue         float64\ncount         float64\nsumweights    float64\nmean          float64\nstddev        float64\nmin           float64\nmax           float64\nbinedges       object\nbinvalues      object\nvectime        object\nvecvalue       object\ndtype: object\n\n\n\n\n\nThe two most commonly used dtypes are \nfloat64\n and \nobject\n. A \nfloat64\n column\ncontains floating-point numbers, and missing values are represented with NaNs.\nAn \nobject\n column may contain basically anything -- usually strings, but we'll\nalso have NumPy arrays (\nnp.ndarray\n) as elements in this tutorial.\nNumeric values and booleans may also occur in an \nobject\n column. Missing values\nin an \nobject\n column are usually represented with \nNone\n, but Pandas also\ninterprets the floating-point NaN like that.\nSome degree of confusion arises from fact that some Pandas functions check\nthe column's dtype, while others are already happy if the contained elements\nare of the required type. To clarify: applying \ndescribe()\n to a column\nprints a type inferred from the individual elements, \nnot\n the column dtype.\nThe column dtype type can be changed with the \nastype()\n method; we'll see an\nexample for using it later in this tutorial.\n\n\nThe column dtype can be accessed as the \ndtype\n property of a column, for example\n\naloha.stddev.dtype\n yields \ndtype(\nfloat64\n)\n. There are also convenience\nfunctions such as \nis_numeric_dtype()\n and \nis_string_dtype()\n for checking\ncolumn dtype. (They need to be imported from the \npandas.api.types\n package\nthough.)\n\n\nAnother vital thing to know, especially due of the existence of the \ntype\n\ncolumn in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly,\nthe array index syntax can be used here as well. For example, the following expression\nselects the rows that contain iteration variables: \naloha[aloha.type == \nitervar\n]\n.\nWith a healthy degree of sloppiness, here's how it works: \naloha.type\n yields\nthe values in the \ntype\n column as an array-like data structure;\n\naloha.type==\nitervar\n performs element-wise comparison and produces an array\nof booleans containing \nTrue\n where the condition holds and \nFalse\n where not;\nand indexing a data frame with an array of booleans returns the rows that\ncorrespond to \nTrue\n values in the array.\n\n\nConditions can be combined with AND/OR using the \"\n\" and \"\n|\n\" operators, but\nyou need parentheses because of operator precedence. The following command\nselects the rows that contain scalars with a certain name and owner module:\n\n\nIn[13]:\n\n\n\ntmp\n \n=\n \naloha\n[(\naloha\n.\ntype\n==\nscalar\n)\n \n \n(\naloha\n.\nmodule\n==\nAloha.server\n)\n \n \n(\naloha\n.\nname\n==\nchannelUtilization:last\n)]\n\n\ntmp\n.\nhead\n()\n\n\n\n\n\nOut[13]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n    \n\n  \n\n  \n\n    \n\n      \n1186\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.156057\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1203\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.156176\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1220\n\n      \nPureAlohaExperiment-2-20170627-20:42:19-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.196381\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1237\n\n      \nPureAlohaExperiment-3-20170627-20:42:20-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.193253\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n    \n\n      \n1254\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n      \nNaN\n\n      \n0.176507\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nYou'll also need to know how to add a new column to the data frame. Now that is\na bit controversial topic, because at the time of writing, there is a \"convenient\"\nsyntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple\nassignment, for example:\n\n\nIn[14]:\n\n\n\naloha\n[\nqname\n]\n \n=\n \naloha\n.\nmodule\n \n+\n \n.\n \n+\n \naloha\n.\nname\n\n\naloha\n[\naloha\n.\ntype\n==\nscalar\n]\n.\nhead\n()\n  \n# print excerpt\n\n\n\n\n\nOut[14]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n      \nqname\n\n    \n\n  \n\n  \n\n    \n\n      \n1176\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nduration\n\n      \nNaN\n\n      \nNaN\n\n      \n5400.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.duration\n\n    \n\n    \n\n      \n1177\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:mean\n\n      \nNaN\n\n      \nNaN\n\n      \n0.198275\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:mean\n\n    \n\n    \n\n      \n1179\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:sum\n\n      \nNaN\n\n      \nNaN\n\n      \n2457.026781\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:sum\n\n    \n\n    \n\n      \n1181\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:max\n\n      \nNaN\n\n      \nNaN\n\n      \n0.901897\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:max\n\n    \n\n    \n\n      \n1183\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \nNaN\n\n      \nNaN\n\n      \n40805.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collidedFrames:last\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nIt looks nice and natural, but it is not entirely correct. It often results in\na warning: \nSettingWithCopyWarning: A value is trying to be set on a copy of a\nslice from a DataFrame...\n. The message essentially says that the operation\n(here, adding the new column) might have been applied to a temporary object\ninstead of the original data frame, and thus might have been ineffective.\nLuckily, that is not the case most of the time (the operation \ndoes\n take\neffect). Nevertheless, for production code, i.e. scripts, the \"official\"\nsolution, the \nassign()\n method of the data frame is recommended, like this:\n\n\nIn[15]:\n\n\n\naloha\n \n=\n \naloha\n.\nassign\n(\nqname\n \n=\n \naloha\n.\nmodule\n \n+\n \n.\n \n+\n \naloha\n.\nname\n)\n\n\naloha\n[\naloha\n.\ntype\n==\nscalar\n]\n.\nhead\n()\n\n\n\n\n\nOut[15]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n      \nqname\n\n    \n\n  \n\n  \n\n    \n\n      \n1176\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \nduration\n\n      \nNaN\n\n      \nNaN\n\n      \n5400.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.duration\n\n    \n\n    \n\n      \n1177\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:mean\n\n      \nNaN\n\n      \nNaN\n\n      \n0.198275\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:mean\n\n    \n\n    \n\n      \n1179\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:sum\n\n      \nNaN\n\n      \nNaN\n\n      \n2457.026781\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:sum\n\n    \n\n    \n\n      \n1181\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollisionLength:max\n\n      \nNaN\n\n      \nNaN\n\n      \n0.901897\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collisionLength:max\n\n    \n\n    \n\n      \n1183\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \nNaN\n\n      \nNaN\n\n      \n40805.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nAloha.server.collidedFrames:last\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nFor completeness, one can remove a column from a data frame using either the\n\ndel\n operator or the \ndrop()\n method of the data frame. Here we show the former\n(also to remove the column we added above, as we won't need it for now):\n\n\nIn[16]:\n\n\n\ndel\n \naloha\n[\nqname\n]\n\n\n\n\n\n5. Revisiting CSV loading\n\n\nThe way we have read the CSV file has one small deficiency: all data in the\n\nattrvalue\n column are represented as strings, event though many of them\nare really numbers, for example the values of the \niaMean\n and \nnumHosts\n\niteration variables. You can verify that by printing the unique values (\n\naloha.attrvalue.unique()\n -- it will print all values with quotes), or using\nthe \ntype()\n operator on an element:\n\n\nIn[17]:\n\n\n\ntype\n(\n \naloha\n[\naloha\n.\ntype\n==\nscalar\n]\n.\niloc\n[\n0\n]\n.\nvalue\n \n)\n\n\n\n\n\nOut[17]:\n\n\n\nnumpy.float64\n\n\n\n\n\nThe reason is that \nread_csv()\n infers data types of columns from the data\nit finds in them. Since the \nattrvalue\n column is shared by run attributes,\nresult item attributes, iteration variables and some other types of rows,\nthere are many non-numeric strings in it, and \nread_csv()\n decides that it is\na string column.\n\n\nA similar issue arises with the \nbinedges\n, \nbinvalues\n, \nvectime\n, \nvecvalue\n\ncolumns. These columns contain lists of numbers separated by spaces, so they\nare read into strings as well. However, we would like to store them as NumPy\narrays (\nndarray\n) inside the data frame, because that's the form we can use\nin plots or as computation input.\n\n\nLuckily, \nread_csv()\n allows us to specify conversion functions for each column.\nSo, armed with the following two short functions:\n\n\nIn[18]:\n\n\n\ndef\n \nparse_if_number\n(\ns\n):\n\n    \ntry\n:\n \nreturn\n \nfloat\n(\ns\n)\n\n    \nexcept\n:\n \nreturn\n \nTrue\n \nif\n \ns\n==\ntrue\n \nelse\n \nFalse\n \nif\n \ns\n==\nfalse\n \nelse\n \ns\n \nif\n \ns\n \nelse\n \nNone\n\n\n\ndef\n \nparse_ndarray\n(\ns\n):\n\n    \nreturn\n \nnp\n.\nfromstring\n(\ns\n,\n \nsep\n=\n \n)\n \nif\n \ns\n \nelse\n \nNone\n\n\n\n\n\nwe can read the CSV file again, this time with the correct conversions:\n\n\nIn[19]:\n\n\n\naloha\n \n=\n \npd\n.\nread_csv\n(\naloha.csv\n,\n \nconverters\n \n=\n \n{\n\n    \nattrvalue\n:\n \nparse_if_number\n,\n\n    \nbinedges\n:\n \nparse_ndarray\n,\n\n    \nbinvalues\n:\n \nparse_ndarray\n,\n\n    \nvectime\n:\n \nparse_ndarray\n,\n\n    \nvecvalue\n:\n \nparse_ndarray\n})\n\n\n\n\n\nYou can verify the result e.g. by printing the unique values again.\n\n\n6. Load-time filtering\n\n\nIf the CSV file is large, you may want to skip certain columns or rows when\nreading it into memory. (File size is about the only valid reason for using\nload-time filtering, because you can also filter out or drop rows/columns\nfrom the data frame when it is already loaded.)\n\n\nTo filter out columns, you need to specify in the \nusecols\n parameter\nthe list of columns to keep:\n\n\nIn[20]:\n\n\n\ntmp\n \n=\n \npd\n.\nread_csv\n(\naloha.csv\n,\n \nusecols\n=\n[\nrun\n,\n \ntype\n,\n \nmodule\n,\n \nname\n,\n \nvalue\n])\n\n\n\n\n\nThere is no such direct support for filtering out rows based on their content,\nbut we can implement it using the iterator API that reads the CSV file\nin chunks. We can filter each chunk before storing and finally concatenating\nthem into a single data frame:\n\n\nIn[21]:\n\n\n\niter\n \n=\n \npd\n.\nread_csv\n(\naloha.csv\n,\n \niterator\n=\nTrue\n,\n \nchunksize\n=\n100\n)\n\n\nchunks\n \n=\n \n[\n \nchunk\n[\nchunk\n[\ntype\n]\n!=\nhistogram\n]\n \nfor\n \nchunk\n \nin\n \niter\n \n]\n  \n# discards type==\nhistogram\n lines\n\n\ntmp\n \n=\n \npd\n.\nconcat\n(\nchunks\n)\n\n\n\n\n\n7. Plotting scalars\n\n\nScalars can serve as input for many different kinds of plots. Here we'll show\nhow one can create a \"throughput versus offered load\" type plot. We will plot\nthe channel utilization in the Aloha model in the function of the packet\ngeneration frequency. Channel utilization is also affected by the number of\nhosts in the network -- we want results belonging to the same number of hosts\nto form iso lines. Packet generation frequency and the number of hosts are\npresent in the results as iteration variables named \niaMean\n and \nnumHosts\n;\nchannel utilization values are the \nchannelUtilization\n:\nlast\n scalars saved\nby the \nAloha.server\n module. The data contains the results from two simulation\nruns for each \n(iaMean, numHosts)\n pair done with different seeds; we want\nto average them for the plot.\n\n\nThe first few steps are fairly straightforward. We only need the scalars and the\niteration variables from the data frame, so we filter out the rest. Then we\ncreate a \nqname\n column from other columns to hold the names of our variables:\nthe names of scalars are in the \nmodule\n and \nname\n columns (we want to join them\nwith a dot), and the names of iteration variables are in the \nattrname\n column.\nSince \nattrname\n is not filled in for scalar rows, we can take \nattrname\n as\nqname\n\nfirst, then fill in the holes with \nmodule.name\n. We use the \ncombine_first()\n\nmethod for that: \na.combine_first(b)\n fills the holes in \na\n using the\ncorresponding values from \nb\n.\n\n\nThe similar issue arises with values: values of output scalars are in the \nvalue\n\ncolumn, while that of iteration variables are in the \nattrvalue\n column.\nSince \nattrvalue\n is unfilled for scalar rows, we can again utilize\n\ncombine_first()\n to merge two. There is one more catch: we need to change\nthe dtype of the \nattrvalue\n to \nfloat64\n, otherwise the resulting \nvalue\n\ncolumn also becomes \nobject\n dtype. (Luckily, all our iteration variables are\nnumeric, so the dtype conversion is possible. In other simulations that contain\nnon-numeric itervars, one needs to filter those out, force them into numeric\nvalues somehow, or find some other trick to make things work.)\n\n\nIn[22]:\n\n\n\nscalars\n \n=\n \naloha\n[(\naloha\n.\ntype\n==\nscalar\n)\n \n|\n \n(\naloha\n.\ntype\n==\nitervar\n)]\n  \n# filter rows\n\n\nscalars\n \n=\n \nscalars\n.\nassign\n(\nqname\n \n=\n \nscalars\n.\nattrname\n.\ncombine_first\n(\nscalars\n.\nmodule\n \n+\n \n.\n \n+\n \nscalars\n.\nname\n))\n  \n# add qname column\n\n\nscalars\n.\nvalue\n \n=\n \nscalars\n.\nvalue\n.\ncombine_first\n(\nscalars\n.\nattrvalue\n.\nastype\n(\nfloat64\n))\n  \n# merge value columns\n\n\nscalars\n[[\nrun\n,\n \ntype\n,\n \nqname\n,\n \nvalue\n,\n \nmodule\n,\n \nname\n,\n \nattrname\n]]\n.\niloc\n[\n80\n:\n90\n]\n  \n# print an excerpt of the result\n\n\n\n\n\nOut[22]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nqname\n\n      \nvalue\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n    \n\n  \n\n  \n\n    \n\n      \n1134\n\n      \nPureAlohaExperiment-40-20170627-20:42:22-22773\n\n      \nitervar\n\n      \niaMean\n\n      \n9.000000\n\n      \nNaN\n\n      \nNaN\n\n      \niaMean\n\n    \n\n    \n\n      \n1135\n\n      \nPureAlohaExperiment-40-20170627-20:42:22-22773\n\n      \nitervar\n\n      \nnumHosts\n\n      \n20.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nnumHosts\n\n    \n\n    \n\n      \n1162\n\n      \nPureAlohaExperiment-41-20170627-20:42:22-22773\n\n      \nitervar\n\n      \niaMean\n\n      \n9.000000\n\n      \nNaN\n\n      \nNaN\n\n      \niaMean\n\n    \n\n    \n\n      \n1163\n\n      \nPureAlohaExperiment-41-20170627-20:42:22-22773\n\n      \nitervar\n\n      \nnumHosts\n\n      \n20.000000\n\n      \nNaN\n\n      \nNaN\n\n      \nnumHosts\n\n    \n\n    \n\n      \n1176\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.duration\n\n      \n5400.000000\n\n      \nAloha.server\n\n      \nduration\n\n      \nNaN\n\n    \n\n    \n\n      \n1177\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.collisionLength:mean\n\n      \n0.198275\n\n      \nAloha.server\n\n      \ncollisionLength:mean\n\n      \nNaN\n\n    \n\n    \n\n      \n1179\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.collisionLength:sum\n\n      \n2457.026781\n\n      \nAloha.server\n\n      \ncollisionLength:sum\n\n      \nNaN\n\n    \n\n    \n\n      \n1181\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.collisionLength:max\n\n      \n0.901897\n\n      \nAloha.server\n\n      \ncollisionLength:max\n\n      \nNaN\n\n    \n\n    \n\n      \n1183\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.collidedFrames:last\n\n      \n40805.000000\n\n      \nAloha.server\n\n      \ncollidedFrames:last\n\n      \nNaN\n\n    \n\n    \n\n      \n1186\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nscalar\n\n      \nAloha.server.channelUtilization:last\n\n      \n0.156057\n\n      \nAloha.server\n\n      \nchannelUtilization:last\n\n      \nNaN\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nTo work further, it would be very convenient if we had a format where each\nsimulation run corresponds to one row, and all variables produced by that\nrun had their own columns. We can call it the \nwide\n format, and it can be\nproduced using the \npivot()\n method:\n\n\nIn[23]:\n\n\n\nscalars_wide\n \n=\n \nscalars\n.\npivot\n(\nrun\n,\n \ncolumns\n=\nqname\n,\n \nvalues\n=\nvalue\n)\n\n\nscalars_wide\n.\nhead\n()\n\n\n\n\n\nOut[23]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \nqname\n\n      \nAloha.server.channelUtilization:last\n\n      \nAloha.server.collidedFrames:last\n\n      \nAloha.server.collisionLength:max\n\n      \nAloha.server.collisionLength:mean\n\n      \nAloha.server.collisionLength:sum\n\n      \nAloha.server.duration\n\n      \nAloha.server.receivedFrames:last\n\n      \niaMean\n\n      \nnumHosts\n\n    \n\n    \n\n      \nrun\n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \n0.156057\n\n      \n40805.0\n\n      \n0.901897\n\n      \n0.198275\n\n      \n2457.026781\n\n      \n5400.0\n\n      \n8496.0\n\n      \n1.0\n\n      \n10.0\n\n    \n\n    \n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \n0.156176\n\n      \n40692.0\n\n      \n0.958902\n\n      \n0.198088\n\n      \n2456.494983\n\n      \n5400.0\n\n      \n8503.0\n\n      \n1.0\n\n      \n10.0\n\n    \n\n    \n\n      \nPureAlohaExperiment-10-20170627-20:42:16-22741\n\n      \n0.109571\n\n      \n1760.0\n\n      \n0.326138\n\n      \n0.155154\n\n      \n126.450220\n\n      \n5400.0\n\n      \n5965.0\n\n      \n7.0\n\n      \n10.0\n\n    \n\n    \n\n      \nPureAlohaExperiment-11-20170627-20:42:16-22741\n\n      \n0.108992\n\n      \n1718.0\n\n      \n0.340096\n\n      \n0.154529\n\n      \n125.477252\n\n      \n5400.0\n\n      \n5934.0\n\n      \n7.0\n\n      \n10.0\n\n    \n\n    \n\n      \nPureAlohaExperiment-12-20170627-20:42:16-22741\n\n      \n0.090485\n\n      \n1069.0\n\n      \n0.272013\n\n      \n0.152142\n\n      \n78.201174\n\n      \n5400.0\n\n      \n4926.0\n\n      \n9.0\n\n      \n10.0\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nWe are interested in only three columns for our plot:\n\n\nIn[24]:\n\n\n\nscalars_wide\n[[\nnumHosts\n,\n \niaMean\n,\n \nAloha.server.channelUtilization:last\n]]\n.\nhead\n()\n\n\n\n\n\nOut[24]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \nqname\n\n      \nnumHosts\n\n      \niaMean\n\n      \nAloha.server.channelUtilization:last\n\n    \n\n    \n\n      \nrun\n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \n10.0\n\n      \n1.0\n\n      \n0.156057\n\n    \n\n    \n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \n10.0\n\n      \n1.0\n\n      \n0.156176\n\n    \n\n    \n\n      \nPureAlohaExperiment-10-20170627-20:42:16-22741\n\n      \n10.0\n\n      \n7.0\n\n      \n0.109571\n\n    \n\n    \n\n      \nPureAlohaExperiment-11-20170627-20:42:16-22741\n\n      \n10.0\n\n      \n7.0\n\n      \n0.108992\n\n    \n\n    \n\n      \nPureAlohaExperiment-12-20170627-20:42:16-22741\n\n      \n10.0\n\n      \n9.0\n\n      \n0.090485\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nSince we have our \nx\n and \ny\n data in separate columns now, we can utilize the\nscatter plot feature of the data frame for plotting it:\n\n\nIn[25]:\n\n\n\n# set the default image resolution and size\n\n\nplt\n.\nrcParams\n[\nfigure.figsize\n]\n \n=\n \n[\n8.0\n,\n \n3.0\n]\n\n\nplt\n.\nrcParams\n[\nfigure.dpi\n]\n \n=\n \n144\n\n\n# create a scatter plot\n\n\nscalars_wide\n.\nplot\n.\nscatter\n(\niaMean\n,\n \nAloha.server.channelUtilization:last\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[25]:\n\n\n\n\n\nNOTE: Although \nplt.show()\n is not needed in Jupyter (\n%matplotlib\n \ninline\n\nturns on immediate display), we'll continue to include it in further code\nfragments, so that they work without change when you use another Python shell.\n\n\nThe resulting chart looks quite good as the first attempt. However, it has some\nshortcomings:\n\n\n\n\nDots are not connected. The dots that have the same \nnumHosts\n value should\n  be connected with iso lines.\n\n\nAs the result of having two simulation runs for each \n(iaMean,numHosts)\n pair,\n  the dots appear in pairs. We'd like to see their averages instead.\n\n\n\n\nUnfortunately, scatter plot can only take us this far, we need to look for\nanother way.\n\n\nWhat we really need as chart input is a table where rows correspond to different\n\niaMean\n values, columns correspond to different \nnumHosts\n values, and cells\ncontain channel utilization values (the average of the repetitions).\nSuch table can be produced from the \"wide format\" with another pivoting\noperation. We use \npivot_table()\n, a cousin of the \npivot()\n method we've seen above.\nThe difference between them is that \npivot()\n is a reshaping operation (it just\nrearranges elements), while \npivot_table()\n is more of a spreadsheet-style\npivot table creation operation, and primarily intended for numerical data.\n\npivot_table()\n accepts an aggregation function with the default being \nmean\n,\nwhich is quite convenient for us now (we want to average channel utilization\nover repetitions.)\n\n\nIn[26]:\n\n\n\naloha_pivot\n \n=\n \nscalars_wide\n.\npivot_table\n(\nindex\n=\niaMean\n,\n \ncolumns\n=\nnumHosts\n,\n \nvalues\n=\nAloha.server.channelUtilization:last\n)\n  \n# note: aggregation function = mean (that\ns the default)\n\n\naloha_pivot\n.\nhead\n()\n\n\n\n\n\nOut[26]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \nnumHosts\n\n      \n10.0\n\n      \n15.0\n\n      \n20.0\n\n    \n\n    \n\n      \niaMean\n\n      \n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \n1.0\n\n      \n0.156116\n\n      \n0.089539\n\n      \n0.046586\n\n    \n\n    \n\n      \n2.0\n\n      \n0.194817\n\n      \n0.178159\n\n      \n0.147564\n\n    \n\n    \n\n      \n3.0\n\n      \n0.176321\n\n      \n0.191571\n\n      \n0.183976\n\n    \n\n    \n\n      \n4.0\n\n      \n0.153569\n\n      \n0.182324\n\n      \n0.190452\n\n    \n\n    \n\n      \n5.0\n\n      \n0.136997\n\n      \n0.168780\n\n      \n0.183742\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nNote that rows correspond to various \niaMean\n values (\niaMean\n serves as index);\nthere is one column for each value of \nnumHosts\n; and that data in the table\nare the averages of the channel utilizations produced by the simulations\nperformed with the respective \niaMean\n and \nnumHosts\n values.\n\n\nFor the plot, every column should generate a separate line (with the \nx\n values\ncoming from the index column, \niaMean\n) labelled with the column name.\nThe basic Matplotlib interface cannot create such plot in one step. However,\nthe Pandas data frame itself has a plotting interface which knows how to\ninterpret the data, and produces the correct plot without much convincing:\n\n\nIn[27]:\n\n\n\naloha_pivot\n.\nplot\n.\nline\n()\n\n\nplt\n.\nylabel\n(\nchannel utilization\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[27]:\n\n\n\n\n\n8. Interactive pivot tables\n\n\nGetting the pivot table right is not always easy, so having a GUI where\none can drag columns around and immediately see the result is definitely\na blessing. Pivottable.js presents such a GUI inside a browser, and\nalthough the bulk of the code is Javascript, it has a Python frond-end\nthat integrates nicely with Jupyter. Let's try it!\n\n\nIn[28]:\n\n\n\nimport\n \npivottablejs\n \nas\n \npj\n\n\npj\n.\npivot_ui\n(\nscalars_wide\n)\n\n\n\n\n\nOut[28]:\n\n\n\n\n\n        \n\n\n\n\n\n\nAn interactive panel containing the pivot table will appear. Here is how\nyou can reproduce the above \"Channel utilization vs iaMean\" plot in it:\n\n\n\n\nDrag \nnumHosts\n to the \"rows\" area of the pivot table.\n   The table itself is the area on the left that initially only displays \"Totals | 42\",\n   and the \"rows\" area is the empty rectangle directly of left it.\n   The table should show have two columns (\nnumHosts\n and \nTotals\n) and\n   five rows in total after dragging.\n\n\nDrag \niaMean\n to the \"columns\" area (above the table). Columns for each value\n   of \niaMean\n should appear in the table.\n\n\nNear the top-left corner of the table, select \nAverage\n from the combo box\n   that originally displays \nCount\n, and select \nChannelUtilization\n:\nlast\n\n   from the combo box that appears below it.\n\n\nIn the top-left corner of the panel, select \nLine Chart\n from the combo box\n   that originally displays \nTable\n.\n\n\n\n\nIf you can't get to see it, the following command will programmatically\nconfigure the pivot table in the appropriate way:\n\n\nIn[29]:\n\n\n\npj\n.\npivot_ui\n(\nscalars_wide\n,\n \nrows\n=\n[\nnumHosts\n],\n \ncols\n=\n[\niaMean\n],\n \nvals\n=\n[\nAloha.server.channelUtilization:last\n],\n \naggregatorName\n=\nAverage\n,\n \nrendererName\n=\nLine Chart\n)\n\n\n\n\n\nOut[29]:\n\n\n\n\n\n        \n\n\n\n\n\n\nIf you want experiment with Excel's or LibreOffice's built-in pivot table\nfunctionality, the data frame's \nto_clipboard()\n and \nto_csv()\n methods\nwill help you transfer the data. For example, you can issue the\n\nscalars_wide.to_clipboard()\n command to put the data on the clipboard, then\npaste it into the spreadsheet. Alternatively, type \nprint(scalars_wide.to_csv())\n\nto print the data in CSV format that you can select and then copy/paste.\nOr, use \nscalars_wide.to_csv(\nscalars.csv\n)\n to save the data into a file\nwhich you can import.\n\n\n9. Plotting histograms\n\n\nIn this section we explore how to plot histograms recorded by the simulation.\nHistograms are in rows that have \nhistogram\n in the \ntype\n column.\nHistogram bin edges and bin values (counts) are in the \nbinedges\n and\n\nbinvalues\n columns as NumPy array objects (\nndarray\n).\n\n\nLet us begin by selecting the histograms into a new data frame for convenience.\n\n\nIn[30]:\n\n\n\nhistograms\n \n=\n \naloha\n[\naloha\n.\ntype\n==\nhistogram\n]\n\n\nlen\n(\nhistograms\n)\n\n\n\n\n\nOut[30]:\n\n\n\n84\n\n\n\n\n\nWe have 84 histograms. It makes no sense to plot so many histograms on one chart,\nso let's just take one on them, and examine its content.\n\n\nIn[31]:\n\n\n\nhist\n \n=\n \nhistograms\n.\niloc\n[\n0\n]\n  \n# the first histogram\n\n\nhist\n.\nbinedges\n,\n \nhist\n.\nbinvalues\n\n\n\n\n\nOut[31]:\n\n\n\n(array([-0.11602833, -0.08732314, -0.05861794, -0.02991275, -0.00120756,\n         0.02749763,  0.05620283,  0.08490802,  0.11361321,  0.1423184 ,\n         0.1710236 ,  0.19972879,  0.22843398,  0.25713917,  0.28584437,\n         0.31454956,  0.34325475,  0.37195994,  0.40066514,  0.42937033,\n         0.45807552,  0.48678071,  0.51548591,  0.5441911 ,  0.57289629,\n         0.60160148,  0.63030668,  0.65901187,  0.68771706,  0.71642225,\n         0.74512745]),\n array([    0.,     0.,     0.,     0.,     0.,     0.,     0.,  1234.,\n         2372.,  2180.,  2115.,  1212.,   917.,   663.,   473.,   353.,\n          251.,   186.,   123.,    99.,    60.,    44.,    31.,    25.,\n           15.,    13.,     9.,     3.,     5.,     3.]))\n\n\n\n\n\nThe easiest way to plot the histogram from these two arrays is to look at it\nas a step function, and create a line plot with the appropriate drawing style.\nThe only caveat is that we need to add an extra \n0\n element to draw the right\nside of the last histogram bin.\n\n\nIn[32]:\n\n\n\nplt\n.\nplot\n(\nhist\n.\nbinedges\n,\n \nnp\n.\nappend\n(\nhist\n.\nbinvalues\n,\n \n0\n),\n \ndrawstyle\n=\nsteps-post\n)\n   \n# or maybe steps-mid, for integers\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[32]:\n\n\n\n\n\nAnother way to plot a recorded histogram is Matplotlib's \nhist()\n method,\nalthough that is a bit tricky. Instead of taking histogram data, \nhist()\n\ninsists on computing the histogram itself from an array of values -- but we only\nhave the histogram, and not the data it was originally computed from.\nFortunately, \nhist()\n can accept a bin edges array, and another array as weights\nfor the values. Thus, we can trick it into doing what we want by passing\nin our \nbinedges\n array twice, once as bin edges and once as values, and\nspecifying \nbinvalues\n as weights.\n\n\nIn[33]:\n\n\n\nplt\n.\nhist\n(\nbins\n=\nhist\n.\nbinedges\n,\n \nx\n=\nhist\n.\nbinedges\n[:\n-\n1\n],\n \nweights\n=\nhist\n.\nbinvalues\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[33]:\n\n\n\n\n\nhist()\n has some interesting options. For example, we can change the plotting\nstyle to be similar to a line plot by setting \nhisttype=\nstep\n. To plot the\nnormalized version of the histogram, specify \nnormed=True\n or \ndensity=True\n\n(they work differently; see the Matplotlib documentation for details).\nTo draw the cumulative density function, also specify \ncumulative=True\n.\nThe following plot shows the effect of some of these options.\n\n\nIn[34]:\n\n\n\nplt\n.\nhist\n(\nbins\n=\nhist\n.\nbinedges\n,\n \nx\n=\nhist\n.\nbinedges\n[:\n-\n1\n],\n \nweights\n=\nhist\n.\nbinvalues\n,\n \nhisttype\n=\nstep\n,\n \nnormed\n=\nTrue\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[34]:\n\n\n\n\n\nTo plot several histograms, we can iterate over the histograms and draw them\none by one on the same plot. The following code does that, and also adds a\nlegend and adjusts the bounds of the x axis.\n\n\nIn[35]:\n\n\n\nsomehistograms\n \n=\n \nhistograms\n[\nhistograms\n.\nname\n \n==\n \ncollisionLength:histogram\n][:\n5\n]\n\n\nfor\n \nrow\n \nin\n \nsomehistograms\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nbinedges\n,\n \nnp\n.\nappend\n(\nrow\n.\nbinvalues\n,\n \n0\n),\n \ndrawstyle\n=\nsteps-post\n)\n\n\nplt\n.\nlegend\n(\nsomehistograms\n.\nmodule\n \n+\n \n.\n \n+\n \nsomehistograms\n.\nname\n)\n\n\nplt\n.\nxlim\n(\n0\n,\n \n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[35]:\n\n\n\n\n\nNote, however, that the legend contains the same string for all histograms,\nwhich is not very meaningful. We could improve that by including some\ncharacteristics of the simulation that generated them, i.e. the number of hosts\n(\nnumHosts\n iteration variable) and frame interarrival times (\niaTime\n iteration\nvariable). We'll see in the next section how that can be achieved.\n\n\n10. Adding iteration variables as columns\n\n\nIn this step, we add the iteration variables associated with the simulation\nrun to the data frame as columns. There are several reasons why this is a\ngood idea: they are very useful for generating the legends for plots of\ne.g. histograms and vectors (e.g. \"collision multiplicity histogram for\nnumHosts=20 and iaMean=2s\"), and often needed as chart input as well.\n\n\nFirst, we select the iteration variables vars as a smaller data frame.\n\n\nIn[36]:\n\n\n\nitervars_df\n \n=\n \naloha\n.\nloc\n[\naloha\n.\ntype\n==\nitervar\n,\n \n[\nrun\n,\n \nattrname\n,\n \nattrvalue\n]]\n\n\nitervars_df\n.\nhead\n()\n\n\n\n\n\nOut[36]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \nattrname\n\n      \nattrvalue\n\n    \n\n  \n\n  \n\n    \n\n      \n14\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \niaMean\n\n      \n3\n\n    \n\n    \n\n      \n15\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nnumHosts\n\n      \n10\n\n    \n\n    \n\n      \n42\n\n      \nPureAlohaExperiment-3-20170627-20:42:20-22739\n\n      \niaMean\n\n      \n2\n\n    \n\n    \n\n      \n43\n\n      \nPureAlohaExperiment-3-20170627-20:42:20-22739\n\n      \nnumHosts\n\n      \n10\n\n    \n\n    \n\n      \n70\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \niaMean\n\n      \n1\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nWe reshape the result by using the \npivot()\n method. The following statement\nwill convert unique values in the \nattrname\n column into separate columns:\n\niaMean\n and \nnumHosts\n. The new data frame will be indexed with the run id.\n\n\nIn[37]:\n\n\n\nitervarspivot_df\n \n=\n \nitervars_df\n.\npivot\n(\nindex\n=\nrun\n,\n \ncolumns\n=\nattrname\n,\n \nvalues\n=\nattrvalue\n)\n\n\nitervarspivot_df\n.\nhead\n()\n\n\n\n\n\nOut[37]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \nattrname\n\n      \niaMean\n\n      \nnumHosts\n\n    \n\n    \n\n      \nrun\n\n      \n\n      \n\n    \n\n  \n\n  \n\n    \n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \n1\n\n      \n10\n\n    \n\n    \n\n      \nPureAlohaExperiment-10-20170627-20:42:16-22741\n\n      \n7\n\n      \n10\n\n    \n\n    \n\n      \nPureAlohaExperiment-11-20170627-20:42:16-22741\n\n      \n7\n\n      \n10\n\n    \n\n    \n\n      \nPureAlohaExperiment-12-20170627-20:42:16-22741\n\n      \n9\n\n      \n10\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nNow, we only need to add the new columns back into the original dataframe, using\n\nmerge()\n. This operation is not quite unlike an SQL join of two tables on the\n\nrun\n column.\n\n\nIn[38]:\n\n\n\naloha2\n \n=\n \naloha\n.\nmerge\n(\nitervarspivot_df\n,\n \nleft_on\n=\nrun\n,\n \nright_index\n=\nTrue\n,\n \nhow\n=\nouter\n)\n\n\naloha2\n.\nhead\n()\n\n\n\n\n\nOut[38]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n      \niaMean\n\n      \nnumHosts\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nconfigname\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n    \n\n    \n\n      \n1\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ndatetime\n\n      \n20170627-20:42:20\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n    \n\n    \n\n      \n2\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nexperiment\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n    \n\n    \n\n      \n3\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ninifile\n\n      \nomnetpp.ini\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n    \n\n    \n\n      \n4\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \niterationvars\n\n      \nnumHosts=10, iaMean=3\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nFor plot legends, it is also useful to have a single \niterationvars\n column with\nstring values like \nnumHosts=10, iaMean=2\n. This is easier than the above: we\ncan just select the rows containing the run attribute named \niterationvars\n\n(it contains exactly the string we need), take only the \nrun\n and \nattrvalue\n\ncolumns, rename the \nattrvalue\n column to \niterationvars\n, and then merge back the\nresult into the original data frame in a way we did above.\n\n\nThe selection and renaming step can be done as follows. (Note: we need\n\n.astype\n(\nstr\n)\n in the condition so that rows where \nattrname\n is not filled in\ndo not cause trouble.)\n\n\nIn[39]:\n\n\n\nitervarscol_df\n \n=\n \naloha\n.\nloc\n[(\naloha\n.\ntype\n==\nrunattr\n)\n \n \n(\naloha\n.\nattrname\n.\nastype\n(\nstr\n)\n==\niterationvars\n),\n \n[\nrun\n,\n \nattrvalue\n]]\n\n\nitervarscol_df\n \n=\n \nitervarscol_df\n.\nrename\n(\ncolumns\n=\n{\nattrvalue\n:\n \niterationvars\n})\n\n\nitervarscol_df\n.\nhead\n()\n\n\n\n\n\nOut[39]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \niterationvars\n\n    \n\n  \n\n  \n\n    \n\n      \n4\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n    \n\n      \n32\n\n      \nPureAlohaExperiment-3-20170627-20:42:20-22739\n\n      \nnumHosts=10, iaMean=2\n\n    \n\n    \n\n      \n60\n\n      \nPureAlohaExperiment-0-20170627-20:42:16-22739\n\n      \nnumHosts=10, iaMean=1\n\n    \n\n    \n\n      \n88\n\n      \nPureAlohaExperiment-1-20170627-20:42:17-22739\n\n      \nnumHosts=10, iaMean=1\n\n    \n\n    \n\n      \n116\n\n      \nPureAlohaExperiment-2-20170627-20:42:19-22739\n\n      \nnumHosts=10, iaMean=2\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nIn the merging step, we join the two tables (I mean, data frames) on the \nrun\n\ncolumn:\n\n\nIn[40]:\n\n\n\naloha3\n \n=\n \naloha2\n.\nmerge\n(\nitervarscol_df\n,\n \nleft_on\n=\nrun\n,\n \nright_on\n=\nrun\n,\n \nhow\n=\nouter\n)\n\n\naloha3\n.\nhead\n()\n\n\n\n\n\nOut[40]:\n\n\n\n\n\n\n\n\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n\n\n\n  \n\n    \n\n      \n\n      \nrun\n\n      \ntype\n\n      \nmodule\n\n      \nname\n\n      \nattrname\n\n      \nattrvalue\n\n      \nvalue\n\n      \ncount\n\n      \nsumweights\n\n      \nmean\n\n      \nstddev\n\n      \nmin\n\n      \nmax\n\n      \nbinedges\n\n      \nbinvalues\n\n      \nvectime\n\n      \nvecvalue\n\n      \niaMean\n\n      \nnumHosts\n\n      \niterationvars\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nconfigname\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n    \n\n      \n1\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ndatetime\n\n      \n20170627-20:42:20\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n    \n\n      \n2\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \nexperiment\n\n      \nPureAlohaExperiment\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n    \n\n      \n3\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \ninifile\n\n      \nomnetpp.ini\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n    \n\n      \n4\n\n      \nPureAlohaExperiment-4-20170627-20:42:20-22739\n\n      \nrunattr\n\n      \nNaN\n\n      \nNaN\n\n      \niterationvars\n\n      \nnumHosts=10, iaMean=3\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNaN\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \nNone\n\n      \n3\n\n      \n10\n\n      \nnumHosts=10, iaMean=3\n\n    \n\n  \n\n\n\n\n\n\n\n\n\nTo see the result of our work, let's try plotting the same histograms again,\nthis time with a proper legend:\n\n\nIn[41]:\n\n\n\nhistograms\n \n=\n \naloha3\n[\naloha3\n.\ntype\n==\nhistogram\n]\n\n\nsomehistograms\n \n=\n \nhistograms\n[\nhistograms\n.\nname\n \n==\n \ncollisionLength:histogram\n][:\n5\n]\n\n\nfor\n \nrow\n \nin\n \nsomehistograms\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nbinedges\n,\n \nnp\n.\nappend\n(\nrow\n.\nbinvalues\n,\n \n0\n),\n \ndrawstyle\n=\nsteps-post\n)\n\n\nplt\n.\ntitle\n(\ncollisionLength:histogram\n)\n\n\nplt\n.\nlegend\n(\nsomehistograms\n.\niterationvars\n)\n\n\nplt\n.\nxlim\n(\n0\n,\n \n0.5\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[41]:\n\n\n\n\n\n11. Plotting vectors\n\n\nThis section deals with basic plotting of output vectors. Output vectors\nare basically time series data, but values have timestamps instead\nof being evenly spaced. Vectors are in rows that have \nvector\n\nin the \ntype\n column. The values and their timestamps are in the\n\nvecvalue\n and \nvectime\n columns as NumPy array objects (\nndarray\n).\n\n\nWe'll use a different data set for exploring output vector plotting, one from\nthe \nrouting\n example simulation. There are pre-recorded result files in the\n\nsamples/resultfiles/routing\n directory; change into it in the terminal, and\nissue the following command to convert them to CSV:\n\n\nscavetool x *.sca *.vec -o routing.csv\n\n\n\n\n\nThen we read the the CSV file into a data frame in the same way we saw with the\n\naloha\n dataset:\n\n\nIn[42]:\n\n\n\nrouting\n \n=\n \npd\n.\nread_csv\n(\nrouting.csv\n,\n \nconverters\n \n=\n \n{\n\n    \nattrvalue\n:\n \nparse_if_number\n,\n\n    \nbinedges\n:\n \nparse_ndarray\n,\n\n    \nbinvalues\n:\n \nparse_ndarray\n,\n\n    \nvectime\n:\n \nparse_ndarray\n,\n\n    \nvecvalue\n:\n \nparse_ndarray\n})\n\n\n\n\n\nLet us begin by selecting the vectors into a new data frame for convenience.\n\n\nIn[43]:\n\n\n\nvectors\n \n=\n \nrouting\n[\nrouting\n.\ntype\n==\nvector\n]\n\n\nlen\n(\nvectors\n)\n\n\n\n\n\nOut[43]:\n\n\n\n65\n\n\n\n\n\nOur data frame contains results from one run. To get some idea what vectors\nwe have, let's print the list unique vector names and module names:\n\n\nIn[44]:\n\n\n\nvectors\n.\nname\n.\nunique\n(),\n \nvectors\n.\nmodule\n.\nunique\n()\n\n\n\n\n\nOut[44]:\n\n\n\n(array([\nbusy:vector\n, \nqlen:vector\n, \ntxBytes:vector\n,\n        \nendToEndDelay:vector\n, \nhopCount:vector\n, \nsourceAddress:vector\n,\n        \nrxBytes:vector\n, \ndrop:vector\n], dtype=object),\n array([\nNet5.rte[0].port$o[0].channel\n, \nNet5.rte[0].port$o[1].channel\n,\n        \nNet5.rte[1].port$o[0].channel\n, \nNet5.rte[1].port$o[1].channel\n,\n        \nNet5.rte[1].port$o[2].channel\n, \nNet5.rte[2].port$o[0].channel\n,\n        \nNet5.rte[2].port$o[1].channel\n, \nNet5.rte[2].port$o[2].channel\n,\n        \nNet5.rte[2].port$o[3].channel\n, \nNet5.rte[3].port$o[0].channel\n,\n        \nNet5.rte[3].port$o[1].channel\n, \nNet5.rte[3].port$o[2].channel\n,\n        \nNet5.rte[4].port$o[0].channel\n, \nNet5.rte[4].port$o[1].channel\n,\n        \nNet5.rte[0].queue[0]\n, \nNet5.rte[0].queue[1]\n,\n        \nNet5.rte[1].queue[0]\n, \nNet5.rte[1].queue[1]\n,\n        \nNet5.rte[1].queue[2]\n, \nNet5.rte[2].queue[0]\n,\n        \nNet5.rte[2].queue[1]\n, \nNet5.rte[2].queue[2]\n,\n        \nNet5.rte[2].queue[3]\n, \nNet5.rte[3].queue[0]\n,\n        \nNet5.rte[3].queue[1]\n, \nNet5.rte[3].queue[2]\n,\n        \nNet5.rte[4].queue[0]\n, \nNet5.rte[4].queue[1]\n, \nNet5.rte[4].app\n,\n        \nNet5.rte[1].app\n], dtype=object))\n\n\n\n\n\nA vector can be plotted on a line chart by simply passing the \nvectime\n and\n\nvecvalue\n arrays to \nplt.plot()\n:\n\n\nIn[45]:\n\n\n\nvec\n \n=\n \nvectors\n[\nvectors\n.\nname\n \n==\n \nqlen:vector\n]\n.\niloc\n[\n4\n]\n  \n# take some vector\n\n\nplt\n.\nplot\n(\nvec\n.\nvectime\n,\n \nvec\n.\nvecvalue\n,\n \ndrawstyle\n=\nsteps-post\n)\n\n\nplt\n.\nxlim\n(\n0\n,\n100\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[45]:\n\n\n\n\n\nWhen several vectors need to be placed on the same plot, one can simply\nuse a \nfor\n loop.\n\n\nIn[46]:\n\n\n\nsomevectors\n \n=\n \nvectors\n[\nvectors\n.\nname\n \n==\n \nqlen:vector\n][:\n5\n]\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n,\n \nrow\n.\nvecvalue\n,\n \ndrawstyle\n=\nsteps-post\n)\n\n\nplt\n.\ntitle\n(\nsomevectors\n.\nname\n.\nvalues\n[\n0\n])\n\n\nplt\n.\nlegend\n(\nsomevectors\n.\nmodule\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[46]:\n\n\n\n\n\n12. Vector Filtering\n\n\nPlotting vectors \"as is\" is often not practical, as the result will be a crowded\nplot that's difficult to draw conclusions from. To remedy that, one can apply\nsome kind of filtering before plotting, or plot a derived quantity such as the\nintegral, sum or running average instead of the original. Such things can easily\nbe achieved with the help of NumPy.\n\n\nVector time and value are already stored in the data frame as NumPy arrays\n(\nndarray\n), so we can apply NumPy functions to them. For example, let's\ntry \nnp.cumsum()\n which computes cumulative sum:\n\n\nIn[47]:\n\n\n\nx\n \n=\n \nnp\n.\narray\n([\n8\n,\n \n2\n,\n \n1\n,\n \n5\n,\n \n7\n])\n\n\nnp\n.\ncumsum\n(\nx\n)\n\n\n\n\n\nOut[47]:\n\n\n\narray([ 8, 10, 11, 16, 23])\n\n\n\n\n\nIn[48]:\n\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n,\n \nnp\n.\ncumsum\n(\nrow\n.\nvecvalue\n))\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[48]:\n\n\n\n\n\nPlotting cumulative sum against time might be useful e.g. for an output\nvector where the simulation emits the packet length for each packet\nthat has arrived at its destination. There, the sum would represent\n\"total bytes received\".\n\n\nPlotting the count against time for the same output vector would\nrepresent \"number of packets received\". For such a plot, we can utilize\n\nnp.arange(1,n)\n which simply returns the numbers 1, 2, .., n-1\nas an array:\n\n\nIn[49]:\n\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n,\n \nnp\n.\narange\n(\n1\n,\n \nrow\n.\nvecvalue\n.\nsize\n+\n1\n),\n \n.-\n,\n \ndrawstyle\n=\nsteps-post\n)\n\n\nplt\n.\nxlim\n(\n0\n,\n5\n);\n \nplt\n.\nylim\n(\n0\n,\n20\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[49]:\n\n\n\n\n\nNote that we changed the plotting style to \"steps-post\", so\nthat for any \nt\n time the plot accurately represents the number\nof values whose timestamp is less than or equal to \nt\n.\n\n\nAs another warm-up exercise, let's plot the time interval\nthat elapses between adjacent values; that is, for each element\nwe want to plot the time difference between the that element\nand the previous one.\nThis can be achieved by computing \nt\n[\n1\n:\n]\n \n-\n \nt\n[:-\n1\n]\n, which is the\nelementwise subtraction of the \nt\n array and its shifted version.\nArray indexing starts at 0, so \nt[1:]\n means \"drop the first element\".\nNegative indices count from the end of the array, so \nt\n[:-\n1\n]\n means\n\"without the last element\". The latter is necessary because the\nsizes of the two arrays must match. or convenience, we encapsulate\nthe formula into a Python function:\n\n\nIn[50]:\n\n\n\ndef\n \ndiff\n(\nt\n):\n\n    \nreturn\n \nt\n[\n1\n:]\n \n-\n \nt\n[:\n-\n1\n]\n\n\n\n# example\n\n\nt\n \n=\n \nnp\n.\narray\n([\n0.1\n,\n \n1.5\n,\n \n1.6\n,\n \n2.0\n,\n \n3.1\n])\n\n\ndiff\n(\nt\n)\n\n\n\n\n\nOut[50]:\n\n\n\narray([ 1.4,  0.1,  0.4,  1.1])\n\n\n\n\n\nWe can now plot it. Note that as \ndiff()\n makes the array one element\nshorter, we need to write \nrow.vectime[1:]\n to drop the first element\n(it has no preceding element, so \ndiff()\n cannot be computed for it.)\nAlso, we use dots for plotting instead of lines, as it makes more\nsense here.\n\n\nIn[51]:\n\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n[\n1\n:],\n \ndiff\n(\nrow\n.\nvectime\n),\n \no\n)\n\n\nplt\n.\nxlim\n(\n0\n,\n100\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[51]:\n\n\n\n\n\nWe now know enough NumPy to be able to write a function that computes\nrunning average (a.k.a. \"mean filter\"). Let's try it out in a plot\nimmediately.\n\n\nIn[52]:\n\n\n\ndef\n \nrunning_avg\n(\nx\n):\n\n    \nreturn\n \nnp\n.\ncumsum\n(\nx\n)\n \n/\n \nnp\n.\narange\n(\n1\n,\n \nx\n.\nsize\n \n+\n \n1\n)\n\n\n\n# example plot:\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n,\n \nrunning_avg\n(\nrow\n.\nvecvalue\n))\n\n\nplt\n.\nxlim\n(\n0\n,\n100\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[52]:\n\n\n\n\n\nFor certain quantities such as queue length or on-off status,\nweighted average (with time intervals used as weights) makes\nmore sense. Here is a function that computes running time-average:\n\n\nIn[53]:\n\n\n\ndef\n \nrunning_timeavg\n(\nt\n,\nx\n):\n\n    \ndt\n \n=\n \nt\n[\n1\n:]\n \n-\n \nt\n[:\n-\n1\n]\n\n    \nreturn\n \nnp\n.\ncumsum\n(\nx\n[:\n-\n1\n]\n \n*\n \ndt\n)\n \n/\n \nt\n[\n1\n:]\n\n\n\n# example plot:\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n[\n1\n:],\n \nrunning_timeavg\n(\nrow\n.\nvectime\n,\n \nrow\n.\nvecvalue\n))\n\n\nplt\n.\nxlim\n(\n0\n,\n100\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[53]:\n\n\n\n\n\nComputing the integral of the vector as a step function is very similar\nto the \nrunning_timeavg()\n function. (Note: Computing integral in other\nways is part of NumPy and SciPy, if you ever need it. For example,\n\nnp.trapz(y,x)\n computes integral using the trapezoidal rule.)\n\n\nIn[54]:\n\n\n\ndef\n \nintegrate_steps\n(\nt\n,\nx\n):\n\n    \ndt\n \n=\n \nt\n[\n1\n:]\n \n-\n \nt\n[:\n-\n1\n]\n\n    \nreturn\n \nnp\n.\ncumsum\n(\nx\n[:\n-\n1\n]\n \n*\n \ndt\n)\n\n\n\n# example plot:\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n[\n1\n:],\n \nintegrate_steps\n(\nrow\n.\nvectime\n,\n \nrow\n.\nvecvalue\n))\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[54]:\n\n\n\n\n\nAs the last example in this section, here is a function that computes\nmoving window average. It relies on the clever trick of subtracting\nthe cumulative sum of the original vector from its shifted version\nto get the sum of values in every \nN\n-sized window.\n\n\nIn[55]:\n\n\n\ndef\n \nwinavg\n(\nx\n,\n \nN\n):\n\n    \nxpad\n \n=\n \nnp\n.\nconcatenate\n((\nnp\n.\nzeros\n(\nN\n),\n \nx\n))\n \n# pad with zeroes\n\n    \ns\n \n=\n \nnp\n.\ncumsum\n(\nxpad\n)\n\n    \nss\n \n=\n \ns\n[\nN\n:]\n \n-\n \ns\n[:\n-\nN\n]\n\n    \nss\n[\nN\n-\n1\n:]\n \n/=\n \nN\n\n    \nss\n[:\nN\n-\n1\n]\n \n/=\n \nnp\n.\narange\n(\n1\n,\n \nmin\n(\nN\n-\n1\n,\nss\n.\nsize\n)\n+\n1\n)\n\n    \nreturn\n \nss\n\n\n\n# example:\n\n\nfor\n \nrow\n \nin\n \nsomevectors\n.\nitertuples\n():\n\n    \nplt\n.\nplot\n(\nrow\n.\nvectime\n,\n \nwinavg\n(\nrow\n.\nvecvalue\n,\n \n10\n))\n\n\nplt\n.\nxlim\n(\n0\n,\n200\n)\n\n\nplt\n.\nshow\n()\n\n\n\n\n\nOut[55]:\n\n\n\n\n\nYou can find further hints for smoothing the plot of an output vector\nin the signal processing chapter of the SciPy Cookbook (see References).\n\n\nResources\n\n\nThe primary and authentic source of information on Pandas, Matplotlib and other\nlibraries is their official documentation. I do not link them here because they\nare trivial to find via Google. Instead, here is a random collection of other\nresources that I found useful while writing this tutorial (not counting all the\nStackOverflow pages I visited.)\n\n\n\n\nPandas tutorial from Greg Reda:\n  \nhttp://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/\n\n\nOn reshaping data frames:\n  \nhttps://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping\n\n\nMatplotlib tutorial of Nicolas P. Rougier:\n  \nhttps://www.labri.fr/perso/nrougier/teaching/matplotlib/\n\n\nCreating boxplots with Matplotlib, from Bharat Bhole:\n  \nhttp://blog.bharatbhole.com/creating-boxplots-with-matplotlib/\n\n\nSciPy Cookbook on signal smoothing:\n  \nhttp://scipy-cookbook.readthedocs.io/items/SignalSmooth.html\n\n\nVisual Guide on Pandas (video):\n  \nhttps://www.youtube.com/watch?v=9d5-Ti6onew\n\n\nPython Pandas Cookbook (videos):\n  \nhttps://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3\n\n\n\n\nAcknowledgements\n\n\nThe author, Andras Varga would like to thank the participants of the\n2016 OMNeT++ Summit for the valuable feedback, and especially\nDr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.", 
            "title": "Result Analysis with Python"
        }, 
        {
            "location": "/tutorials/pandas/#1-when-to-use-python", 
            "text": "The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of\nsimulation results. If you are doing sophisticated result analysis, you will\nnotice after a while that you have outgrown the IDE. The need for customized\ncharts, the necessity of multi-step computations to produce chart input, or the\nsheer volume of raw simulation results might all be causes to make you look for\nsomething else.  If you are an R or Matlab expert, you'll probably reach for those tools, but for\neveryone else, Python with the right libraries is pretty much the best choice.\nPython has a big momentum for data science, and in addition to having excellent\nlibraries for data analysis and visualization, it is also a great general-purpose\nprogramming language. Python is used for diverse problems ranging from building\ndesktop GUIs to machine learning and AI, so the knowledge you gain by learning\nit will be convertible to other areas.  This tutorial will walk you through the initial steps of using Python for\nanalysing simulation results, and shows how to do some of the most common tasks.\nThe tutorial assumes that you have a working knowledge of OMNeT++ with regard\nto result recording, and basic familiarity with Python.", 
            "title": "1. When to use Python?"
        }, 
        {
            "location": "/tutorials/pandas/#2-setting-up", 
            "text": "Before we can start, you need to install the necessary software.\nFirst, make sure you have Python, either version 2.x or 3.x (they are\nslightly incompatible.) If you have both versions available on your system,\nwe recommend version 3.x. You also need OMNeT++ version 5.2 or later.  We will heavily rely on three Python packages:  NumPy , Pandas , and  Matplotlib .\nThere are also optional packages that will be useful for certain tasks: SciPy , PivotTable.js .\nWe also recommend that you install  IPython  and Jupyter , because they let you work much more comfortably\nthan the bare Python shell.  On most systems, these packages can be installed with  pip , the Python package\nmanager (if you go for Python 3, replace  pip  with  pip3  in the commands\nbelow):  sudo pip install ipython jupyter\nsudo pip install numpy pandas matplotlib\nsudo pip install scipy pivottablejs  As packages continually evolve, there might be incompatibilities between\nversions. We used the following versions when writing this tutorial:\nPandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0.\nAn easy way to determine which versions you have installed is using the  pip list \ncommand. (Note that the last one is the version of the Python interface library,\nthe PivotTable.js main Javascript library uses different version numbers, e.g.\n2.7.0.)", 
            "title": "2. Setting up"
        }, 
        {
            "location": "/tutorials/pandas/#3-getting-your-simulation-results-into-python", 
            "text": "OMNeT++ result files have their own file format which is not directly\ndigestible by Python. There are a number of ways to get your data\ninside Python:    Export from the IDE. The Analysis Tool can export data in a number of\n  formats, the ones that are useful here are CSV and Python-flavoured JSON.\n  In this tutorial we'll use the CSV export, and read the result into Pandas\n  using its  read_csv()  function.    Export using scavetool. Exporting from the IDE may become tedious\n  after a while, because you have to go through the GUI every time your\n  simulations are re-run. Luckily, you can automate the exporting with\n  OMNeT++'s scavetool program. scavetool exposes the same export\n  functionality as the IDE, and also allows filtering of the data.    Read the OMNeT++ result files directly from Python. Development\n  of a Python package to read these files into Pandas data frames is\n  underway, but given that these files are line-oriented text files\n  with a straightforward and well-documented structure, writing your\n  own custom reader is also a perfectly feasible option.    SQLite. Since version 5.1, OMNeT++ has the ability to record simulation\n  results int SQLite3 database files, which can be opened directly from\n  Python using the  sqlite \n  package. This lets you use SQL queries to select the input data for your\n  charts or computations, which is kind of cool! You can even use GUIs like\n   SQLiteBrowser  to browse the database and\n  craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3\n  output, you'll still get  .vec  and  .sca  files as before, only their\n  format will change from textual to SQLite's binary format. When querying\n  the contents of the files, one issue  to deal with is that SQLite does not\n  allow cross-database queries, so you either need to configure OMNeT++\n  to record everything into one file (i.e. each run should append instead\n  of creating a new file), or use scavetool's export functionality to\n  merge the files into one.    Custom result recording. There is also the option to instrument\n  the simulation (via C++ code) or OMNeT++ (via custom result recorders)\n  to produce files that Python can directly digest, e.g. CSV.\n  However, in the light of the above options, it is rarely necessary\n  to go this far.    With large-scale simulation studies, it can easily happen that the\nfull set of simulation results do not fit into the memory at once.\nThere are also multiple approaches to deal with this problem:   If you don't need all simulation results for the analysis, you can\n  configure OMNeT++ to record only a subset of them. Fine-grained control\n  is available.  Perform filtering and aggregation steps before analysis. The IDE and\n  scavetool are both capable of filtering the results before export.  When the above approaches are not enough, it can help to move\n  part of the result processing (typically, filtering and aggregation)\n  into the simulation model as dedicated result collection modules.\n  However, this solution requires significantly more work than the previous\n  two, so use with care.   In this tutorial, we'll work with the contents of the  samples/resultfiles \ndirectory distributed with OMNeT++. The directory contains result\nfiles produced by the Aloha and Routing sample simulations, both\nof which are parameter studies. We'll start by looking at the Aloha results.  As the first step, we use OMNeT++'s  scavetool  to convert Aloha's scalar files\nto CSV. Run the following commands in the terminal (replace  ~/omnetpp  with\nthe location of your OMNeT++ installation):  cd ~/omnetpp/samples/resultfiles/aloha\nscavetool x *.sca -o aloha.csv  In the scavetool command line,  x  means export, and the export format is\ninferred from the output file's extension. (Note that scavetool supports\ntwo different CSV output formats. We need  CSV Records , or CSV-R for short,\nwhich is the default for the  .csv  extension.)  Let us spend a minute on what the export has created. The CSV file\nhas a fixed number of columns named  run ,  type ,  module ,  name , value , etc. Each result item, i.e. scalar, statistic, histogram\nand vector, produces one row of output in the CSV. Other items such\nas run attributes, iteration variables of the parameter study and result\nattributes also generate their own rows. The content of the  type  column\ndetermines what type of information a given row contains. The  type \ncolumn also determines which other columns are in use. For example,\nthe  binedges  and  binvalues  columns are only filled in for histogram\nitems. The colums are:   run : Identifies the simulation run  type : Row type, one of the following:  scalar ,  vector ,  statistics ,\n   histogram ,  runattr ,  itervar ,  param ,  attr  module : Hierarchical name (a.k.a. full path) of the module that recorded the\n  result item  name : Name of the result item (scalar, statistic, histogram or vector)  attrname : Name of the run attribute or result item attribute (in the latter\n  case, the  module  and  name  columns identify the result item the attribute\n  belongs to)  attrvalue : Value of run and result item attributes, iteration variables,\n  saved ini param settings ( runattr ,  attr ,  itervar ,  param )  value : Output scalar value  count ,  sumweights ,  mean ,  min ,  max ,  stddev : Fields of the statistics\n  or histogram  binedges ,  binvalues : Histogram bin edges and bin values, as space-separated\n  lists.  len(binedges)==len(binvalues)+1  vectime ,  vecvalue : Output vector time and value arrays, as space-separated\n  lists   When the export is done, you can start Jupyter server with the following command:  jupyter notebook  Open a web browser with the displayed URL to access the Jupyter GUI. Once there,\nchoose  New  -   Python3  in the top right corner to open a blank notebook.\nThe notebook allows you to enter Python commands or sequences of commands,\nrun them, and view the output. Note that  Enter  simply inserts a newline;\nhit  Ctrl+Enter  to execute the commands in the current cell, or  Alt+Enter \nto execute them and also insert a new cell below.  If you cannot use Jupyter for some reason, a terminal-based Python shell\n( python  or  ipython ) will also allow you to follow the tutorial.  On the Python prompt, enter the following lines to make the functionality of\nPandas, NumpPy and Matplotlib available in the session. The last,  %matplotlib \nline is only needed for Jupyter. (It is a \"magic command\" that arranges plots\nto be displayed within the notebook.)  In[1]:  import   pandas   as   pd  import   numpy   as   np  import   matplotlib.pyplot   as   plt  % matplotlib   inline   We utilize the  read_csv()  function to import the contents of the\nCSV file into a data frame. The data frame is the central concept of\nPandas. We will continue to work with this data frame throughout\nthe whole tutorial.  In[2]:  aloha   =   pd . read_csv ( aloha.csv )", 
            "title": "3. Getting your simulation results into Python"
        }, 
        {
            "location": "/tutorials/pandas/#4-exploring-the-data-frame", 
            "text": "You can view the contents of the data frame by simply entering the name\nof the variable ( aloha ). Alternatively, you can use the  head()  method\nof the data frame to view just the first few lines.  In[3]:  aloha . head ()   Out[3]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n     \n   \n   \n     \n       0 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       configname \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       datetime \n       20170627-20:42:20 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       2 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       experiment \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       3 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       inifile \n       omnetpp.ini \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       4 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       iterationvars \n       numHosts=10, iaMean=3 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n       You can see that the structure of the data frame, i.e. rows and columns,\ndirectly corresponds to the contents of the CSV file. Column names have\nbeen taken from the first line of the CSV file. Missing values are\nrepresented with NaNs (not-a-number).  The complementary  tail()  method shows the last few lines. There is also\nan  iloc  method that we use at places in this tutorial to show rows\nfrom the middle of the data frame. It accepts a range:  aloha.iloc[20:30] \nselects 10 lines from line 20,  aloha.iloc[:5]  is like  head() , and aloha.iloc[-5:]  is like  tail() .  In[4]:  aloha . iloc [ 1200 : 1205 ]   Out[4]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n     \n   \n   \n     \n       1200 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       scalar \n       Aloha.server \n       collidedFrames:last \n       NaN \n       NaN \n       40692.000000 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1201 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       attr \n       Aloha.server \n       collidedFrames:last \n       source \n       sum(collision) \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1202 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       attr \n       Aloha.server \n       collidedFrames:last \n       title \n       collided frames, last \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1203 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.156176 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1204 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       attr \n       Aloha.server \n       channelUtilization:last \n       interpolationmode \n       linear \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n       Hint: If you are in the terminal and you find that the data frame printout does\nnot make use of the whole width of the terminal, you can increase the display\nwidth for better readability with the following commands:  In[5]:  pd . set_option ( display.width ,   180 )  pd . set_option ( display.max_colwidth ,   100 )   If you have not looked at any Pandas tutorial yet, now is a very good\ntime to read one. (See References at the bottom of this page for hints.)\nUntil you finish, here are some basics for your short-term survival.  You can refer to a column as a whole with the array index syntax:  aloha[ run ] .\nAlternatively, the more convenient member access syntax ( aloha.run ) can\nalso be used, with restrictions. (E.g. the column name must be valid as a Python\nidentifier, and should not collide with existing methods of the data frame.\nNames that are known to cause trouble include  name ,  min ,  max ,  mean ).  In[6]:  aloha . run . head ()    # .head() is for limiting the output to 5 lines here   Out[6]:  0    PureAlohaExperiment-4-20170627-20:42:20-22739\n1    PureAlohaExperiment-4-20170627-20:42:20-22739\n2    PureAlohaExperiment-4-20170627-20:42:20-22739\n3    PureAlohaExperiment-4-20170627-20:42:20-22739\n4    PureAlohaExperiment-4-20170627-20:42:20-22739\nName: run, dtype: object  Selecting multiple columns is also possible, one just needs to use a list of\ncolumn names as index. The result will be another data frame. (The double\nbrackets in the command are due to the fact that both the array indexing and\nthe list syntax use square brackets.)  In[7]:  tmp   =   aloha [[ run ,   attrname ,   attrvalue ]]  tmp . head ()   Out[7]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       attrname \n       attrvalue \n     \n   \n   \n     \n       0 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       configname \n       PureAlohaExperiment \n     \n     \n       1 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       datetime \n       20170627-20:42:20 \n     \n     \n       2 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       experiment \n       PureAlohaExperiment \n     \n     \n       3 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       inifile \n       omnetpp.ini \n     \n     \n       4 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       iterationvars \n       numHosts=10, iaMean=3 \n     \n       The  describe()  method can be used to get an idea about the contents of a\ncolumn. When applied to a non-numeric column, it prints the number of\nnon-null elements in it ( count ), the number of unique values ( unique ),\nthe most frequently occurring value ( top ) and its multiplicity ( freq ),\nand the inferred data type (more about that later.)  In[8]:  aloha . module . describe ()   Out[8]:  count             1012\nunique              11\ntop       Aloha.server\nfreq               932\nName: module, dtype: object  You can get a list of the unique values using the  unique()  method. For example,\nthe following command lists the names of modules that have recorded any statistics:  In[9]:  aloha . module . unique ()   Out[9]:  array([nan,  Aloha.server ,  Aloha.host[0] ,  Aloha.host[1] ,\n        Aloha.host[2] ,  Aloha.host[3] ,  Aloha.host[4] ,  Aloha.host[5] ,\n        Aloha.host[6] ,  Aloha.host[7] ,  Aloha.host[8] ,  Aloha.host[9] ], dtype=object)  When you apply  describe()  to a numeric column, you get a statistical summary\nwith things like mean, standard deviation, minimum, maximum, and various\nquantiles.  In[10]:  aloha . value . describe ()   Out[10]:  count      294.000000\nmean      4900.038749\nstd      11284.077075\nmin          0.045582\n25%          0.192537\n50%        668.925298\n75%       5400.000000\nmax      95630.000000\nName: value, dtype: float64  Applying  describe()  to the whole data frame creates a similar report about\nall numeric columns.  In[11]:  aloha . describe ()   Out[11]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n     \n   \n   \n     \n       count \n       294.000000 \n       84.000000 \n       0.0 \n       84.000000 \n       84.000000 \n       84.000000 \n       84.000000 \n     \n     \n       mean \n       4900.038749 \n       5591.380952 \n       NaN \n       1.489369 \n       0.599396 \n       1.049606 \n       6.560987 \n     \n     \n       std \n       11284.077075 \n       4528.796760 \n       NaN \n       1.530455 \n       0.962515 \n       0.956102 \n       9.774404 \n     \n     \n       min \n       0.045582 \n       470.000000 \n       NaN \n       0.152142 \n       0.031326 \n       0.099167 \n       0.272013 \n     \n     \n       25% \n       0.192537 \n       1803.000000 \n       NaN \n       0.164796 \n       0.049552 \n       0.099186 \n       0.498441 \n     \n     \n       50% \n       668.925298 \n       4065.500000 \n       NaN \n       1.197140 \n       0.243035 \n       1.049776 \n       3.084077 \n     \n     \n       75% \n       5400.000000 \n       8815.000000 \n       NaN \n       2.384397 \n       0.741081 \n       2.000000 \n       9.000000 \n     \n     \n       max \n       95630.000000 \n       14769.000000 \n       NaN \n       6.936747 \n       5.323887 \n       2.000000 \n       54.000000 \n     \n       Let's spend a minute on data types and column data types. Every column has a\ndata type (abbreviated  dtype ) that determines what type of values it may\ncontain. Column dtypes can be printed with  dtypes :  In[12]:  aloha . dtypes   Out[12]:  run            object\ntype           object\nmodule         object\nname           object\nattrname       object\nattrvalue      object\nvalue         float64\ncount         float64\nsumweights    float64\nmean          float64\nstddev        float64\nmin           float64\nmax           float64\nbinedges       object\nbinvalues      object\nvectime        object\nvecvalue       object\ndtype: object  The two most commonly used dtypes are  float64  and  object . A  float64  column\ncontains floating-point numbers, and missing values are represented with NaNs.\nAn  object  column may contain basically anything -- usually strings, but we'll\nalso have NumPy arrays ( np.ndarray ) as elements in this tutorial.\nNumeric values and booleans may also occur in an  object  column. Missing values\nin an  object  column are usually represented with  None , but Pandas also\ninterprets the floating-point NaN like that.\nSome degree of confusion arises from fact that some Pandas functions check\nthe column's dtype, while others are already happy if the contained elements\nare of the required type. To clarify: applying  describe()  to a column\nprints a type inferred from the individual elements,  not  the column dtype.\nThe column dtype type can be changed with the  astype()  method; we'll see an\nexample for using it later in this tutorial.  The column dtype can be accessed as the  dtype  property of a column, for example aloha.stddev.dtype  yields  dtype( float64 ) . There are also convenience\nfunctions such as  is_numeric_dtype()  and  is_string_dtype()  for checking\ncolumn dtype. (They need to be imported from the  pandas.api.types  package\nthough.)  Another vital thing to know, especially due of the existence of the  type \ncolumn in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly,\nthe array index syntax can be used here as well. For example, the following expression\nselects the rows that contain iteration variables:  aloha[aloha.type ==  itervar ] .\nWith a healthy degree of sloppiness, here's how it works:  aloha.type  yields\nthe values in the  type  column as an array-like data structure; aloha.type== itervar  performs element-wise comparison and produces an array\nof booleans containing  True  where the condition holds and  False  where not;\nand indexing a data frame with an array of booleans returns the rows that\ncorrespond to  True  values in the array.  Conditions can be combined with AND/OR using the \" \" and \" | \" operators, but\nyou need parentheses because of operator precedence. The following command\nselects the rows that contain scalars with a certain name and owner module:  In[13]:  tmp   =   aloha [( aloha . type == scalar )     ( aloha . module == Aloha.server )     ( aloha . name == channelUtilization:last )]  tmp . head ()   Out[13]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n     \n   \n   \n     \n       1186 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.156057 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1203 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.156176 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1220 \n       PureAlohaExperiment-2-20170627-20:42:19-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.196381 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1237 \n       PureAlohaExperiment-3-20170627-20:42:20-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.193253 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n     \n       1254 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       scalar \n       Aloha.server \n       channelUtilization:last \n       NaN \n       NaN \n       0.176507 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n     \n       You'll also need to know how to add a new column to the data frame. Now that is\na bit controversial topic, because at the time of writing, there is a \"convenient\"\nsyntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple\nassignment, for example:  In[14]:  aloha [ qname ]   =   aloha . module   +   .   +   aloha . name  aloha [ aloha . type == scalar ] . head ()    # print excerpt   Out[14]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n       qname \n     \n   \n   \n     \n       1176 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       duration \n       NaN \n       NaN \n       5400.000000 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.duration \n     \n     \n       1177 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:mean \n       NaN \n       NaN \n       0.198275 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:mean \n     \n     \n       1179 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:sum \n       NaN \n       NaN \n       2457.026781 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:sum \n     \n     \n       1181 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:max \n       NaN \n       NaN \n       0.901897 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:max \n     \n     \n       1183 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collidedFrames:last \n       NaN \n       NaN \n       40805.000000 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collidedFrames:last \n     \n       It looks nice and natural, but it is not entirely correct. It often results in\na warning:  SettingWithCopyWarning: A value is trying to be set on a copy of a\nslice from a DataFrame... . The message essentially says that the operation\n(here, adding the new column) might have been applied to a temporary object\ninstead of the original data frame, and thus might have been ineffective.\nLuckily, that is not the case most of the time (the operation  does  take\neffect). Nevertheless, for production code, i.e. scripts, the \"official\"\nsolution, the  assign()  method of the data frame is recommended, like this:  In[15]:  aloha   =   aloha . assign ( qname   =   aloha . module   +   .   +   aloha . name )  aloha [ aloha . type == scalar ] . head ()   Out[15]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n       qname \n     \n   \n   \n     \n       1176 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       duration \n       NaN \n       NaN \n       5400.000000 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.duration \n     \n     \n       1177 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:mean \n       NaN \n       NaN \n       0.198275 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:mean \n     \n     \n       1179 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:sum \n       NaN \n       NaN \n       2457.026781 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:sum \n     \n     \n       1181 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collisionLength:max \n       NaN \n       NaN \n       0.901897 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collisionLength:max \n     \n     \n       1183 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server \n       collidedFrames:last \n       NaN \n       NaN \n       40805.000000 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       Aloha.server.collidedFrames:last \n     \n       For completeness, one can remove a column from a data frame using either the del  operator or the  drop()  method of the data frame. Here we show the former\n(also to remove the column we added above, as we won't need it for now):  In[16]:  del   aloha [ qname ]", 
            "title": "4. Exploring the data frame"
        }, 
        {
            "location": "/tutorials/pandas/#5-revisiting-csv-loading", 
            "text": "The way we have read the CSV file has one small deficiency: all data in the attrvalue  column are represented as strings, event though many of them\nare really numbers, for example the values of the  iaMean  and  numHosts \niteration variables. You can verify that by printing the unique values ( aloha.attrvalue.unique()  -- it will print all values with quotes), or using\nthe  type()  operator on an element:  In[17]:  type (   aloha [ aloha . type == scalar ] . iloc [ 0 ] . value   )   Out[17]:  numpy.float64  The reason is that  read_csv()  infers data types of columns from the data\nit finds in them. Since the  attrvalue  column is shared by run attributes,\nresult item attributes, iteration variables and some other types of rows,\nthere are many non-numeric strings in it, and  read_csv()  decides that it is\na string column.  A similar issue arises with the  binedges ,  binvalues ,  vectime ,  vecvalue \ncolumns. These columns contain lists of numbers separated by spaces, so they\nare read into strings as well. However, we would like to store them as NumPy\narrays ( ndarray ) inside the data frame, because that's the form we can use\nin plots or as computation input.  Luckily,  read_csv()  allows us to specify conversion functions for each column.\nSo, armed with the following two short functions:  In[18]:  def   parse_if_number ( s ): \n     try :   return   float ( s ) \n     except :   return   True   if   s == true   else   False   if   s == false   else   s   if   s   else   None  def   parse_ndarray ( s ): \n     return   np . fromstring ( s ,   sep =   )   if   s   else   None   we can read the CSV file again, this time with the correct conversions:  In[19]:  aloha   =   pd . read_csv ( aloha.csv ,   converters   =   { \n     attrvalue :   parse_if_number , \n     binedges :   parse_ndarray , \n     binvalues :   parse_ndarray , \n     vectime :   parse_ndarray , \n     vecvalue :   parse_ndarray })   You can verify the result e.g. by printing the unique values again.", 
            "title": "5. Revisiting CSV loading"
        }, 
        {
            "location": "/tutorials/pandas/#6-load-time-filtering", 
            "text": "If the CSV file is large, you may want to skip certain columns or rows when\nreading it into memory. (File size is about the only valid reason for using\nload-time filtering, because you can also filter out or drop rows/columns\nfrom the data frame when it is already loaded.)  To filter out columns, you need to specify in the  usecols  parameter\nthe list of columns to keep:  In[20]:  tmp   =   pd . read_csv ( aloha.csv ,   usecols = [ run ,   type ,   module ,   name ,   value ])   There is no such direct support for filtering out rows based on their content,\nbut we can implement it using the iterator API that reads the CSV file\nin chunks. We can filter each chunk before storing and finally concatenating\nthem into a single data frame:  In[21]:  iter   =   pd . read_csv ( aloha.csv ,   iterator = True ,   chunksize = 100 )  chunks   =   [   chunk [ chunk [ type ] != histogram ]   for   chunk   in   iter   ]    # discards type== histogram  lines  tmp   =   pd . concat ( chunks )", 
            "title": "6. Load-time filtering"
        }, 
        {
            "location": "/tutorials/pandas/#7-plotting-scalars", 
            "text": "Scalars can serve as input for many different kinds of plots. Here we'll show\nhow one can create a \"throughput versus offered load\" type plot. We will plot\nthe channel utilization in the Aloha model in the function of the packet\ngeneration frequency. Channel utilization is also affected by the number of\nhosts in the network -- we want results belonging to the same number of hosts\nto form iso lines. Packet generation frequency and the number of hosts are\npresent in the results as iteration variables named  iaMean  and  numHosts ;\nchannel utilization values are the  channelUtilization : last  scalars saved\nby the  Aloha.server  module. The data contains the results from two simulation\nruns for each  (iaMean, numHosts)  pair done with different seeds; we want\nto average them for the plot.  The first few steps are fairly straightforward. We only need the scalars and the\niteration variables from the data frame, so we filter out the rest. Then we\ncreate a  qname  column from other columns to hold the names of our variables:\nthe names of scalars are in the  module  and  name  columns (we want to join them\nwith a dot), and the names of iteration variables are in the  attrname  column.\nSince  attrname  is not filled in for scalar rows, we can take  attrname  as qname \nfirst, then fill in the holes with  module.name . We use the  combine_first() \nmethod for that:  a.combine_first(b)  fills the holes in  a  using the\ncorresponding values from  b .  The similar issue arises with values: values of output scalars are in the  value \ncolumn, while that of iteration variables are in the  attrvalue  column.\nSince  attrvalue  is unfilled for scalar rows, we can again utilize combine_first()  to merge two. There is one more catch: we need to change\nthe dtype of the  attrvalue  to  float64 , otherwise the resulting  value \ncolumn also becomes  object  dtype. (Luckily, all our iteration variables are\nnumeric, so the dtype conversion is possible. In other simulations that contain\nnon-numeric itervars, one needs to filter those out, force them into numeric\nvalues somehow, or find some other trick to make things work.)  In[22]:  scalars   =   aloha [( aloha . type == scalar )   |   ( aloha . type == itervar )]    # filter rows  scalars   =   scalars . assign ( qname   =   scalars . attrname . combine_first ( scalars . module   +   .   +   scalars . name ))    # add qname column  scalars . value   =   scalars . value . combine_first ( scalars . attrvalue . astype ( float64 ))    # merge value columns  scalars [[ run ,   type ,   qname ,   value ,   module ,   name ,   attrname ]] . iloc [ 80 : 90 ]    # print an excerpt of the result   Out[22]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       qname \n       value \n       module \n       name \n       attrname \n     \n   \n   \n     \n       1134 \n       PureAlohaExperiment-40-20170627-20:42:22-22773 \n       itervar \n       iaMean \n       9.000000 \n       NaN \n       NaN \n       iaMean \n     \n     \n       1135 \n       PureAlohaExperiment-40-20170627-20:42:22-22773 \n       itervar \n       numHosts \n       20.000000 \n       NaN \n       NaN \n       numHosts \n     \n     \n       1162 \n       PureAlohaExperiment-41-20170627-20:42:22-22773 \n       itervar \n       iaMean \n       9.000000 \n       NaN \n       NaN \n       iaMean \n     \n     \n       1163 \n       PureAlohaExperiment-41-20170627-20:42:22-22773 \n       itervar \n       numHosts \n       20.000000 \n       NaN \n       NaN \n       numHosts \n     \n     \n       1176 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.duration \n       5400.000000 \n       Aloha.server \n       duration \n       NaN \n     \n     \n       1177 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.collisionLength:mean \n       0.198275 \n       Aloha.server \n       collisionLength:mean \n       NaN \n     \n     \n       1179 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.collisionLength:sum \n       2457.026781 \n       Aloha.server \n       collisionLength:sum \n       NaN \n     \n     \n       1181 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.collisionLength:max \n       0.901897 \n       Aloha.server \n       collisionLength:max \n       NaN \n     \n     \n       1183 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.collidedFrames:last \n       40805.000000 \n       Aloha.server \n       collidedFrames:last \n       NaN \n     \n     \n       1186 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       scalar \n       Aloha.server.channelUtilization:last \n       0.156057 \n       Aloha.server \n       channelUtilization:last \n       NaN \n     \n       To work further, it would be very convenient if we had a format where each\nsimulation run corresponds to one row, and all variables produced by that\nrun had their own columns. We can call it the  wide  format, and it can be\nproduced using the  pivot()  method:  In[23]:  scalars_wide   =   scalars . pivot ( run ,   columns = qname ,   values = value )  scalars_wide . head ()   Out[23]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       qname \n       Aloha.server.channelUtilization:last \n       Aloha.server.collidedFrames:last \n       Aloha.server.collisionLength:max \n       Aloha.server.collisionLength:mean \n       Aloha.server.collisionLength:sum \n       Aloha.server.duration \n       Aloha.server.receivedFrames:last \n       iaMean \n       numHosts \n     \n     \n       run \n       \n       \n       \n       \n       \n       \n       \n       \n       \n     \n   \n   \n     \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       0.156057 \n       40805.0 \n       0.901897 \n       0.198275 \n       2457.026781 \n       5400.0 \n       8496.0 \n       1.0 \n       10.0 \n     \n     \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       0.156176 \n       40692.0 \n       0.958902 \n       0.198088 \n       2456.494983 \n       5400.0 \n       8503.0 \n       1.0 \n       10.0 \n     \n     \n       PureAlohaExperiment-10-20170627-20:42:16-22741 \n       0.109571 \n       1760.0 \n       0.326138 \n       0.155154 \n       126.450220 \n       5400.0 \n       5965.0 \n       7.0 \n       10.0 \n     \n     \n       PureAlohaExperiment-11-20170627-20:42:16-22741 \n       0.108992 \n       1718.0 \n       0.340096 \n       0.154529 \n       125.477252 \n       5400.0 \n       5934.0 \n       7.0 \n       10.0 \n     \n     \n       PureAlohaExperiment-12-20170627-20:42:16-22741 \n       0.090485 \n       1069.0 \n       0.272013 \n       0.152142 \n       78.201174 \n       5400.0 \n       4926.0 \n       9.0 \n       10.0 \n     \n       We are interested in only three columns for our plot:  In[24]:  scalars_wide [[ numHosts ,   iaMean ,   Aloha.server.channelUtilization:last ]] . head ()   Out[24]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       qname \n       numHosts \n       iaMean \n       Aloha.server.channelUtilization:last \n     \n     \n       run \n       \n       \n       \n     \n   \n   \n     \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       10.0 \n       1.0 \n       0.156057 \n     \n     \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       10.0 \n       1.0 \n       0.156176 \n     \n     \n       PureAlohaExperiment-10-20170627-20:42:16-22741 \n       10.0 \n       7.0 \n       0.109571 \n     \n     \n       PureAlohaExperiment-11-20170627-20:42:16-22741 \n       10.0 \n       7.0 \n       0.108992 \n     \n     \n       PureAlohaExperiment-12-20170627-20:42:16-22741 \n       10.0 \n       9.0 \n       0.090485 \n     \n       Since we have our  x  and  y  data in separate columns now, we can utilize the\nscatter plot feature of the data frame for plotting it:  In[25]:  # set the default image resolution and size  plt . rcParams [ figure.figsize ]   =   [ 8.0 ,   3.0 ]  plt . rcParams [ figure.dpi ]   =   144  # create a scatter plot  scalars_wide . plot . scatter ( iaMean ,   Aloha.server.channelUtilization:last )  plt . show ()   Out[25]:   NOTE: Although  plt.show()  is not needed in Jupyter ( %matplotlib   inline \nturns on immediate display), we'll continue to include it in further code\nfragments, so that they work without change when you use another Python shell.  The resulting chart looks quite good as the first attempt. However, it has some\nshortcomings:   Dots are not connected. The dots that have the same  numHosts  value should\n  be connected with iso lines.  As the result of having two simulation runs for each  (iaMean,numHosts)  pair,\n  the dots appear in pairs. We'd like to see their averages instead.   Unfortunately, scatter plot can only take us this far, we need to look for\nanother way.  What we really need as chart input is a table where rows correspond to different iaMean  values, columns correspond to different  numHosts  values, and cells\ncontain channel utilization values (the average of the repetitions).\nSuch table can be produced from the \"wide format\" with another pivoting\noperation. We use  pivot_table() , a cousin of the  pivot()  method we've seen above.\nThe difference between them is that  pivot()  is a reshaping operation (it just\nrearranges elements), while  pivot_table()  is more of a spreadsheet-style\npivot table creation operation, and primarily intended for numerical data. pivot_table()  accepts an aggregation function with the default being  mean ,\nwhich is quite convenient for us now (we want to average channel utilization\nover repetitions.)  In[26]:  aloha_pivot   =   scalars_wide . pivot_table ( index = iaMean ,   columns = numHosts ,   values = Aloha.server.channelUtilization:last )    # note: aggregation function = mean (that s the default)  aloha_pivot . head ()   Out[26]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       numHosts \n       10.0 \n       15.0 \n       20.0 \n     \n     \n       iaMean \n       \n       \n       \n     \n   \n   \n     \n       1.0 \n       0.156116 \n       0.089539 \n       0.046586 \n     \n     \n       2.0 \n       0.194817 \n       0.178159 \n       0.147564 \n     \n     \n       3.0 \n       0.176321 \n       0.191571 \n       0.183976 \n     \n     \n       4.0 \n       0.153569 \n       0.182324 \n       0.190452 \n     \n     \n       5.0 \n       0.136997 \n       0.168780 \n       0.183742 \n     \n       Note that rows correspond to various  iaMean  values ( iaMean  serves as index);\nthere is one column for each value of  numHosts ; and that data in the table\nare the averages of the channel utilizations produced by the simulations\nperformed with the respective  iaMean  and  numHosts  values.  For the plot, every column should generate a separate line (with the  x  values\ncoming from the index column,  iaMean ) labelled with the column name.\nThe basic Matplotlib interface cannot create such plot in one step. However,\nthe Pandas data frame itself has a plotting interface which knows how to\ninterpret the data, and produces the correct plot without much convincing:  In[27]:  aloha_pivot . plot . line ()  plt . ylabel ( channel utilization )  plt . show ()   Out[27]:", 
            "title": "7. Plotting scalars"
        }, 
        {
            "location": "/tutorials/pandas/#8-interactive-pivot-tables", 
            "text": "Getting the pivot table right is not always easy, so having a GUI where\none can drag columns around and immediately see the result is definitely\na blessing. Pivottable.js presents such a GUI inside a browser, and\nalthough the bulk of the code is Javascript, it has a Python frond-end\nthat integrates nicely with Jupyter. Let's try it!  In[28]:  import   pivottablejs   as   pj  pj . pivot_ui ( scalars_wide )   Out[28]:  \n\n           An interactive panel containing the pivot table will appear. Here is how\nyou can reproduce the above \"Channel utilization vs iaMean\" plot in it:   Drag  numHosts  to the \"rows\" area of the pivot table.\n   The table itself is the area on the left that initially only displays \"Totals | 42\",\n   and the \"rows\" area is the empty rectangle directly of left it.\n   The table should show have two columns ( numHosts  and  Totals ) and\n   five rows in total after dragging.  Drag  iaMean  to the \"columns\" area (above the table). Columns for each value\n   of  iaMean  should appear in the table.  Near the top-left corner of the table, select  Average  from the combo box\n   that originally displays  Count , and select  ChannelUtilization : last \n   from the combo box that appears below it.  In the top-left corner of the panel, select  Line Chart  from the combo box\n   that originally displays  Table .   If you can't get to see it, the following command will programmatically\nconfigure the pivot table in the appropriate way:  In[29]:  pj . pivot_ui ( scalars_wide ,   rows = [ numHosts ],   cols = [ iaMean ],   vals = [ Aloha.server.channelUtilization:last ],   aggregatorName = Average ,   rendererName = Line Chart )   Out[29]:  \n\n           If you want experiment with Excel's or LibreOffice's built-in pivot table\nfunctionality, the data frame's  to_clipboard()  and  to_csv()  methods\nwill help you transfer the data. For example, you can issue the scalars_wide.to_clipboard()  command to put the data on the clipboard, then\npaste it into the spreadsheet. Alternatively, type  print(scalars_wide.to_csv()) \nto print the data in CSV format that you can select and then copy/paste.\nOr, use  scalars_wide.to_csv( scalars.csv )  to save the data into a file\nwhich you can import.", 
            "title": "8. Interactive pivot tables"
        }, 
        {
            "location": "/tutorials/pandas/#9-plotting-histograms", 
            "text": "In this section we explore how to plot histograms recorded by the simulation.\nHistograms are in rows that have  histogram  in the  type  column.\nHistogram bin edges and bin values (counts) are in the  binedges  and binvalues  columns as NumPy array objects ( ndarray ).  Let us begin by selecting the histograms into a new data frame for convenience.  In[30]:  histograms   =   aloha [ aloha . type == histogram ]  len ( histograms )   Out[30]:  84  We have 84 histograms. It makes no sense to plot so many histograms on one chart,\nso let's just take one on them, and examine its content.  In[31]:  hist   =   histograms . iloc [ 0 ]    # the first histogram  hist . binedges ,   hist . binvalues   Out[31]:  (array([-0.11602833, -0.08732314, -0.05861794, -0.02991275, -0.00120756,\n         0.02749763,  0.05620283,  0.08490802,  0.11361321,  0.1423184 ,\n         0.1710236 ,  0.19972879,  0.22843398,  0.25713917,  0.28584437,\n         0.31454956,  0.34325475,  0.37195994,  0.40066514,  0.42937033,\n         0.45807552,  0.48678071,  0.51548591,  0.5441911 ,  0.57289629,\n         0.60160148,  0.63030668,  0.65901187,  0.68771706,  0.71642225,\n         0.74512745]),\n array([    0.,     0.,     0.,     0.,     0.,     0.,     0.,  1234.,\n         2372.,  2180.,  2115.,  1212.,   917.,   663.,   473.,   353.,\n          251.,   186.,   123.,    99.,    60.,    44.,    31.,    25.,\n           15.,    13.,     9.,     3.,     5.,     3.]))  The easiest way to plot the histogram from these two arrays is to look at it\nas a step function, and create a line plot with the appropriate drawing style.\nThe only caveat is that we need to add an extra  0  element to draw the right\nside of the last histogram bin.  In[32]:  plt . plot ( hist . binedges ,   np . append ( hist . binvalues ,   0 ),   drawstyle = steps-post )     # or maybe steps-mid, for integers  plt . show ()   Out[32]:   Another way to plot a recorded histogram is Matplotlib's  hist()  method,\nalthough that is a bit tricky. Instead of taking histogram data,  hist() \ninsists on computing the histogram itself from an array of values -- but we only\nhave the histogram, and not the data it was originally computed from.\nFortunately,  hist()  can accept a bin edges array, and another array as weights\nfor the values. Thus, we can trick it into doing what we want by passing\nin our  binedges  array twice, once as bin edges and once as values, and\nspecifying  binvalues  as weights.  In[33]:  plt . hist ( bins = hist . binedges ,   x = hist . binedges [: - 1 ],   weights = hist . binvalues )  plt . show ()   Out[33]:   hist()  has some interesting options. For example, we can change the plotting\nstyle to be similar to a line plot by setting  histtype= step . To plot the\nnormalized version of the histogram, specify  normed=True  or  density=True \n(they work differently; see the Matplotlib documentation for details).\nTo draw the cumulative density function, also specify  cumulative=True .\nThe following plot shows the effect of some of these options.  In[34]:  plt . hist ( bins = hist . binedges ,   x = hist . binedges [: - 1 ],   weights = hist . binvalues ,   histtype = step ,   normed = True )  plt . show ()   Out[34]:   To plot several histograms, we can iterate over the histograms and draw them\none by one on the same plot. The following code does that, and also adds a\nlegend and adjusts the bounds of the x axis.  In[35]:  somehistograms   =   histograms [ histograms . name   ==   collisionLength:histogram ][: 5 ]  for   row   in   somehistograms . itertuples (): \n     plt . plot ( row . binedges ,   np . append ( row . binvalues ,   0 ),   drawstyle = steps-post )  plt . legend ( somehistograms . module   +   .   +   somehistograms . name )  plt . xlim ( 0 ,   0.5 )  plt . show ()   Out[35]:   Note, however, that the legend contains the same string for all histograms,\nwhich is not very meaningful. We could improve that by including some\ncharacteristics of the simulation that generated them, i.e. the number of hosts\n( numHosts  iteration variable) and frame interarrival times ( iaTime  iteration\nvariable). We'll see in the next section how that can be achieved.", 
            "title": "9. Plotting histograms"
        }, 
        {
            "location": "/tutorials/pandas/#10-adding-iteration-variables-as-columns", 
            "text": "In this step, we add the iteration variables associated with the simulation\nrun to the data frame as columns. There are several reasons why this is a\ngood idea: they are very useful for generating the legends for plots of\ne.g. histograms and vectors (e.g. \"collision multiplicity histogram for\nnumHosts=20 and iaMean=2s\"), and often needed as chart input as well.  First, we select the iteration variables vars as a smaller data frame.  In[36]:  itervars_df   =   aloha . loc [ aloha . type == itervar ,   [ run ,   attrname ,   attrvalue ]]  itervars_df . head ()   Out[36]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       attrname \n       attrvalue \n     \n   \n   \n     \n       14 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       iaMean \n       3 \n     \n     \n       15 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       numHosts \n       10 \n     \n     \n       42 \n       PureAlohaExperiment-3-20170627-20:42:20-22739 \n       iaMean \n       2 \n     \n     \n       43 \n       PureAlohaExperiment-3-20170627-20:42:20-22739 \n       numHosts \n       10 \n     \n     \n       70 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       iaMean \n       1 \n     \n       We reshape the result by using the  pivot()  method. The following statement\nwill convert unique values in the  attrname  column into separate columns: iaMean  and  numHosts . The new data frame will be indexed with the run id.  In[37]:  itervarspivot_df   =   itervars_df . pivot ( index = run ,   columns = attrname ,   values = attrvalue )  itervarspivot_df . head ()   Out[37]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       attrname \n       iaMean \n       numHosts \n     \n     \n       run \n       \n       \n     \n   \n   \n     \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       1 \n       10 \n     \n     \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       1 \n       10 \n     \n     \n       PureAlohaExperiment-10-20170627-20:42:16-22741 \n       7 \n       10 \n     \n     \n       PureAlohaExperiment-11-20170627-20:42:16-22741 \n       7 \n       10 \n     \n     \n       PureAlohaExperiment-12-20170627-20:42:16-22741 \n       9 \n       10 \n     \n       Now, we only need to add the new columns back into the original dataframe, using merge() . This operation is not quite unlike an SQL join of two tables on the run  column.  In[38]:  aloha2   =   aloha . merge ( itervarspivot_df ,   left_on = run ,   right_index = True ,   how = outer )  aloha2 . head ()   Out[38]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n       iaMean \n       numHosts \n     \n   \n   \n     \n       0 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       configname \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n     \n     \n       1 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       datetime \n       20170627-20:42:20 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n     \n     \n       2 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       experiment \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n     \n     \n       3 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       inifile \n       omnetpp.ini \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n     \n     \n       4 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       iterationvars \n       numHosts=10, iaMean=3 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n     \n       For plot legends, it is also useful to have a single  iterationvars  column with\nstring values like  numHosts=10, iaMean=2 . This is easier than the above: we\ncan just select the rows containing the run attribute named  iterationvars \n(it contains exactly the string we need), take only the  run  and  attrvalue \ncolumns, rename the  attrvalue  column to  iterationvars , and then merge back the\nresult into the original data frame in a way we did above.  The selection and renaming step can be done as follows. (Note: we need .astype ( str )  in the condition so that rows where  attrname  is not filled in\ndo not cause trouble.)  In[39]:  itervarscol_df   =   aloha . loc [( aloha . type == runattr )     ( aloha . attrname . astype ( str ) == iterationvars ),   [ run ,   attrvalue ]]  itervarscol_df   =   itervarscol_df . rename ( columns = { attrvalue :   iterationvars })  itervarscol_df . head ()   Out[39]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       iterationvars \n     \n   \n   \n     \n       4 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       numHosts=10, iaMean=3 \n     \n     \n       32 \n       PureAlohaExperiment-3-20170627-20:42:20-22739 \n       numHosts=10, iaMean=2 \n     \n     \n       60 \n       PureAlohaExperiment-0-20170627-20:42:16-22739 \n       numHosts=10, iaMean=1 \n     \n     \n       88 \n       PureAlohaExperiment-1-20170627-20:42:17-22739 \n       numHosts=10, iaMean=1 \n     \n     \n       116 \n       PureAlohaExperiment-2-20170627-20:42:19-22739 \n       numHosts=10, iaMean=2 \n     \n       In the merging step, we join the two tables (I mean, data frames) on the  run \ncolumn:  In[40]:  aloha3   =   aloha2 . merge ( itervarscol_df ,   left_on = run ,   right_on = run ,   how = outer )  aloha3 . head ()   Out[40]:    \n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }  \n   \n     \n       \n       run \n       type \n       module \n       name \n       attrname \n       attrvalue \n       value \n       count \n       sumweights \n       mean \n       stddev \n       min \n       max \n       binedges \n       binvalues \n       vectime \n       vecvalue \n       iaMean \n       numHosts \n       iterationvars \n     \n   \n   \n     \n       0 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       configname \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n       numHosts=10, iaMean=3 \n     \n     \n       1 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       datetime \n       20170627-20:42:20 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n       numHosts=10, iaMean=3 \n     \n     \n       2 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       experiment \n       PureAlohaExperiment \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n       numHosts=10, iaMean=3 \n     \n     \n       3 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       inifile \n       omnetpp.ini \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n       numHosts=10, iaMean=3 \n     \n     \n       4 \n       PureAlohaExperiment-4-20170627-20:42:20-22739 \n       runattr \n       NaN \n       NaN \n       iterationvars \n       numHosts=10, iaMean=3 \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       NaN \n       None \n       None \n       None \n       None \n       3 \n       10 \n       numHosts=10, iaMean=3 \n     \n       To see the result of our work, let's try plotting the same histograms again,\nthis time with a proper legend:  In[41]:  histograms   =   aloha3 [ aloha3 . type == histogram ]  somehistograms   =   histograms [ histograms . name   ==   collisionLength:histogram ][: 5 ]  for   row   in   somehistograms . itertuples (): \n     plt . plot ( row . binedges ,   np . append ( row . binvalues ,   0 ),   drawstyle = steps-post )  plt . title ( collisionLength:histogram )  plt . legend ( somehistograms . iterationvars )  plt . xlim ( 0 ,   0.5 )  plt . show ()   Out[41]:", 
            "title": "10. Adding iteration variables as columns"
        }, 
        {
            "location": "/tutorials/pandas/#11-plotting-vectors", 
            "text": "This section deals with basic plotting of output vectors. Output vectors\nare basically time series data, but values have timestamps instead\nof being evenly spaced. Vectors are in rows that have  vector \nin the  type  column. The values and their timestamps are in the vecvalue  and  vectime  columns as NumPy array objects ( ndarray ).  We'll use a different data set for exploring output vector plotting, one from\nthe  routing  example simulation. There are pre-recorded result files in the samples/resultfiles/routing  directory; change into it in the terminal, and\nissue the following command to convert them to CSV:  scavetool x *.sca *.vec -o routing.csv  Then we read the the CSV file into a data frame in the same way we saw with the aloha  dataset:  In[42]:  routing   =   pd . read_csv ( routing.csv ,   converters   =   { \n     attrvalue :   parse_if_number , \n     binedges :   parse_ndarray , \n     binvalues :   parse_ndarray , \n     vectime :   parse_ndarray , \n     vecvalue :   parse_ndarray })   Let us begin by selecting the vectors into a new data frame for convenience.  In[43]:  vectors   =   routing [ routing . type == vector ]  len ( vectors )   Out[43]:  65  Our data frame contains results from one run. To get some idea what vectors\nwe have, let's print the list unique vector names and module names:  In[44]:  vectors . name . unique (),   vectors . module . unique ()   Out[44]:  (array([ busy:vector ,  qlen:vector ,  txBytes:vector ,\n         endToEndDelay:vector ,  hopCount:vector ,  sourceAddress:vector ,\n         rxBytes:vector ,  drop:vector ], dtype=object),\n array([ Net5.rte[0].port$o[0].channel ,  Net5.rte[0].port$o[1].channel ,\n         Net5.rte[1].port$o[0].channel ,  Net5.rte[1].port$o[1].channel ,\n         Net5.rte[1].port$o[2].channel ,  Net5.rte[2].port$o[0].channel ,\n         Net5.rte[2].port$o[1].channel ,  Net5.rte[2].port$o[2].channel ,\n         Net5.rte[2].port$o[3].channel ,  Net5.rte[3].port$o[0].channel ,\n         Net5.rte[3].port$o[1].channel ,  Net5.rte[3].port$o[2].channel ,\n         Net5.rte[4].port$o[0].channel ,  Net5.rte[4].port$o[1].channel ,\n         Net5.rte[0].queue[0] ,  Net5.rte[0].queue[1] ,\n         Net5.rte[1].queue[0] ,  Net5.rte[1].queue[1] ,\n         Net5.rte[1].queue[2] ,  Net5.rte[2].queue[0] ,\n         Net5.rte[2].queue[1] ,  Net5.rte[2].queue[2] ,\n         Net5.rte[2].queue[3] ,  Net5.rte[3].queue[0] ,\n         Net5.rte[3].queue[1] ,  Net5.rte[3].queue[2] ,\n         Net5.rte[4].queue[0] ,  Net5.rte[4].queue[1] ,  Net5.rte[4].app ,\n         Net5.rte[1].app ], dtype=object))  A vector can be plotted on a line chart by simply passing the  vectime  and vecvalue  arrays to  plt.plot() :  In[45]:  vec   =   vectors [ vectors . name   ==   qlen:vector ] . iloc [ 4 ]    # take some vector  plt . plot ( vec . vectime ,   vec . vecvalue ,   drawstyle = steps-post )  plt . xlim ( 0 , 100 )  plt . show ()   Out[45]:   When several vectors need to be placed on the same plot, one can simply\nuse a  for  loop.  In[46]:  somevectors   =   vectors [ vectors . name   ==   qlen:vector ][: 5 ]  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime ,   row . vecvalue ,   drawstyle = steps-post )  plt . title ( somevectors . name . values [ 0 ])  plt . legend ( somevectors . module )  plt . show ()   Out[46]:", 
            "title": "11. Plotting vectors"
        }, 
        {
            "location": "/tutorials/pandas/#12-vector-filtering", 
            "text": "Plotting vectors \"as is\" is often not practical, as the result will be a crowded\nplot that's difficult to draw conclusions from. To remedy that, one can apply\nsome kind of filtering before plotting, or plot a derived quantity such as the\nintegral, sum or running average instead of the original. Such things can easily\nbe achieved with the help of NumPy.  Vector time and value are already stored in the data frame as NumPy arrays\n( ndarray ), so we can apply NumPy functions to them. For example, let's\ntry  np.cumsum()  which computes cumulative sum:  In[47]:  x   =   np . array ([ 8 ,   2 ,   1 ,   5 ,   7 ])  np . cumsum ( x )   Out[47]:  array([ 8, 10, 11, 16, 23])  In[48]:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime ,   np . cumsum ( row . vecvalue ))  plt . show ()   Out[48]:   Plotting cumulative sum against time might be useful e.g. for an output\nvector where the simulation emits the packet length for each packet\nthat has arrived at its destination. There, the sum would represent\n\"total bytes received\".  Plotting the count against time for the same output vector would\nrepresent \"number of packets received\". For such a plot, we can utilize np.arange(1,n)  which simply returns the numbers 1, 2, .., n-1\nas an array:  In[49]:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime ,   np . arange ( 1 ,   row . vecvalue . size + 1 ),   .- ,   drawstyle = steps-post )  plt . xlim ( 0 , 5 );   plt . ylim ( 0 , 20 )  plt . show ()   Out[49]:   Note that we changed the plotting style to \"steps-post\", so\nthat for any  t  time the plot accurately represents the number\nof values whose timestamp is less than or equal to  t .  As another warm-up exercise, let's plot the time interval\nthat elapses between adjacent values; that is, for each element\nwe want to plot the time difference between the that element\nand the previous one.\nThis can be achieved by computing  t [ 1 : ]   -   t [:- 1 ] , which is the\nelementwise subtraction of the  t  array and its shifted version.\nArray indexing starts at 0, so  t[1:]  means \"drop the first element\".\nNegative indices count from the end of the array, so  t [:- 1 ]  means\n\"without the last element\". The latter is necessary because the\nsizes of the two arrays must match. or convenience, we encapsulate\nthe formula into a Python function:  In[50]:  def   diff ( t ): \n     return   t [ 1 :]   -   t [: - 1 ]  # example  t   =   np . array ([ 0.1 ,   1.5 ,   1.6 ,   2.0 ,   3.1 ])  diff ( t )   Out[50]:  array([ 1.4,  0.1,  0.4,  1.1])  We can now plot it. Note that as  diff()  makes the array one element\nshorter, we need to write  row.vectime[1:]  to drop the first element\n(it has no preceding element, so  diff()  cannot be computed for it.)\nAlso, we use dots for plotting instead of lines, as it makes more\nsense here.  In[51]:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime [ 1 :],   diff ( row . vectime ),   o )  plt . xlim ( 0 , 100 )  plt . show ()   Out[51]:   We now know enough NumPy to be able to write a function that computes\nrunning average (a.k.a. \"mean filter\"). Let's try it out in a plot\nimmediately.  In[52]:  def   running_avg ( x ): \n     return   np . cumsum ( x )   /   np . arange ( 1 ,   x . size   +   1 )  # example plot:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime ,   running_avg ( row . vecvalue ))  plt . xlim ( 0 , 100 )  plt . show ()   Out[52]:   For certain quantities such as queue length or on-off status,\nweighted average (with time intervals used as weights) makes\nmore sense. Here is a function that computes running time-average:  In[53]:  def   running_timeavg ( t , x ): \n     dt   =   t [ 1 :]   -   t [: - 1 ] \n     return   np . cumsum ( x [: - 1 ]   *   dt )   /   t [ 1 :]  # example plot:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime [ 1 :],   running_timeavg ( row . vectime ,   row . vecvalue ))  plt . xlim ( 0 , 100 )  plt . show ()   Out[53]:   Computing the integral of the vector as a step function is very similar\nto the  running_timeavg()  function. (Note: Computing integral in other\nways is part of NumPy and SciPy, if you ever need it. For example, np.trapz(y,x)  computes integral using the trapezoidal rule.)  In[54]:  def   integrate_steps ( t , x ): \n     dt   =   t [ 1 :]   -   t [: - 1 ] \n     return   np . cumsum ( x [: - 1 ]   *   dt )  # example plot:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime [ 1 :],   integrate_steps ( row . vectime ,   row . vecvalue ))  plt . show ()   Out[54]:   As the last example in this section, here is a function that computes\nmoving window average. It relies on the clever trick of subtracting\nthe cumulative sum of the original vector from its shifted version\nto get the sum of values in every  N -sized window.  In[55]:  def   winavg ( x ,   N ): \n     xpad   =   np . concatenate (( np . zeros ( N ),   x ))   # pad with zeroes \n     s   =   np . cumsum ( xpad ) \n     ss   =   s [ N :]   -   s [: - N ] \n     ss [ N - 1 :]   /=   N \n     ss [: N - 1 ]   /=   np . arange ( 1 ,   min ( N - 1 , ss . size ) + 1 ) \n     return   ss  # example:  for   row   in   somevectors . itertuples (): \n     plt . plot ( row . vectime ,   winavg ( row . vecvalue ,   10 ))  plt . xlim ( 0 , 200 )  plt . show ()   Out[55]:   You can find further hints for smoothing the plot of an output vector\nin the signal processing chapter of the SciPy Cookbook (see References).", 
            "title": "12. Vector Filtering"
        }, 
        {
            "location": "/tutorials/pandas/#resources", 
            "text": "The primary and authentic source of information on Pandas, Matplotlib and other\nlibraries is their official documentation. I do not link them here because they\nare trivial to find via Google. Instead, here is a random collection of other\nresources that I found useful while writing this tutorial (not counting all the\nStackOverflow pages I visited.)   Pandas tutorial from Greg Reda:\n   http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/  On reshaping data frames:\n   https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping  Matplotlib tutorial of Nicolas P. Rougier:\n   https://www.labri.fr/perso/nrougier/teaching/matplotlib/  Creating boxplots with Matplotlib, from Bharat Bhole:\n   http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/  SciPy Cookbook on signal smoothing:\n   http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html  Visual Guide on Pandas (video):\n   https://www.youtube.com/watch?v=9d5-Ti6onew  Python Pandas Cookbook (videos):\n   https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3", 
            "title": "Resources"
        }, 
        {
            "location": "/tutorials/pandas/#acknowledgements", 
            "text": "The author, Andras Varga would like to thank the participants of the\n2016 OMNeT++ Summit for the valuable feedback, and especially\nDr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/tutorials/cloud/", 
            "text": "Introduction\n\n\nThe goal of the tutorial is to give you an insight on how to start harnessing\nthe power of computing clouds for running simulation campaigns in order to\nreduce the time it takes for them to complete. We will focus on using Amazon's\nAWS, but the process can be easily ported to other cloud service providers.\n\n\nMotivation\n\n\nSimulation is a CPU-intensive task. A simulation campaign may easily consist of\nhundreds or thousands of simulation runs, and can easily exceed the capacity of\ncomputing resources usually available to the researcher.\n\n\nNowadays, CPU power is available in abundance in the cloud for anyone, at very\naffordable prices. There are numerous cloud computing services (Amazon AWS,\nMicrosoft Azure, DigitalOcean, Google Cloud Platform, etc.). These services,\nfollowing an easy registration, allow the user to run their own code on a high\nnumber of CPUs, at a surprisingly low price. For example, one hour of usage of\nan 8-core CPU with 32 GiB RAM costs about $0.50 on AWS at the time of writing.\nThere is also a free trial, which grants the user one year of CPU time for free.\n\n\nSimulation campaigns are often trivially parallelizable. Given enough CPUs, the\nwhole campaign may complete in the time it takes for the longest run to finish.\nIn this tutorial, we show you how to harness the power of computing clouds to\ndramatically speed up your simulation campaigns. The gain will not only save you\ntime, but it may also allow you to expand the scope or increase the depth of\nyour research, and come to new discoveries.\n\n\nStructure of this Tutorial\n\n\nIn the first part of this tutorial we will explain the basics of cloud services,\nget you familiar with the concepts of the most commonly used container\ntechnology, Docker, and introduce job queues.\n\n\nIn the second part, we present a concrete solution that allows you to upload\nand execute simulations in AWS. The process should be regarded as a starting\npoint which illustrates the concept, and can serve as a base for future, more\nsophisticated solutions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/cloud/#introduction", 
            "text": "The goal of the tutorial is to give you an insight on how to start harnessing\nthe power of computing clouds for running simulation campaigns in order to\nreduce the time it takes for them to complete. We will focus on using Amazon's\nAWS, but the process can be easily ported to other cloud service providers.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/cloud/#motivation", 
            "text": "Simulation is a CPU-intensive task. A simulation campaign may easily consist of\nhundreds or thousands of simulation runs, and can easily exceed the capacity of\ncomputing resources usually available to the researcher.  Nowadays, CPU power is available in abundance in the cloud for anyone, at very\naffordable prices. There are numerous cloud computing services (Amazon AWS,\nMicrosoft Azure, DigitalOcean, Google Cloud Platform, etc.). These services,\nfollowing an easy registration, allow the user to run their own code on a high\nnumber of CPUs, at a surprisingly low price. For example, one hour of usage of\nan 8-core CPU with 32 GiB RAM costs about $0.50 on AWS at the time of writing.\nThere is also a free trial, which grants the user one year of CPU time for free.  Simulation campaigns are often trivially parallelizable. Given enough CPUs, the\nwhole campaign may complete in the time it takes for the longest run to finish.\nIn this tutorial, we show you how to harness the power of computing clouds to\ndramatically speed up your simulation campaigns. The gain will not only save you\ntime, but it may also allow you to expand the scope or increase the depth of\nyour research, and come to new discoveries.", 
            "title": "Motivation"
        }, 
        {
            "location": "/tutorials/cloud/#structure-of-this-tutorial", 
            "text": "In the first part of this tutorial we will explain the basics of cloud services,\nget you familiar with the concepts of the most commonly used container\ntechnology, Docker, and introduce job queues.  In the second part, we present a concrete solution that allows you to upload\nand execute simulations in AWS. The process should be regarded as a starting\npoint which illustrates the concept, and can serve as a base for future, more\nsophisticated solutions.", 
            "title": "Structure of this Tutorial"
        }, 
        {
            "location": "/tutorials/cloud/page1/", 
            "text": "Concepts\n\n\nComputing Clouds\n\n\nA computing cloud contains a large collection of multi-core computing nodes,\nwith a large amount of memory and storage. Nodes are connected with a high-speed\ninternal network and are attached to the internet also with a high-bandwidth\nlink. Users do not run their programs directly on the operating system of the\nnodes, but rather over some virtualization technology (Xen, KVM, VMWare).\nVirtualization isolates cloud users from each other, providing a confined\nenvironment to them, where they can't do any harm to others or the cloud. It\nalso allows the cloud operator to freeze and resume the user's programs,\ntransparently migrate them to other nodes, and control their access to resources\nlike memory and CPU. This way the cloud operator can effectively treat the\ncomputing cluster as a resource pool. This kind of resource sharing is what\nallows the cloud services to be provided at a low cost.\n\n\nSome properties of computing clouds:\n\n\n\n\non-demand or reserved allocation\n\n\nbilling based on metered usage\n\n\ndifferent configurations are available (CPU speed, memory size, storage type, network speed)\n\n\nguest OS is typically Linux\n\n\nSSH access is available\n\n\nspecialized higher level services are often also offered (machine learning, data mining, 3D rendering, etc.)\n\n\n\n\nThere are numerous cloud computing services; in this tutorial we will use AWS\nwhich even offers a free trial period of one year.\n\n\nDocker\n\n\nAbout Docker\n\n\nDocker is probably the most popular software container platform nowadays. A\ncontainer is similar in purpose to virtual machines and emulators, but is\nmuch more lightweight. The overhead of running a container is much closer\nto running an ordinary operating system process.\nIn fact, a Docker container is indeed an OS process, isolated from all other\nprocesses in a variety of ways -- a different root file system, network stack,\nresource limit group, etc. Linux kernel features that make Docker possible\ninclude chroot, kernel namespaces, and control groups a.k.a. cgroups.\n\n\nA Docker image is extremely portable: it can be run without changes on a\nvariety of systems, including any Linux distribution, macOS, Windows, and many\nothers. It is partly due to this portability that Docker images are extremely\nconvenient to use as a way for packaging up for applications for running\nin the cloud.\n\n\nA Docker image contains a file system to be mounted as root for the process,\nadditional settings and metadata such as environment variables to be set, and\nan entry point (the program to be started on container launch.) Images\nmay be are based on each other like layers. Many standard and user-created\nimages are easily accessible from registries like Docker Hub.\n\n\nA container is a running instance of an image, similar to how a process is a\nrunning instance of a program. Containers interact with each other and the\noutside world most commonly through a network connection, a terminal emulator,\nor sometimes via directories mounted into their file systems as volumes.\n\n\nRunning Containers\n\n\nIf you have Docker installed, you can try it out right away by typing the\nfollowing commands into a terminal:\n\n\n$ docker run -ti hello-world\n\n\n\n\nhello-world\n is the name of a Docker image. Docker will first check if the\n\nhello-world\n image is already available on your machine. If not, it will fetch\nit from Docker Hub first. Then it starts the image as a container. The process\nin the container will simply print a message, then terminate.\n\n\nA bit more useful one is the \nubuntu\n image, which packages the latest Ubuntu\nLTS version without the graphical parts:\n\n\n$ docker run -ti ubuntu\n\n\n\n\nAfter the image is downloaded and started, you will be presented with an Ubuntu\n\nbash\n prompt, and you can start issuing commands in the isolation of the\ncontainer. It is important to note that running the \nubuntu\n image does \nnot\n\ninvolve booting up a complete Linux system inside the container with its own\n\ninit\n process and all. Instead, only a plain \nbash\n process is started,\nwhich will have the illusion (via the \nchroot\n'ed file system, kernel namespaces\nand other mechanisms) that it is running on a separate Ubuntu system. The \n-ti\n\nflag in the above command tells Docker to attach the container to the current\nterminal, so we can interact with the \nbash\n shell.\n\n\nAbout Docker Hub\n\n\nDocker Hub is an online service operated by Docker Inc. It is a registry and\nstorage space for Docker images. It can be used for free for accessing and\nuploading public images.\n\n\nTo be able to submit you own image to Docker Hub, you need to create an account,\ncalled Docker ID. For this, an e-mail address, a user name, and a password has\nto be provided, just like with any other online account registration.\n\n\nImages are identified by a hexadecimal ID (hash), and can have a name (or\nseveral names) as well. There are three kinds of image names you will see:\n  - \nubuntu\n: If it is a single word, then it refers to an \"official\" image on Docker Hub.\n  - \njoe/foobar\n: This is an image submitted by a user on to Docker Hub.\n  - \nrepo1.company.com/foobar\n: This is an image in a third-party repository.\n\n\nSome image names end with a version like \n:2.42\n, or typically \n:latest\n.\n\n\nNote:  Docker Hub is now part of Docker Cloud, which offers Swarm management\nservice as well, but we use the old name here to reduce confusion with the\ngeneral term \"cloud\".\n\n\nCreating Docker Images\n\n\nThe primary way of creating Docker images is using a Dockerfile.\n\n\nA Dockerfile is a kind of a recipe. It's a line-oriented text file where each\nline starts with an instruction, followed by arguments. Lines starting with a\nhashmark (\n#\n) are comments, and a backslash (\n\\\n) at the end of a line means\nthat the instruction is continued on the next line.\n\n\nWhen building an image, Docker executes instructions sequentially, creating a\nnew image layer after each one. This way, subsequent builds can be incremental.\nThe image layer after the last instruction is the final image output.\n\n\nEach Dockerfile must start with the \nFROM\n instruction, which determines the\nbase image we are starting from.\n\n\nAn example Dockerfile looks like this:\n\n\n# starting with Ubuntu 16.04 as base image\nFROM ubuntu:16.04\n\n## update the package cache, and install a C compiler\nRUN apt-get update -y \n&\n&\n apt-get install build-essential -y\n\n## set an environment variable\nENV LANG C.UTF-8\n\n## copy a file into the image\nCOPY main.c /opt/\n\n## change the working directory, for build- and runtime as well\nWORKDIR /opt/\n## compile the copied source file\nRUN gcc main.c -o program\n\n## set the newly built program to start in the containers created from this image\nENTRYPOINT ./program\n\n\n\nThe build process takes place within a build context. This is usually a\ndirectory on the host machine, but it can be a URL as well. The context is where\nthe \nmain.c\n file is taken from in the above example.\n\n\nTo build this image, save the example into a file named \nDockerfile\n into an\nempty directory, and a C program of your choice (a \"Hello World\" will be fine)\nnext to it, named \nmain.c\n.\n\n\nThen issue the following command:\n\n\n$ docker build . -t myimage\n\n\n\n\nHere the \n.\n is the build context, the \nDockerfile\n will be picked up by its\nname, as this is the default, and the \n-t myimage\n will add the \nmyimage\n tag\n(an alternative name) to the image we are building.\n\n\nYou will see all the instructions being executed. Once it finishes, you can run\nthe new image with:\n\n\n$ docker run -ti myimage\n\n\n\n\nTo push this image to Docker Hub so it can be reused by anyone, first\nlog in to docker with you Docker ID:\n\n\n$ docker login\n\n\n\n\nThen add another tag to this image, substituting \njoe\n with your Docker ID:\n\n\n$ docker tag myimage joe/myimage\n\n\n\n\nAnd finally start the upload:\n\n\n$ docker push joe/myimage\n\n\n\n\nNow anyone anywhere can run the C program in this image by typing \ndocker run-ti joe/myimage\n.\n\n\nAnd logging in to Docker Hub on the web interface, you will also see a new\nrepository for this image has appeared in your account.\n\n\nAlternatives\n\n\nDocker also offers a build service called Docker Cloud, where a Dockerfile\nhosted in a source repository on GitHub or BitBucket is built on their servers\nautomatically after each commit in the repository.\n\n\nA alternative way for creating images is constructing the desired result by\nissuing commands manually inside a running container, then saving the final\nstate into an image using \ndocker commit\n. This is not recommended, as the\nresulting image is a lot less reproducible.\n\n\nAWS Services\n\n\nAWS (Amazon Web Services) offers many services, the following two are important\nfor us: EC2 and ECS.\n\n\nEC2 (Elastic Compute Cloud)\n\n\nEC2 is a classic cloud service, where one can run virtual machines (called\nInstances) typically with a Linux installation, and access them via SSH.  They\ncan also be set up to accept connections from the Internet. They come in a\nvariety of configurations (Instance Types), each offering different\ncapabilities, like the number of CPU cores and available RAM size. One of them\nis \nt2.micro\n, which is the only one eligible for use in the free trial. It is\namong the smaller ones, but it is still perfectly suitable for our purposes.\nInstances can reach each other on internal virtual networks.\n\n\nECS (EC2 Container Service)\n\n\nECS is a service that is offered on top of EC2, and allows us to run Docker\ncontainers on EC2 instances easily. The Docker images it runs can come from\nDocker Hub, from repositories hosted in AWS (ECR), or from anywhere else. The\nEC2 Instances used to run the containers are managed automatically by ECS.\n\n\nECS Terminology\n\n\nAWS services, including EC2 and ECS, can be configured using command-line tools\n(AWS CLI), or a more comfortable web interface (AWS Management Console). We will\nonly use the latter in this tutorial, as it requires less setup.\n\n\nOn ECS, the managed EC2 Instances are grouped in Clusters. The number of\ninstances can be easily scaled just by adjusting a parameter of the Cluster.\nThere are ways to automate this based on system load using Auto Scaling Groups,\nbut we won't do this to keep things simple.\n\n\nECS Clusters run Tasks. Each Task consists of one or more Containers (Docker\ncontainers). Tasks are launched from Task Definitions, which describe which\nimages will be used, what storage volumes will be attached, which network ports\nneed to be exposed to the network, and so forth.\n\n\nTasks can be launched and stopped manually, or by using Services. The job of a\nService is to start and manage a given number of Tasks. The properties of a\nService include: a Task Definition from which it will start its Tasks, the\nnumber of Tasks it should keep running, and a placement rule describing how the\ncontainers in the Tasks will be assigned to EC2 machines. Again, the number of\nTasks in a Service can be scaled easily just by adjusting a parameter. The\nnumber of Tasks in a Service can also be set up to scale automatically based on\nload, but we won't use this feature.\n\n\nJob Queues\n\n\nAn OMNeT++ simulation campaign typically consists of more runs than we have CPUs\navailable, so we will need a kind of job queue. Job queues are widely used in\napplications where a large number of tasks need to be performed, using a bounded\nnumber of workers. A job queue automates the starting of jobs on workers and\nmonitors their completions, thereby ensuring the optimal utilization of\ncomputing resources.\n\n\nGeneral Operation\n\n\nJobs are submitted to the queue by some producer, where they are stored together\nwith their parameters. The queue can be a simple FIFO, where jobs are executed\non a first-come, first-served basis, or they can be prioritized in a variety of\nways.\n\n\nThere are one or more workers connected to the job queue to perform the jobs.\nWorkers sit in a loop, and wait for new jobs. When a job becomes available, they\npop it off the queue and process it in the way appropriate for the job. Once it\nis done, the result will either be put back into a different queue for further\nprocessing, or simply stored somewhere. Then the worker starts the next job,\nuntil there are no more pending jobs left.\n\n\nRQ\n\n\nIn this tutorial, we're going use the \nRQ\n library to\nimplement our job queue. We chose it because of its lightweightness and emphasis\non simplicity and ease of use.\n\n\nRQ is backed by Redis, a high-performance in-memory key-value store. This means\nthat we will need to start a Redis server to operate the job queue, and have the\nworkers connect to it.\n\n\nIn RQ, each job is an invocation of an ordinary Python function. The parameters\nof the job are the function's arguments, and the result is its return value. The\ncode itself is not passed through the queue, so the job function must be present\non the worker nodes.", 
            "title": "Concepts"
        }, 
        {
            "location": "/tutorials/cloud/page1/#concepts", 
            "text": "", 
            "title": "Concepts"
        }, 
        {
            "location": "/tutorials/cloud/page1/#computing-clouds", 
            "text": "A computing cloud contains a large collection of multi-core computing nodes,\nwith a large amount of memory and storage. Nodes are connected with a high-speed\ninternal network and are attached to the internet also with a high-bandwidth\nlink. Users do not run their programs directly on the operating system of the\nnodes, but rather over some virtualization technology (Xen, KVM, VMWare).\nVirtualization isolates cloud users from each other, providing a confined\nenvironment to them, where they can't do any harm to others or the cloud. It\nalso allows the cloud operator to freeze and resume the user's programs,\ntransparently migrate them to other nodes, and control their access to resources\nlike memory and CPU. This way the cloud operator can effectively treat the\ncomputing cluster as a resource pool. This kind of resource sharing is what\nallows the cloud services to be provided at a low cost.  Some properties of computing clouds:   on-demand or reserved allocation  billing based on metered usage  different configurations are available (CPU speed, memory size, storage type, network speed)  guest OS is typically Linux  SSH access is available  specialized higher level services are often also offered (machine learning, data mining, 3D rendering, etc.)   There are numerous cloud computing services; in this tutorial we will use AWS\nwhich even offers a free trial period of one year.", 
            "title": "Computing Clouds"
        }, 
        {
            "location": "/tutorials/cloud/page1/#docker", 
            "text": "", 
            "title": "Docker"
        }, 
        {
            "location": "/tutorials/cloud/page1/#about-docker", 
            "text": "Docker is probably the most popular software container platform nowadays. A\ncontainer is similar in purpose to virtual machines and emulators, but is\nmuch more lightweight. The overhead of running a container is much closer\nto running an ordinary operating system process.\nIn fact, a Docker container is indeed an OS process, isolated from all other\nprocesses in a variety of ways -- a different root file system, network stack,\nresource limit group, etc. Linux kernel features that make Docker possible\ninclude chroot, kernel namespaces, and control groups a.k.a. cgroups.  A Docker image is extremely portable: it can be run without changes on a\nvariety of systems, including any Linux distribution, macOS, Windows, and many\nothers. It is partly due to this portability that Docker images are extremely\nconvenient to use as a way for packaging up for applications for running\nin the cloud.  A Docker image contains a file system to be mounted as root for the process,\nadditional settings and metadata such as environment variables to be set, and\nan entry point (the program to be started on container launch.) Images\nmay be are based on each other like layers. Many standard and user-created\nimages are easily accessible from registries like Docker Hub.  A container is a running instance of an image, similar to how a process is a\nrunning instance of a program. Containers interact with each other and the\noutside world most commonly through a network connection, a terminal emulator,\nor sometimes via directories mounted into their file systems as volumes.", 
            "title": "About Docker"
        }, 
        {
            "location": "/tutorials/cloud/page1/#running-containers", 
            "text": "If you have Docker installed, you can try it out right away by typing the\nfollowing commands into a terminal:  $ docker run -ti hello-world  hello-world  is the name of a Docker image. Docker will first check if the hello-world  image is already available on your machine. If not, it will fetch\nit from Docker Hub first. Then it starts the image as a container. The process\nin the container will simply print a message, then terminate.  A bit more useful one is the  ubuntu  image, which packages the latest Ubuntu\nLTS version without the graphical parts:  $ docker run -ti ubuntu  After the image is downloaded and started, you will be presented with an Ubuntu bash  prompt, and you can start issuing commands in the isolation of the\ncontainer. It is important to note that running the  ubuntu  image does  not \ninvolve booting up a complete Linux system inside the container with its own init  process and all. Instead, only a plain  bash  process is started,\nwhich will have the illusion (via the  chroot 'ed file system, kernel namespaces\nand other mechanisms) that it is running on a separate Ubuntu system. The  -ti \nflag in the above command tells Docker to attach the container to the current\nterminal, so we can interact with the  bash  shell.", 
            "title": "Running Containers"
        }, 
        {
            "location": "/tutorials/cloud/page1/#about-docker-hub", 
            "text": "Docker Hub is an online service operated by Docker Inc. It is a registry and\nstorage space for Docker images. It can be used for free for accessing and\nuploading public images.  To be able to submit you own image to Docker Hub, you need to create an account,\ncalled Docker ID. For this, an e-mail address, a user name, and a password has\nto be provided, just like with any other online account registration.  Images are identified by a hexadecimal ID (hash), and can have a name (or\nseveral names) as well. There are three kinds of image names you will see:\n  -  ubuntu : If it is a single word, then it refers to an \"official\" image on Docker Hub.\n  -  joe/foobar : This is an image submitted by a user on to Docker Hub.\n  -  repo1.company.com/foobar : This is an image in a third-party repository.  Some image names end with a version like  :2.42 , or typically  :latest .  Note:  Docker Hub is now part of Docker Cloud, which offers Swarm management\nservice as well, but we use the old name here to reduce confusion with the\ngeneral term \"cloud\".", 
            "title": "About Docker Hub"
        }, 
        {
            "location": "/tutorials/cloud/page1/#creating-docker-images", 
            "text": "The primary way of creating Docker images is using a Dockerfile.  A Dockerfile is a kind of a recipe. It's a line-oriented text file where each\nline starts with an instruction, followed by arguments. Lines starting with a\nhashmark ( # ) are comments, and a backslash ( \\ ) at the end of a line means\nthat the instruction is continued on the next line.  When building an image, Docker executes instructions sequentially, creating a\nnew image layer after each one. This way, subsequent builds can be incremental.\nThe image layer after the last instruction is the final image output.  Each Dockerfile must start with the  FROM  instruction, which determines the\nbase image we are starting from.  An example Dockerfile looks like this:  # starting with Ubuntu 16.04 as base image\nFROM ubuntu:16.04\n\n## update the package cache, and install a C compiler\nRUN apt-get update -y  & &  apt-get install build-essential -y\n\n## set an environment variable\nENV LANG C.UTF-8\n\n## copy a file into the image\nCOPY main.c /opt/\n\n## change the working directory, for build- and runtime as well\nWORKDIR /opt/\n## compile the copied source file\nRUN gcc main.c -o program\n\n## set the newly built program to start in the containers created from this image\nENTRYPOINT ./program  The build process takes place within a build context. This is usually a\ndirectory on the host machine, but it can be a URL as well. The context is where\nthe  main.c  file is taken from in the above example.  To build this image, save the example into a file named  Dockerfile  into an\nempty directory, and a C program of your choice (a \"Hello World\" will be fine)\nnext to it, named  main.c .  Then issue the following command:  $ docker build . -t myimage  Here the  .  is the build context, the  Dockerfile  will be picked up by its\nname, as this is the default, and the  -t myimage  will add the  myimage  tag\n(an alternative name) to the image we are building.  You will see all the instructions being executed. Once it finishes, you can run\nthe new image with:  $ docker run -ti myimage  To push this image to Docker Hub so it can be reused by anyone, first\nlog in to docker with you Docker ID:  $ docker login  Then add another tag to this image, substituting  joe  with your Docker ID:  $ docker tag myimage joe/myimage  And finally start the upload:  $ docker push joe/myimage  Now anyone anywhere can run the C program in this image by typing  docker run-ti joe/myimage .  And logging in to Docker Hub on the web interface, you will also see a new\nrepository for this image has appeared in your account.", 
            "title": "Creating Docker Images"
        }, 
        {
            "location": "/tutorials/cloud/page1/#alternatives", 
            "text": "Docker also offers a build service called Docker Cloud, where a Dockerfile\nhosted in a source repository on GitHub or BitBucket is built on their servers\nautomatically after each commit in the repository.  A alternative way for creating images is constructing the desired result by\nissuing commands manually inside a running container, then saving the final\nstate into an image using  docker commit . This is not recommended, as the\nresulting image is a lot less reproducible.", 
            "title": "Alternatives"
        }, 
        {
            "location": "/tutorials/cloud/page1/#aws-services", 
            "text": "AWS (Amazon Web Services) offers many services, the following two are important\nfor us: EC2 and ECS.", 
            "title": "AWS Services"
        }, 
        {
            "location": "/tutorials/cloud/page1/#ec2-elastic-compute-cloud", 
            "text": "EC2 is a classic cloud service, where one can run virtual machines (called\nInstances) typically with a Linux installation, and access them via SSH.  They\ncan also be set up to accept connections from the Internet. They come in a\nvariety of configurations (Instance Types), each offering different\ncapabilities, like the number of CPU cores and available RAM size. One of them\nis  t2.micro , which is the only one eligible for use in the free trial. It is\namong the smaller ones, but it is still perfectly suitable for our purposes.\nInstances can reach each other on internal virtual networks.", 
            "title": "EC2 (Elastic Compute Cloud)"
        }, 
        {
            "location": "/tutorials/cloud/page1/#ecs-ec2-container-service", 
            "text": "ECS is a service that is offered on top of EC2, and allows us to run Docker\ncontainers on EC2 instances easily. The Docker images it runs can come from\nDocker Hub, from repositories hosted in AWS (ECR), or from anywhere else. The\nEC2 Instances used to run the containers are managed automatically by ECS.", 
            "title": "ECS (EC2 Container Service)"
        }, 
        {
            "location": "/tutorials/cloud/page1/#ecs-terminology", 
            "text": "AWS services, including EC2 and ECS, can be configured using command-line tools\n(AWS CLI), or a more comfortable web interface (AWS Management Console). We will\nonly use the latter in this tutorial, as it requires less setup.  On ECS, the managed EC2 Instances are grouped in Clusters. The number of\ninstances can be easily scaled just by adjusting a parameter of the Cluster.\nThere are ways to automate this based on system load using Auto Scaling Groups,\nbut we won't do this to keep things simple.  ECS Clusters run Tasks. Each Task consists of one or more Containers (Docker\ncontainers). Tasks are launched from Task Definitions, which describe which\nimages will be used, what storage volumes will be attached, which network ports\nneed to be exposed to the network, and so forth.  Tasks can be launched and stopped manually, or by using Services. The job of a\nService is to start and manage a given number of Tasks. The properties of a\nService include: a Task Definition from which it will start its Tasks, the\nnumber of Tasks it should keep running, and a placement rule describing how the\ncontainers in the Tasks will be assigned to EC2 machines. Again, the number of\nTasks in a Service can be scaled easily just by adjusting a parameter. The\nnumber of Tasks in a Service can also be set up to scale automatically based on\nload, but we won't use this feature.", 
            "title": "ECS Terminology"
        }, 
        {
            "location": "/tutorials/cloud/page1/#job-queues", 
            "text": "An OMNeT++ simulation campaign typically consists of more runs than we have CPUs\navailable, so we will need a kind of job queue. Job queues are widely used in\napplications where a large number of tasks need to be performed, using a bounded\nnumber of workers. A job queue automates the starting of jobs on workers and\nmonitors their completions, thereby ensuring the optimal utilization of\ncomputing resources.", 
            "title": "Job Queues"
        }, 
        {
            "location": "/tutorials/cloud/page1/#general-operation", 
            "text": "Jobs are submitted to the queue by some producer, where they are stored together\nwith their parameters. The queue can be a simple FIFO, where jobs are executed\non a first-come, first-served basis, or they can be prioritized in a variety of\nways.  There are one or more workers connected to the job queue to perform the jobs.\nWorkers sit in a loop, and wait for new jobs. When a job becomes available, they\npop it off the queue and process it in the way appropriate for the job. Once it\nis done, the result will either be put back into a different queue for further\nprocessing, or simply stored somewhere. Then the worker starts the next job,\nuntil there are no more pending jobs left.", 
            "title": "General Operation"
        }, 
        {
            "location": "/tutorials/cloud/page1/#rq", 
            "text": "In this tutorial, we're going use the  RQ  library to\nimplement our job queue. We chose it because of its lightweightness and emphasis\non simplicity and ease of use.  RQ is backed by Redis, a high-performance in-memory key-value store. This means\nthat we will need to start a Redis server to operate the job queue, and have the\nworkers connect to it.  In RQ, each job is an invocation of an ordinary Python function. The parameters\nof the job are the function's arguments, and the result is its return value. The\ncode itself is not passed through the queue, so the job function must be present\non the worker nodes.", 
            "title": "RQ"
        }, 
        {
            "location": "/tutorials/cloud/page2/", 
            "text": "Implementation\n\n\nTo put all of this together into a working solution, in the rest of this\ntutorial, we're going to:\n\n\n\n\nInstall some necessary software packages on our machine\n\n\nBuild a Docker image which includes OMNeT++ and the worker code\n\n\nDeploy the system on AWS\n\n\nRun a simple simulation campaign with it using a custom client\n\n\nTake a look at all the code needed to perform the above\n\n\n\n\nBefore you begin, create a new empty folder. Later save all linked source files\nfrom this tutorial in that.\n\n\nSolution Architecture\n\n\nWe will create the following architecture for running simulations on AWS:\n\n\nWe want to use Docker images for easy deployment, so we will use an ECS cluster.\nOne container will run a Redis-based job queue, and others will be workers. The\nworkers will need to run simulations, so their image will have OMNeT++ installed\nin addition to the RQ worker client. After the worker completes a simulation\nrun, the results will be stored in the Redis database.\n\n\nWe will provide a custom tool that submits jobs to the job queue from the user's\ncomputer and downloads the results after simulation completion.\n\n\nThe final architecture will look like this:\n\n\n\n\nIn the following sections, we will show how to implement this architecture.\nWe will discuss how to create the Docker images, how to configure the cluster,\nhow to implement the worker and the end-user client and so on.\n\n\nPreparation\n\n\nFirst we need to install a few things on our computer.\n\n\nOMNeT++\n\n\nDownload the archive from the \nofficial website\n.\nThen follow the \nInstallation\nGuide\n.\n\n\nThe \ncore\n version will work fine as well, if you don't need the IDE.\n\n\nDocker\n\n\nFollow the guides on the official website: \nfor\nUbuntu\n or\n\nfor Fedora\n.\n\n\nNote that even if your distribution has Docker in its own native package\narchive, it is most likely an outdated, and you will have to uninstall it first.\n\n\nPython 3 and pip3\n\n\nWe will use Python version 3; and pip, which is the recommended tool for\ninstalling packages from the \nPython Package Index\n.\nInstall these using the native package manager of your distribution.\n\n\nOn Ubuntu:\n\n\n$ sudo apt install python3 python3-pip\n\n\n\n\nOn Fedora:\n\n\n$ sudo dnf install python3 python3-pip\n\n\n\n\nThen, in any case, upgrade \npip\n:\n\n\n$ sudo pip3 install --upgrade pip\n\n\n\n\nFeel free to use a \nvirtualenv\n for this instead of \nsudo\n if you're familiar\nwith the concept.\n\n\nRQ\n\n\nUse \npip\n to install the RQ library:\n\n\n$ sudo pip3 install rq\n\n\n\n\nThis will install the \nredis\n client module as well, as a dependency.\n\n\nGetting the Code\n\n\nDownload the following files into the directory you just created:\n\n\n\n\nutils.py\n - Contains some common utilities used by both \nworker.py\n and \nclient.py\n.\n\n\nworker.py\n -  Contains the code to be executed by the workers as a job.\n\n\nDockerfile\n - Is a recipe to build the Docker image for the workers.\n\n\nclient.py\n - Is the client-side software to submit the simulations.\n\n\n\n\nWe will take a closer look at their contents in the \"Examining the Code\" chapter.\n\n\nBuilding the Docker Image\n\n\nBuilding the image is done by issuing the following command in the directory\nwhere the above files can be found:\n\n\n$ docker build . -t worker\n\n\n\n\nThe \nDockerfile\n is picked up automatically by name, the build context is the \ncurrent directory (\n.\n), and the resulting image will be named \nworker\n. \n\n\nThis will likely take a few minutes to complete. If you see a couple of warnings\nwritten in red, but the process continues, don't worry, this is expected.\n\n\nPublishing\n\n\nNow that we built our image for the worker nodes, we need to make it available\nfor our AWS Instances by uploading it to Docker Hub.\n\n\nFirst authenticate yourself (in case you haven't already) by typing in your\nDocker ID and password after running this command:\n\n\n$ docker login\n\n\n\n\nNow tag the image \"into your repo\", substituting your user name (Docker ID), so\nDocker knows where to push the image:\n\n\n$ docker tag worker username/worker\n\n\n\n\nAnd finally issue the actual push:\n\n\n$ docker push username/worker\n\n\n\n\nThis will upload about 400-500 MB of data, so it can take a while. Once it's\ndone, your image is available worldwide. You can see it appeared on \nDocker\nHub\n. You may even get email notification if it was\nsuccessful.", 
            "title": "Implementation"
        }, 
        {
            "location": "/tutorials/cloud/page2/#implementation", 
            "text": "To put all of this together into a working solution, in the rest of this\ntutorial, we're going to:   Install some necessary software packages on our machine  Build a Docker image which includes OMNeT++ and the worker code  Deploy the system on AWS  Run a simple simulation campaign with it using a custom client  Take a look at all the code needed to perform the above   Before you begin, create a new empty folder. Later save all linked source files\nfrom this tutorial in that.", 
            "title": "Implementation"
        }, 
        {
            "location": "/tutorials/cloud/page2/#solution-architecture", 
            "text": "We will create the following architecture for running simulations on AWS:  We want to use Docker images for easy deployment, so we will use an ECS cluster.\nOne container will run a Redis-based job queue, and others will be workers. The\nworkers will need to run simulations, so their image will have OMNeT++ installed\nin addition to the RQ worker client. After the worker completes a simulation\nrun, the results will be stored in the Redis database.  We will provide a custom tool that submits jobs to the job queue from the user's\ncomputer and downloads the results after simulation completion.  The final architecture will look like this:   In the following sections, we will show how to implement this architecture.\nWe will discuss how to create the Docker images, how to configure the cluster,\nhow to implement the worker and the end-user client and so on.", 
            "title": "Solution Architecture"
        }, 
        {
            "location": "/tutorials/cloud/page2/#preparation", 
            "text": "First we need to install a few things on our computer.", 
            "title": "Preparation"
        }, 
        {
            "location": "/tutorials/cloud/page2/#omnet", 
            "text": "Download the archive from the  official website .\nThen follow the  Installation\nGuide .  The  core  version will work fine as well, if you don't need the IDE.", 
            "title": "OMNeT++"
        }, 
        {
            "location": "/tutorials/cloud/page2/#docker", 
            "text": "Follow the guides on the official website:  for\nUbuntu  or for Fedora .  Note that even if your distribution has Docker in its own native package\narchive, it is most likely an outdated, and you will have to uninstall it first.", 
            "title": "Docker"
        }, 
        {
            "location": "/tutorials/cloud/page2/#python-3-and-pip3", 
            "text": "We will use Python version 3; and pip, which is the recommended tool for\ninstalling packages from the  Python Package Index .\nInstall these using the native package manager of your distribution.  On Ubuntu:  $ sudo apt install python3 python3-pip  On Fedora:  $ sudo dnf install python3 python3-pip  Then, in any case, upgrade  pip :  $ sudo pip3 install --upgrade pip  Feel free to use a  virtualenv  for this instead of  sudo  if you're familiar\nwith the concept.", 
            "title": "Python 3 and pip3"
        }, 
        {
            "location": "/tutorials/cloud/page2/#rq", 
            "text": "Use  pip  to install the RQ library:  $ sudo pip3 install rq  This will install the  redis  client module as well, as a dependency.", 
            "title": "RQ"
        }, 
        {
            "location": "/tutorials/cloud/page2/#getting-the-code", 
            "text": "Download the following files into the directory you just created:   utils.py  - Contains some common utilities used by both  worker.py  and  client.py .  worker.py  -  Contains the code to be executed by the workers as a job.  Dockerfile  - Is a recipe to build the Docker image for the workers.  client.py  - Is the client-side software to submit the simulations.   We will take a closer look at their contents in the \"Examining the Code\" chapter.", 
            "title": "Getting the Code"
        }, 
        {
            "location": "/tutorials/cloud/page2/#building-the-docker-image", 
            "text": "Building the image is done by issuing the following command in the directory\nwhere the above files can be found:  $ docker build . -t worker  The  Dockerfile  is picked up automatically by name, the build context is the \ncurrent directory ( . ), and the resulting image will be named  worker .   This will likely take a few minutes to complete. If you see a couple of warnings\nwritten in red, but the process continues, don't worry, this is expected.", 
            "title": "Building the Docker Image"
        }, 
        {
            "location": "/tutorials/cloud/page2/#publishing", 
            "text": "Now that we built our image for the worker nodes, we need to make it available\nfor our AWS Instances by uploading it to Docker Hub.  First authenticate yourself (in case you haven't already) by typing in your\nDocker ID and password after running this command:  $ docker login  Now tag the image \"into your repo\", substituting your user name (Docker ID), so\nDocker knows where to push the image:  $ docker tag worker username/worker  And finally issue the actual push:  $ docker push username/worker  This will upload about 400-500 MB of data, so it can take a while. Once it's\ndone, your image is available worldwide. You can see it appeared on  Docker\nHub . You may even get email notification if it was\nsuccessful.", 
            "title": "Publishing"
        }, 
        {
            "location": "/tutorials/cloud/page4/", 
            "text": "Deploying on AWS\n\n\nFirst you need an AWS account, so if you don't already have one, follow the\n\nsign-up procedure\n.\nYou will need to provide some personal information to Amazon, including full\nname, address, phone number, and a valid credit card. Then you will need to\nverify your identity via a phone call, or similar method.\n\n\nDon't worry, the computing power we will use is included in the Free Tier\npackage. If you're eligible for that, and won't leave virtual machines running\nand forget about them for days, you will only be charged for the network\ntraffic you generate. From experience, the total cost of completing this\ntutorial is about $0.01 (one cent). This is mostly the price of the data\ntransfer occurring when the Docker image is fetched from Docker Hub. You can\nkeep track of your spending on the \"Billing Dashboard\" page.\n\n\nIt's important to choose a Region, preferably the one closest to you\ngeographically. Then make sure you always have that Region selected, because\nmost resources in many AWS services are bound to the Region in which they were\ncreated.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an ECS Cluster\n\n\nIn the AWS Console, select the ECS (EC2 Container Service). If this is your\nfirst time here, you will be greeted with a short introductory video. You can\nwatch it if you're interested.\n\n\nThen click the blue \"Get started\" button below. On the next page, you will be\npresented with a few choices that introduce you to the usage of ECS, but for\nnow, just hit Cancel. You can get back to the greeting video, and these example\nchoices anytime, if you delete all your ECS Clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick \"Create Cluster\". You could name your cluster anything, but let's type in\n\nopp-cluster\n now. If you wish to use the Free Tier, it's important to set the\n\"EC2 Instance Type\" to \nt2.micro\n. Set the \"Number of instances\" option to \n4\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\nScroll down, and in the Networking section, and the \"Security group inbound\nrules\" group, type \n6379\n to the \"Port range\" field. This is the port on which\nRedis listens for incoming connections, so we must let traffic into our virtual\nnetwork on this port.\n\n\nNo need to change anything else; scroll down and click \"Create\".\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn a few moments, your cluster will be ready.\n\n\nCreating the Redis Task Definition\n\n\nOnce all resources are created, Switch to the \"Task Definitions\" page on the top\nleft.\n\n\nClick \"Create new Task Definition\". Type in \nredis\n as Name.\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick the blue \"Add container\" button. The Container name can be anything, but\nlet's enter \nredis\n here as well, just for the sake of consistency.\n\n\nThe image name should be \nredis\n, since we will run the official Redis image\navailable on Docker Hub.\n\n\nWe will also have to enter a memory limit. Type in \n512\n. You can refine this\nlater if you run into issues.\n\n\nTo make the server accessible from the outside, we need to add a Port mapping as\nwell. Just enter \n6379\n in the \"Host port\" and \"Container port\" fields, and\nleave the \"Protocol\" on \ntcp\n.\n\n\n\n\n\n\n\n\n\n\n\nThere is no need to change any of the dozens of additional options, just click\n\"Add\" on the bottom right.\n\n\nThis Task definition is done, scroll down and click \"Create\".\n\n\n\n\n\n\n\n\n\nStarting The Redis Task\n\n\nOn the details page of the \nredis\n Task Definition, select Actions / Run Task.\nThere is only one Cluster, so it should be selected already.\nWe only need a single Redis instance, so leave \"Number of tasks\" at \n1\n.\n\n\nNo need to change any more settings, click \"Run Task\" below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou should see the task appeared in \nPENDING\n status. In a few moments, it\nshould change to \nRUNNING\n. The table is updated automatically from time to\ntime, but you can refresh it manually as well, with the circular arrow button.\n\n\n\n\n\n\n\n\n\nGetting the IP of the Redis Server\n\n\nOnce the Task is running, click on its ID. Then on the \"Task details\" page,\nclick on the \"EC2 instance id\". On the bottom right corner, take a note of the\n\"IPv4 Public IP\" of the selected Instance. We will need this later.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe EC2 instance page most likely opened in a new browser tab. You can now close\nthat. If it opened in the same tab, navigate back to the ECS console.\n\n\n\n\n\n\n\n\n\nCreating the Worker Task Definition\n\n\nCreate another Task Definition, named \nworker\n. Add a container, name it\n\nworker\n, and in its configuration, set the image name to \nusername/worker\n.\nAgain, enter 512 as \"Memory Limit\".\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up port mapping for this container is not necessary. Instead, enter this\nas \"Command\", substituting the IP address of the Redis server you noted in the\nprevious step: \n-u,redis://172.17.1.19\n. Don't forget to click \"Add\", then\n\"Create\".\n\n\n\n\n\n\n\n\n\n\n\nCreating the Worker Service\n\n\nSwitch back to the Clusters page, and select the \nopp-cluster\n Cluster.\n\n\n\n\n\n\n\n\n\n\n\nOn the Services tab, click \"Create\". Select the \nworker\n:\n1\n Task Definition.\nName the Service \nworker\n as well. Set the number of tasks to \n3\n.\n\n\n\n\n\n\n\n\n\n\n\nClick \"Next step\". Leave ELB type on \"None\", and click \"Next step\". Skip auto\nscaling by clicking \"Next step\" yet again.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the review page, click \"Create Service\". Once it was succesfully created,\nclick \"View Service\". The worker services should appear on the \"Tasks\" tab as\n\nPENDING\n. You can't see the \nredis\n task here, as it is not part of this\nservice, but it is still running, and it's listed among the tasks of the\ncluster. Again, the table is updated automatically, but you can refresh it\nmanually as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce all tasks change into \nRUNNING\n status, the cluster is ready to use.", 
            "title": "Deploying on AWS"
        }, 
        {
            "location": "/tutorials/cloud/page4/#deploying-on-aws", 
            "text": "First you need an AWS account, so if you don't already have one, follow the sign-up procedure .\nYou will need to provide some personal information to Amazon, including full\nname, address, phone number, and a valid credit card. Then you will need to\nverify your identity via a phone call, or similar method.  Don't worry, the computing power we will use is included in the Free Tier\npackage. If you're eligible for that, and won't leave virtual machines running\nand forget about them for days, you will only be charged for the network\ntraffic you generate. From experience, the total cost of completing this\ntutorial is about $0.01 (one cent). This is mostly the price of the data\ntransfer occurring when the Docker image is fetched from Docker Hub. You can\nkeep track of your spending on the \"Billing Dashboard\" page.  It's important to choose a Region, preferably the one closest to you\ngeographically. Then make sure you always have that Region selected, because\nmost resources in many AWS services are bound to the Region in which they were\ncreated.", 
            "title": "Deploying on AWS"
        }, 
        {
            "location": "/tutorials/cloud/page4/#creating-an-ecs-cluster", 
            "text": "In the AWS Console, select the ECS (EC2 Container Service). If this is your\nfirst time here, you will be greeted with a short introductory video. You can\nwatch it if you're interested.  Then click the blue \"Get started\" button below. On the next page, you will be\npresented with a few choices that introduce you to the usage of ECS, but for\nnow, just hit Cancel. You can get back to the greeting video, and these example\nchoices anytime, if you delete all your ECS Clusters.       Click \"Create Cluster\". You could name your cluster anything, but let's type in opp-cluster  now. If you wish to use the Free Tier, it's important to set the\n\"EC2 Instance Type\" to  t2.micro . Set the \"Number of instances\" option to  4 .       Scroll down, and in the Networking section, and the \"Security group inbound\nrules\" group, type  6379  to the \"Port range\" field. This is the port on which\nRedis listens for incoming connections, so we must let traffic into our virtual\nnetwork on this port.  No need to change anything else; scroll down and click \"Create\".       In a few moments, your cluster will be ready.", 
            "title": "Creating an ECS Cluster"
        }, 
        {
            "location": "/tutorials/cloud/page4/#creating-the-redis-task-definition", 
            "text": "Once all resources are created, Switch to the \"Task Definitions\" page on the top\nleft.  Click \"Create new Task Definition\". Type in  redis  as Name.       Click the blue \"Add container\" button. The Container name can be anything, but\nlet's enter  redis  here as well, just for the sake of consistency.  The image name should be  redis , since we will run the official Redis image\navailable on Docker Hub.  We will also have to enter a memory limit. Type in  512 . You can refine this\nlater if you run into issues.  To make the server accessible from the outside, we need to add a Port mapping as\nwell. Just enter  6379  in the \"Host port\" and \"Container port\" fields, and\nleave the \"Protocol\" on  tcp .      There is no need to change any of the dozens of additional options, just click\n\"Add\" on the bottom right.  This Task definition is done, scroll down and click \"Create\".", 
            "title": "Creating the Redis Task Definition"
        }, 
        {
            "location": "/tutorials/cloud/page4/#starting-the-redis-task", 
            "text": "On the details page of the  redis  Task Definition, select Actions / Run Task.\nThere is only one Cluster, so it should be selected already.\nWe only need a single Redis instance, so leave \"Number of tasks\" at  1 .  No need to change any more settings, click \"Run Task\" below.       You should see the task appeared in  PENDING  status. In a few moments, it\nshould change to  RUNNING . The table is updated automatically from time to\ntime, but you can refresh it manually as well, with the circular arrow button.", 
            "title": "Starting The Redis Task"
        }, 
        {
            "location": "/tutorials/cloud/page4/#getting-the-ip-of-the-redis-server", 
            "text": "Once the Task is running, click on its ID. Then on the \"Task details\" page,\nclick on the \"EC2 instance id\". On the bottom right corner, take a note of the\n\"IPv4 Public IP\" of the selected Instance. We will need this later.       The EC2 instance page most likely opened in a new browser tab. You can now close\nthat. If it opened in the same tab, navigate back to the ECS console.", 
            "title": "Getting the IP of the Redis Server"
        }, 
        {
            "location": "/tutorials/cloud/page4/#creating-the-worker-task-definition", 
            "text": "Create another Task Definition, named  worker . Add a container, name it worker , and in its configuration, set the image name to  username/worker .\nAgain, enter 512 as \"Memory Limit\".       Setting up port mapping for this container is not necessary. Instead, enter this\nas \"Command\", substituting the IP address of the Redis server you noted in the\nprevious step:  -u,redis://172.17.1.19 . Don't forget to click \"Add\", then\n\"Create\".", 
            "title": "Creating the Worker Task Definition"
        }, 
        {
            "location": "/tutorials/cloud/page4/#creating-the-worker-service", 
            "text": "Switch back to the Clusters page, and select the  opp-cluster  Cluster.      On the Services tab, click \"Create\". Select the  worker : 1  Task Definition.\nName the Service  worker  as well. Set the number of tasks to  3 .      Click \"Next step\". Leave ELB type on \"None\", and click \"Next step\". Skip auto\nscaling by clicking \"Next step\" yet again.       On the review page, click \"Create Service\". Once it was succesfully created,\nclick \"View Service\". The worker services should appear on the \"Tasks\" tab as PENDING . You can't see the  redis  task here, as it is not part of this\nservice, but it is still running, and it's listed among the tasks of the\ncluster. Again, the table is updated automatically, but you can refresh it\nmanually as well.        Once all tasks change into  RUNNING  status, the cluster is ready to use.", 
            "title": "Creating the Worker Service"
        }, 
        {
            "location": "/tutorials/cloud/page5/", 
            "text": "Trying It Out\n\n\nConfiguration\n\n\nTo try it out, we add a new Configuration to the \nrouting\n sample that comes\nwith OMNeT++.\n\n\nAppend this section to the \nomnetpp.ini\n file in the \nsamples/routing\n directory:\n\n\n[Config MeshExperiment]\nnetwork = networks.Mesh\n**.width = ${10..20 step 2}\n**.height = ${10..20 step 2}\n\n**.destAddresses = \"0 2 5 6 9 14 17 18 23 27 29 36 42 52 89 123 150 183 192\"\n**.sendIaTime = uniform(100us, 10ms)  # high traffic\n\n**.vector-recording = false\nsim-time-limit = 10s\n\n\n\n\nThis configuration tries to make each of its runs execute for a significant\namount of time - about a minute each on average - by setting up a large network\nwith a small packet inter-arrival time and a relatively long simulation time\nlimit. It also disables vector recording to keep the size of the result files\nwithin the limits of the presented solution.\n\n\nExecution\n\n\nTo run it, open a terminal, set up the necessary environment variables using\n\nsetenv\n, then change into the directory of the sample:\n\n\n$ \ncd\n /path/to/omnetpp/\n$ . setenv\n$ \ncd\n samples/routing\n\n\n\n\nThen execute the \nclient.py\n script, substituting the IP address of your Redis\nserver on AWS:\n\n\n$ python3 /path/to/client.py ./routing -c MeshExperiment --redis-host \n172\n.17.1.19\n\n\n\n\nYou should see all runs submitted to the job queue, and after a while, finishing\nthem. The results should appear in the \nresults\n directory, just like with local\nexecution.\n\n\nComparison\n\n\nYou can compare the performance of the cloud solution with that of your own\nmachine by running the same campaign on both, through the \ntime\n utility:\n\n\n$ \ntime\n python3 /path/to/client.py ./routing -c MeshExperiment --redis-host \n172\n.17.1.19\n    \u2026\n$ \ntime\n opp_runall ./routing -c MeshExperiment\n\n\n\n\nAfter the output of each command, \ntime\n will print a summary of the execution\ntimes.\n\n\nLocally on my computer, using all \n8\n of its cores, the times were:\n\n\nreal    4m37.610s\nuser    25m4.576s\nsys 0m0.870s\n\n\n\n\nThe most interesting from our perspective is the \nreal\n time, as this shows the\namount of wall-clock time it took to run each command. The \nuser\n time is the\nnet processing time. It is much more than the \nreal\n time, because it is\naggregated across all cores.\n\n\nAnd of the one running on AWS:\n\n\nreal    4m12.310s\nuser    0m1.931s\nsys 0m1.925s\n\n\n\n\nWith just \n3\n single-core workers, the simulation completes faster than running\non \n8\n local cores. The \nuser\n time is next to negligible in this case, because\nmy computer spent most of the time idling, waiting for the results to come back.\n\n\nShutting Down\n\n\nTo prevent the aimless and useless exhaustion of your Free Tier resource\nallowance (or in the long term, your credit card balance), it is important to\nterminate your allocated resources once you are done using them. In our case,\nthese are the EC2 Instances started by the ECS Cluster. We need to shut these\ndown, because when calculating their usage, the whole amount of time the\ninstances are running (or paused, but kept available) is taken into account,\nnot just the time when they were actually used, doing something useful.\n\n\nIf you wish to use the Cluster again later, the easiest thing to do is scaling\ndown the number of underlying EC2 Instances to \n0\n. This will of course cause\nall running tasks to stop, as they no longer have anything to run on.\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, you can delete the whole Cluster. This will also terminate the\nEC2 Instances, but it will delete the \nworker\n Service as well.\n\n\n\n\n\n\n\n\n\n\n\nThe Task Definitions are not deleted either way, but this is not a problem,\nsince they can be kept around indefinitely, free of charge.", 
            "title": "Trying It Out"
        }, 
        {
            "location": "/tutorials/cloud/page5/#trying-it-out", 
            "text": "", 
            "title": "Trying It Out"
        }, 
        {
            "location": "/tutorials/cloud/page5/#configuration", 
            "text": "To try it out, we add a new Configuration to the  routing  sample that comes\nwith OMNeT++.  Append this section to the  omnetpp.ini  file in the  samples/routing  directory:  [Config MeshExperiment]\nnetwork = networks.Mesh\n**.width = ${10..20 step 2}\n**.height = ${10..20 step 2}\n\n**.destAddresses = \"0 2 5 6 9 14 17 18 23 27 29 36 42 52 89 123 150 183 192\"\n**.sendIaTime = uniform(100us, 10ms)  # high traffic\n\n**.vector-recording = false\nsim-time-limit = 10s  This configuration tries to make each of its runs execute for a significant\namount of time - about a minute each on average - by setting up a large network\nwith a small packet inter-arrival time and a relatively long simulation time\nlimit. It also disables vector recording to keep the size of the result files\nwithin the limits of the presented solution.", 
            "title": "Configuration"
        }, 
        {
            "location": "/tutorials/cloud/page5/#execution", 
            "text": "To run it, open a terminal, set up the necessary environment variables using setenv , then change into the directory of the sample:  $  cd  /path/to/omnetpp/\n$ . setenv\n$  cd  samples/routing  Then execute the  client.py  script, substituting the IP address of your Redis\nserver on AWS:  $ python3 /path/to/client.py ./routing -c MeshExperiment --redis-host  172 .17.1.19  You should see all runs submitted to the job queue, and after a while, finishing\nthem. The results should appear in the  results  directory, just like with local\nexecution.", 
            "title": "Execution"
        }, 
        {
            "location": "/tutorials/cloud/page5/#comparison", 
            "text": "You can compare the performance of the cloud solution with that of your own\nmachine by running the same campaign on both, through the  time  utility:  $  time  python3 /path/to/client.py ./routing -c MeshExperiment --redis-host  172 .17.1.19\n    \u2026\n$  time  opp_runall ./routing -c MeshExperiment  After the output of each command,  time  will print a summary of the execution\ntimes.  Locally on my computer, using all  8  of its cores, the times were:  real    4m37.610s\nuser    25m4.576s\nsys 0m0.870s  The most interesting from our perspective is the  real  time, as this shows the\namount of wall-clock time it took to run each command. The  user  time is the\nnet processing time. It is much more than the  real  time, because it is\naggregated across all cores.  And of the one running on AWS:  real    4m12.310s\nuser    0m1.931s\nsys 0m1.925s  With just  3  single-core workers, the simulation completes faster than running\non  8  local cores. The  user  time is next to negligible in this case, because\nmy computer spent most of the time idling, waiting for the results to come back.", 
            "title": "Comparison"
        }, 
        {
            "location": "/tutorials/cloud/page5/#shutting-down", 
            "text": "To prevent the aimless and useless exhaustion of your Free Tier resource\nallowance (or in the long term, your credit card balance), it is important to\nterminate your allocated resources once you are done using them. In our case,\nthese are the EC2 Instances started by the ECS Cluster. We need to shut these\ndown, because when calculating their usage, the whole amount of time the\ninstances are running (or paused, but kept available) is taken into account,\nnot just the time when they were actually used, doing something useful.  If you wish to use the Cluster again later, the easiest thing to do is scaling\ndown the number of underlying EC2 Instances to  0 . This will of course cause\nall running tasks to stop, as they no longer have anything to run on.      Alternatively, you can delete the whole Cluster. This will also terminate the\nEC2 Instances, but it will delete the  worker  Service as well.      The Task Definitions are not deleted either way, but this is not a problem,\nsince they can be kept around indefinitely, free of charge.", 
            "title": "Shutting Down"
        }, 
        {
            "location": "/tutorials/cloud/page6/", 
            "text": "Examining the Code\n\n\nThis page describes in detail the contents of each used code file.\n\n\nUtilities\n\n\nThere are a few things that both the workers and the client need. These are\nfactored out into a common module, \nutils.py\n. You can download the entire file\nfrom \nhere\n.\n\n\nIn the first half of it, there are the imports it will use, and a \nQuietBytes\n\nhelper class. We use this in place of its superclass, the built-in \nbytes\n type.\nIt only changes the string representation of the base type, to make it shorter\nthan the actual contents. There is a technical reason for using this: It reduces\nthe amount of data transferred to, and stored in, the Redis database.\n\n\n\n\n\nThere are also two functions in it to handle ZIP archives in memory. The first\nis for extracting, and the second is for compressing, with the option to exclude\nsome directories.\n\n\n\n\n\nWorker code\n\n\nAnd the code for the jobs, \nworker.py\n:\n\n\n\n\n\nWith the actual job function:\n\n\n\n\n\nThe comments make its operation pretty straightforward.\n\n\nThe model needs to be cleaned, then rebuilt inside the container, because the\nversion of some basic system libraries might not match that of those present on\nthe host system, which would lead to incompatibility problems, possibly\npreventing the simulation from starting.\n\n\nDockerfile\n\n\nWe select the base image to be \nubuntu\n:\n16.04\n, then we install Python, pip, the\ndependencies of OMNeT++, and wget.\n\n\n\n\n\nWe upgrade pip using itself, then install RQ with it. It will also install the\nRedis client module as a dependency. Then a few environment variables need to be\nset, to make RQ use the right character encoding.\n\n\n\n\n\nNext, we copy the worker source code into the image, and set the working directory.\n\n\n\n\n\nDownloading the OMNeT++ 5.1.1 Core release archive from the official website,\nextracting it, then deleting it. The referer URL has to be passed to \nwget\n,\notherwise the server denies access. The \n--progress\n flag is there just to\nreduce the amount of textual output, which would overly pollute the build log.\n\n\n\n\n\nThe \nbin\n directory added to the \nPATH\n environment variable (which would be\ndone by \nsetenv\n normally). Finally the standard building procedure is performed\nby running \n./configure\n and \nmake\n. Both graphical runtime environments and the\nsupport for 3D rendering are disabled. The \n-j $(nproc)\n arguments to \nmake\n\nenable it to use all your local CPU cores when building OMNeT++ itself.\n\n\n\n\n\nInstalling \nccache\n to make subsequent builds of the same model sources faster:\n\n\n\nAnd finally setting up the entry point to launch the rq worker, asking it to\nkeep the results only for one minute. This will be enough, because the client\nwill start downloading them right away, and it will reduce the amount of data\nstored in the Redis database on average.\n\n\n\n\n\nLater, when we run containers from the image, we will be able to append\nadditional arguments to the entrypoint.\n\n\nClient Software\n\n\nThe following file, \nclient.py\n, implements the\ncommand-line application for submitting jobs and getting the results.\n\n\nFirst, the usual imports:\n\n\n\nThen a helper function to resolve the run filter in a given configuration by\ninvoking the \nopp_run\n tool locally, in \"query\" mode. This is not strictly\nnecessary, since using a run filter is optional, but it's a nice addition.\n\n\n\n\n\nDefining the arguments of the tool and parsing their values:\n\n\n\nSetting up the connection to the job queue, then using the helper function to\nget the actual list of run numbers. Finally pack the model source into a\ncompressed archive (ZIP) in memory. A few directories are excluded from this\narchive, because they are not needed by the workers, and are usually very large.\n\n\n\n\n\nSubmitting a job into the queue for each run, storing the jobs in a list.\nThe run number is also written into the \nmeta\n field of each job, but that is\nnecessary only so we know later which run was performed by a particular job, and\nwe can print it when it is done. The job function itself doesn't use this, only\nits parameters.\n\n\n\n\n\nAnd finally poll for the results of each job, downloading and unpacking the\noutput (the results) of completed jobs, and removing them from the list:\n\n\n\n\n\nAnd exit when all jobs are completed.", 
            "title": "Examining the Code"
        }, 
        {
            "location": "/tutorials/cloud/page6/#examining-the-code", 
            "text": "This page describes in detail the contents of each used code file.", 
            "title": "Examining the Code"
        }, 
        {
            "location": "/tutorials/cloud/page6/#utilities", 
            "text": "There are a few things that both the workers and the client need. These are\nfactored out into a common module,  utils.py . You can download the entire file\nfrom  here .  In the first half of it, there are the imports it will use, and a  QuietBytes \nhelper class. We use this in place of its superclass, the built-in  bytes  type.\nIt only changes the string representation of the base type, to make it shorter\nthan the actual contents. There is a technical reason for using this: It reduces\nthe amount of data transferred to, and stored in, the Redis database.   There are also two functions in it to handle ZIP archives in memory. The first\nis for extracting, and the second is for compressing, with the option to exclude\nsome directories.", 
            "title": "Utilities"
        }, 
        {
            "location": "/tutorials/cloud/page6/#worker-code", 
            "text": "And the code for the jobs,  worker.py :   With the actual job function:   The comments make its operation pretty straightforward.  The model needs to be cleaned, then rebuilt inside the container, because the\nversion of some basic system libraries might not match that of those present on\nthe host system, which would lead to incompatibility problems, possibly\npreventing the simulation from starting.", 
            "title": "Worker code"
        }, 
        {
            "location": "/tutorials/cloud/page6/#dockerfile", 
            "text": "We select the base image to be  ubuntu : 16.04 , then we install Python, pip, the\ndependencies of OMNeT++, and wget.   We upgrade pip using itself, then install RQ with it. It will also install the\nRedis client module as a dependency. Then a few environment variables need to be\nset, to make RQ use the right character encoding.   Next, we copy the worker source code into the image, and set the working directory.   Downloading the OMNeT++ 5.1.1 Core release archive from the official website,\nextracting it, then deleting it. The referer URL has to be passed to  wget ,\notherwise the server denies access. The  --progress  flag is there just to\nreduce the amount of textual output, which would overly pollute the build log.   The  bin  directory added to the  PATH  environment variable (which would be\ndone by  setenv  normally). Finally the standard building procedure is performed\nby running  ./configure  and  make . Both graphical runtime environments and the\nsupport for 3D rendering are disabled. The  -j $(nproc)  arguments to  make \nenable it to use all your local CPU cores when building OMNeT++ itself.   Installing  ccache  to make subsequent builds of the same model sources faster:  And finally setting up the entry point to launch the rq worker, asking it to\nkeep the results only for one minute. This will be enough, because the client\nwill start downloading them right away, and it will reduce the amount of data\nstored in the Redis database on average.   Later, when we run containers from the image, we will be able to append\nadditional arguments to the entrypoint.", 
            "title": "Dockerfile"
        }, 
        {
            "location": "/tutorials/cloud/page6/#client-software", 
            "text": "The following file,  client.py , implements the\ncommand-line application for submitting jobs and getting the results.  First, the usual imports:  Then a helper function to resolve the run filter in a given configuration by\ninvoking the  opp_run  tool locally, in \"query\" mode. This is not strictly\nnecessary, since using a run filter is optional, but it's a nice addition.   Defining the arguments of the tool and parsing their values:  Setting up the connection to the job queue, then using the helper function to\nget the actual list of run numbers. Finally pack the model source into a\ncompressed archive (ZIP) in memory. A few directories are excluded from this\narchive, because they are not needed by the workers, and are usually very large.   Submitting a job into the queue for each run, storing the jobs in a list.\nThe run number is also written into the  meta  field of each job, but that is\nnecessary only so we know later which run was performed by a particular job, and\nwe can print it when it is done. The job function itself doesn't use this, only\nits parameters.   And finally poll for the results of each job, downloading and unpacking the\noutput (the results) of completed jobs, and removing them from the list:   And exit when all jobs are completed.", 
            "title": "Client Software"
        }, 
        {
            "location": "/tutorials/cloud/page7/", 
            "text": "Potential Improvements, Alternatives\n\n\nKeep in mind that the main point of this tutorial was to present a very simple\nsolution. This means that there are some significant compromises we had to make.\nIn this last section, we discuss a few of them, as well some alternatives to\nparts of the system.\n\n\nLimitations\n\n\nSome deficiencies of the current solution, with suggestions on how to alleviate\nthem:\n\n\n\n\n\n\nIt only works well with simple models (like the ones in the \nsamples\n\n   folder). It has no concept of any project features or referenced projects.\n   Advanced use cases, for example ones involving multiple processes (like with\n   Veins), are not supported. Some custom adjustments to the worker function are\n   necessary to make these kinds of simulations possible.\n\n\n\n\n\n\nModel sources are always distributed as a whole. This is not well suited for\n   quick iteration when experimenting with the code or with parameter values,\n   since we can't take advantage of incremental building. This also generates\n   more network traffic, which means longer startup times with large models.\n\n\n\n\n\n\nIf the model is already in an online repository (GitHub or similar), the\n   workers could be set up to pull a specific revision from there before each\n   run. To avoid having to push the code in that repository after each change,\n   a local git server can be started as well.\n\n\nTo avoid even having to commit the changes into git before each iteration,\n   the code can be synchronized to the workers using something like \nrsync\n, or\n   shared with them via a network file system, for example \nsshfs\n or \nnfs\n.\n\n\n\n\nThe model is built before every run. Even with \nccache\n, every model is built\n   from scratch in every worker container at least once, and the linking phase\n   still happens before every run.\n\n\n\n\nThe ideal solution would be building just once, either in a different kind of\n   job on one of the workers, or on the local machine. Then distributing the\n   built model among the workers, where they are cached locally, and shared\n   among containers running on the same host (using a common volume), so they\n   only pass through the network as many times as absolutely necessary.\n\n\n\n\n\n\nThe client script might not be the most convenient to use. It could be useful\n   to extend it with some more options, or even integrate it into the IDE.\n\n\n\n\n\n\nThere is no error handling or logging to speak of, the robustness is\n   questionable, and we paid no attention to security at all. These are\n   relatively significant omissions.\n\n\n\n\n\n\nOn multi-core worker machines, multiple worker containers need to be started\n   to take full advantage of their capabilities, since currently a worker only\n   performs one run at a time. This can be a good or a bad thing, depending on\n   your needs. This way, there is more control over how much resources the\n   system is allowed to use, but makes the overall picture a bit unwieldy.\n\n\n\n\n\n\nThe way the model is passed to the jobs and the results are retrieved is not\n   optimal. All data in both directions is stored in the Redis server operating\n   the job queue. Since Redis is an in-memory database, this places a limit on\n   the overall scalability of the solution, mostly on the size of the results.\n   This is why vector recording and event logging is recommended to be turned\n   off for now, at least for large simulations.\n\n\n\n\n\n\nA good solution for this would be using a dedicated storage space, accessible\n   both by the client and the workers. On AWS, S3 (Simple Storage Service) is a\n   promising candidate. Other cloud providers also have similar data storage\n   services. Additionally, any of the options noted above for sharing the code\n   while iterating can be used for result retrieval as well.\n\n\n\n\nThe progress and console output of the runs are currently not reported at\n   all, not while they are under execution, nor afterward. Real-time monitoring\n   would be useful, and it can be implemented probably the most easily through\n   the already available Redis server.\n\n\n\n\nAlternatives\n\n\nMany parts of the presented architecture can be swapped out for alternatives. A\nfew examples:\n\n\n\n\n\n\nInstead of Docker Hub, the image for the workers could also be provided via\n  AWS ECR (EC2 Container Registry) - or a similar service on other providers.\n  This would likely improve privacy, and lessen out-of-cloud network traffic\n  when the image needs to be fetched, also potentially improving container\n  startup times.\n\n\n\n\n\n\nThis tutorial is supposed to be adaptable to any other cloud provider: Azure,\n  Google Cloud Platform, DigitalOcean, etc.\n\n\n\n\n\n\nThe Docker image could be built automatically on Docker Cloud if the\n  Dockerfile was hosted in a GitHub or BitBucket repository. While this is often\n  useful, its advantages are questionable in this exact situation.\n\n\n\n\n\n\nInstead of RQ, Celery could also be used as job queue.\n\n\n\n\n\n\nAWS has a specialized service for job queuing, called Batch. Azure also offers\n  a similar service with the same name. We could also have used AWS Batch for\n  scheduling instead of running our own job queue. We chose not to use it to\n  facilitate porting of the solution to other cloud providers.\n\n\n\n\n\n\nDocker Cloud can also be used to\n  \ndeploy and manage a swarm\n\n  on AWS or Azure instead of their own container services.\n\n\n\n\n\n\nIf the worker function itself needs to be adjusted often, the image needs to\n  be rebuilt, and the containers need to be restarted each time. This can be\n  avoided by synchronizing the script to the workers using the same methods as\n  described above for model code distribution. The \nrq worker\n process would\n  still need to be restarted when the script changed, so it is reloaded.", 
            "title": "Potential Improvements, Alternatives"
        }, 
        {
            "location": "/tutorials/cloud/page7/#potential-improvements-alternatives", 
            "text": "Keep in mind that the main point of this tutorial was to present a very simple\nsolution. This means that there are some significant compromises we had to make.\nIn this last section, we discuss a few of them, as well some alternatives to\nparts of the system.", 
            "title": "Potential Improvements, Alternatives"
        }, 
        {
            "location": "/tutorials/cloud/page7/#limitations", 
            "text": "Some deficiencies of the current solution, with suggestions on how to alleviate\nthem:    It only works well with simple models (like the ones in the  samples \n   folder). It has no concept of any project features or referenced projects.\n   Advanced use cases, for example ones involving multiple processes (like with\n   Veins), are not supported. Some custom adjustments to the worker function are\n   necessary to make these kinds of simulations possible.    Model sources are always distributed as a whole. This is not well suited for\n   quick iteration when experimenting with the code or with parameter values,\n   since we can't take advantage of incremental building. This also generates\n   more network traffic, which means longer startup times with large models.    If the model is already in an online repository (GitHub or similar), the\n   workers could be set up to pull a specific revision from there before each\n   run. To avoid having to push the code in that repository after each change,\n   a local git server can be started as well.  To avoid even having to commit the changes into git before each iteration,\n   the code can be synchronized to the workers using something like  rsync , or\n   shared with them via a network file system, for example  sshfs  or  nfs .   The model is built before every run. Even with  ccache , every model is built\n   from scratch in every worker container at least once, and the linking phase\n   still happens before every run.   The ideal solution would be building just once, either in a different kind of\n   job on one of the workers, or on the local machine. Then distributing the\n   built model among the workers, where they are cached locally, and shared\n   among containers running on the same host (using a common volume), so they\n   only pass through the network as many times as absolutely necessary.    The client script might not be the most convenient to use. It could be useful\n   to extend it with some more options, or even integrate it into the IDE.    There is no error handling or logging to speak of, the robustness is\n   questionable, and we paid no attention to security at all. These are\n   relatively significant omissions.    On multi-core worker machines, multiple worker containers need to be started\n   to take full advantage of their capabilities, since currently a worker only\n   performs one run at a time. This can be a good or a bad thing, depending on\n   your needs. This way, there is more control over how much resources the\n   system is allowed to use, but makes the overall picture a bit unwieldy.    The way the model is passed to the jobs and the results are retrieved is not\n   optimal. All data in both directions is stored in the Redis server operating\n   the job queue. Since Redis is an in-memory database, this places a limit on\n   the overall scalability of the solution, mostly on the size of the results.\n   This is why vector recording and event logging is recommended to be turned\n   off for now, at least for large simulations.    A good solution for this would be using a dedicated storage space, accessible\n   both by the client and the workers. On AWS, S3 (Simple Storage Service) is a\n   promising candidate. Other cloud providers also have similar data storage\n   services. Additionally, any of the options noted above for sharing the code\n   while iterating can be used for result retrieval as well.   The progress and console output of the runs are currently not reported at\n   all, not while they are under execution, nor afterward. Real-time monitoring\n   would be useful, and it can be implemented probably the most easily through\n   the already available Redis server.", 
            "title": "Limitations"
        }, 
        {
            "location": "/tutorials/cloud/page7/#alternatives", 
            "text": "Many parts of the presented architecture can be swapped out for alternatives. A\nfew examples:    Instead of Docker Hub, the image for the workers could also be provided via\n  AWS ECR (EC2 Container Registry) - or a similar service on other providers.\n  This would likely improve privacy, and lessen out-of-cloud network traffic\n  when the image needs to be fetched, also potentially improving container\n  startup times.    This tutorial is supposed to be adaptable to any other cloud provider: Azure,\n  Google Cloud Platform, DigitalOcean, etc.    The Docker image could be built automatically on Docker Cloud if the\n  Dockerfile was hosted in a GitHub or BitBucket repository. While this is often\n  useful, its advantages are questionable in this exact situation.    Instead of RQ, Celery could also be used as job queue.    AWS has a specialized service for job queuing, called Batch. Azure also offers\n  a similar service with the same name. We could also have used AWS Batch for\n  scheduling instead of running our own job queue. We chose not to use it to\n  facilitate porting of the solution to other cloud providers.    Docker Cloud can also be used to\n   deploy and manage a swarm \n  on AWS or Azure instead of their own container services.    If the worker function itself needs to be adjusted often, the image needs to\n  be rebuilt, and the containers need to be restarted each time. This can be\n  avoided by synchronizing the script to the workers using the same methods as\n  described above for model code distribution. The  rq worker  process would\n  still need to be restarted when the script changed, so it is reloaded.", 
            "title": "Alternatives"
        }, 
        {
            "location": "/tutorials/swarm/", 
            "text": "Introduction\n\n\nThis tutorial will show you how to utilize AWS, Amazon's cloud computing service,\nfor running INET simulation campaigns. Following registration and minimal configuration\non the AWS management interface, you'll be able to use a command-line tool \nfor running simulations on AWS much like you'd run simulations locally. Our tool\ntransparently takes care of submitting the simulation jobs to AWS, orchestrates \ntheir execution, and downloads the result files into the same local \nresults/\n\nfolder where the locally running simulation would create them.\n\n\nYour simulation project needs to be published on GitHub, because AWS cloud\nnodes retrieve the source code of the simulation by checking it out from GitHub.\nThe tool currently has some hardcoded assumptions about the project, so it only \nworks with INET and INET forks. Work is underway to make the tool generic\nenough to run any simulation.\n\n\nRunning the simulation on a cloud service incurs some overhead (retrieving the source of the\nsimulation on cloud nodes, building it, distributing the binaries to all processors,\nand finally downloading the results to your own computer), so your simulation campaign\nneeds to be large enough to benefit from cloud computing: it needs to consist of \nseveral simulation runs, and each run should be longer than at least a couple seconds\n(real time).\n\n\nAlthough AWS offers a Free Tier for trial purposes, the requirements for running\nINET simulations unfortunately exceeds the resource constraints of the Free Tier.\nHowever, AWS usage is very affordable (expect prices of around 1 USD for one hour of uptime),\nso it is still well worth it if you have simulation campaigns that take too\nlong to complete on your locally available computing resources. \n(We are not affiliated with Amazon.)\n\n\nThis solution utilizes Docker Swarm, and a couple of other services and technologies.\nIn addition to scripts that manage the tasks closely associated with running simulations\nremotely, we also provide a command-line tool to make the management of the Swarm on AWS\neasier.", 
            "title": "Motivation"
        }, 
        {
            "location": "/tutorials/swarm/#introduction", 
            "text": "This tutorial will show you how to utilize AWS, Amazon's cloud computing service,\nfor running INET simulation campaigns. Following registration and minimal configuration\non the AWS management interface, you'll be able to use a command-line tool \nfor running simulations on AWS much like you'd run simulations locally. Our tool\ntransparently takes care of submitting the simulation jobs to AWS, orchestrates \ntheir execution, and downloads the result files into the same local  results/ \nfolder where the locally running simulation would create them.  Your simulation project needs to be published on GitHub, because AWS cloud\nnodes retrieve the source code of the simulation by checking it out from GitHub.\nThe tool currently has some hardcoded assumptions about the project, so it only \nworks with INET and INET forks. Work is underway to make the tool generic\nenough to run any simulation.  Running the simulation on a cloud service incurs some overhead (retrieving the source of the\nsimulation on cloud nodes, building it, distributing the binaries to all processors,\nand finally downloading the results to your own computer), so your simulation campaign\nneeds to be large enough to benefit from cloud computing: it needs to consist of \nseveral simulation runs, and each run should be longer than at least a couple seconds\n(real time).  Although AWS offers a Free Tier for trial purposes, the requirements for running\nINET simulations unfortunately exceeds the resource constraints of the Free Tier.\nHowever, AWS usage is very affordable (expect prices of around 1 USD for one hour of uptime),\nso it is still well worth it if you have simulation campaigns that take too\nlong to complete on your locally available computing resources. \n(We are not affiliated with Amazon.)  This solution utilizes Docker Swarm, and a couple of other services and technologies.\nIn addition to scripts that manage the tasks closely associated with running simulations\nremotely, we also provide a command-line tool to make the management of the Swarm on AWS\neasier.", 
            "title": "Introduction"
        }, 
        {
            "location": "/tutorials/swarm/page1/", 
            "text": "In this page we show you how to deploy and use the tools to run simulations on AWS.\n\n\n\n\nCaution\n\n\nThe solution and the toolset presented here are currently experimental, and we are collecting\nfeedback about it. Please let us know if you are interested, and if you try it,\nwe request you to report bugs, errors or any difficulty you experience in using it.\nWe'd also be happy to hear about successful usage.\nUse the bugtracker link at the bottom of this page for feedback.\n\n\n\n\nInstallation\n\n\nInstall Docker, python3 and pip3 on your local computer. We recommend that you use Linux.\nA working OMNeT++ installation, and a checked-out INET (fork) git repository is also necessary.\n\n\nDeployment on AWS\n\n\nCreating an AWS Account\n\n\nTo access any web service AWS offers, you must first create an AWS account at \nhttp://aws.amazon.com\n.\nIf you already have one, just log in to the \nAWS Management Console\n with it;\notherwise, follow the instructions here: \nAWS registration\n.\n\n\nDuring registration, you will have to enter your credit card number. Amazon will charge it with a small amount (1 USD) to verify it.\n\n\nCreating the Access Policy\n\n\nOn the AWS Management Console, navigate to the IAM Service configuration page, and switch to \nPolicies\n. Or just\nclick \nthis link\n, it will take you there.\n\n\nClick the \nCreate policy\n button. Switch over to the \nJSON\n tab.\nPaste the contents of \nthis file\n into the entry field, replacing its entire contents.\n\n\nThis policy is the superset of the officially published one on the \nDocker for AWS website\n.\nIt had to be slightly altered to make it fit into the default size limit, so it grants slightly more privileges than necessary.\nIt also adds the \nec2\n:\nCreateKeyPair\n and the \ncloudwatch\n:\nPutMetricAlarm\n permissions.\nThese are necessary to automate the connection to the swarm, and the shutting down of the machines after they have been idle for a while.\n\n\nClick \nReview policy\n. Enter a \nName\n for the new policy, for example \"inet-docker-swarm-policy\", then click \nCreate policy\n.\n\n\nCreating the User\n\n\nSwitch over to \nUsers\n and click \nAdd user\n.\n\n\nEnter a \nUser name\n, for example \"inet-swarm-cli\", and tick the checkbox next to \nProgrammatic access\n. Leave the other checkbox unchecked. Click \nNext: Permissions\n. Select \nAttach existing policies directly\n, search for the name of the policy we just created \n(by typing in a part of it, like \"inet\")\n, then check the checkboy next to it, and click \nNext: Review\n at the bottom. If everything looks alright, click \nCreate user\n.\n\n\nAs the final step of user creation, save the \nAccess key ID\n and the \nSecret access key\n, somewhere safe. It's a good idea that you do this by clicking \nDownload .csv\n. This will let you download this information into a simple text file, so you won't make any mistakes while copy-pasting them.\n\n\nAlso, read the notice in the green area, particularly this part:\n\n\"This is the last time these credentials will be available to download. However, you can create new credentials at any time.\"\n\nThis means that if you don't save the key ID and the secret key now, you will have to delete this user and create a new one.\n\n\n\n\nImportant\n\n\nKeep these strings private, since these grant access to your account, without the need for your password or other credentials.\n(Of course, only until you delete this user, or revoke its permissions.) Treat them with similar caution as you do your passwords.\n\n\n\n\nConfiguring CLI Access\n\n\nTo let your computer manage and use your AWS account, we have to configure it with the credentials of the user you just created for it.\nFirst we need to install the AWS CLI utility using \npip\n:\n\n\n$ pip3 install --user --upgrade awscli\n\n\n\n\nThen start the configuration:\n\n\n$ aws configure\n\n\n\n\nIf at first you get \naws\n:\n \ncommand\n \nnot\n \nfound\n, or \nThe program \naws\n is currently not installed\n, try running \n~/.local/bin/aws configure\n instead.\n\n\nWhen asked, enter the \nAccess Key ID\n, then the \nSecret Access Key\n. They will be recorded into an INI file, which is by default at \n~/.aws/credentials\n.\n\n\nFor \nDefault region\n, choose the one closest to you geographically. You can find the list of region codes and their locations\n\nhere\n.\nIn my case, the Frankfurt datacenter was closest, so I entered \neu-central-1\n. This setting is recorded in the \n~/.aws/config\n file.\nYou can find more info about Regions and Availability Zones\n\nhere\n,\n\nhere\n, and\n\nhere\n.\n\n\nYou can leave \nDefault output format\n empty.\n\n\n\n\nImportant\n\n\nOnce all this information is entered correctly, any software you run on your computer has access\nto your AWS account, as permitted by the policy attached to the configured user.\nRemove (rename) the \ncredentials\n file mentioned above to (temporarily) disable access.\nThe proper way to completely and permanently revoke this access is to delete the IAM User we just created.\n\n\n\n\nFrom this point on in this tutorial, we won't need the AWS Management Console to initiate any actions. However, if you wish, you can use it to observe and check for yourself what the \naws_swarm_tool.py\n script does.\n\n\nInstalling Local Client Scripts\n\n\nHaving set up your AWS account and access to it, we can proceed by installing the client-side tools for submitting simulations.\n\n\nFirst, install the required Python libraries:\n\n\n$ pip3 install --user --upgrade boto3 psutil pymongo redis rq\n\n\n\n\nSave the following files into \n~/.local/bin\n:\n\n\n\n\naws_swarm_tool.py\n\n\ndocker-compose.yml\n\n\ninet_runall_swarm.py\n\n\n\n\nMake the script files executable, and put \n~/.local/bin\n into the PATH if it does not already contain it:\n\n\n$ \ncd\n ~/.local/bin\n;\n chmod +x aws_swarm_tool.py inet_runall_swarm.py\n$ \nexport\n \nPATH\n=\n$PATH\n:\n$HOME\n/.local/bin\n\n\n\n\nRun the following command to set up the virtual machines and necessary infrastructure on AWS,\nand perform local configuration for accessing it:\n\n\n$ aws_swarm_tool.py init\n\n\n\n\nUsage\n\n\nRunning Simulations\n\n\nChange into the directory under INET (or your INET fork) that contains your simulation.\n\n\nIMPORTANT: Your INET installation should be a checked-out copy of a GitHub repository with all changes pushed up to GitHub,\nbecause our tool only sends the Git URL of your project and the hash of the currently checked-out commit to AWS, not the full source code.\nAdditionally, OMNeT++ should be ready to use, with its tools (like \nopp_run\n or \nopp_run_dbg\n) accessible as commands.\n\n\n$ \ncd\n examples/inet/ber\n\n\n\n\nEnter the command for running the simulations, using our \ninet_runall_swarm.py\n program instead of \n./run\n or \ninet\n:\n\n\n$ inet_runall_swarm.py -c ber-flavour-experiment\n\n\n\n\nThe \ninet_runall_swarm.py\n tool will expand the list of simulation runs to be executed, submit them to the job queue, and wait for the jobs to finish.\nThe results will be downloaded automatically into the \nresults\n folder.\n\n\nYou can monitor progress at \nhttp://localhost:9181/\n which displays the content of the job queue.\n\n\nStopping, Restarting and Deleting the Swarm\n\n\nOnce you are done, you can stop the machines:\n\n\n$ aws_swarm_tool.py halt\n\n\n\n\nThey will also shut down automatically after an hour of being idle.\n\n\n\n\nImportant\n\n\nAWS usage is billed by uptime, i.e. you are charged for the time the Swarm is up and running,\nregardless whether you are actually using it for simulations or not. Our tool configures AWS\nto halt the Swarm after one hour of idle time, but for extra safety, double-check that the\nSwarm has been stopped after you have finished working with it. WE ARE NOT RESPONSIBLE FOR\nEXTRA COSTS CAUSED BY LEAVING AWS NODES RUNNING. Read on for instructions.\n\n\n\n\nThe \nhalt\n command only shuts down the virtual machine instances, but leaves all other resources intact,\nthis way resuming the usage is faster than the initial deployment (with \ninit\n). These additional\nresources being there inactively do not cost you anything.\n\n\nTo use the Swarm again, you have to start it up again:\n\n\n$ aws_swarm_tool.py resume\n\n\n\n\nTo completely delete the entire Swarm:\n\n\n$ aws_swarm_tool.py delete\n\n\n\n\nChecking the Status Manually\n\n\n\n\nImportant\n\n\nWhen you do not intend to work with AWS for an extended period of time, you may want to\ndouble-check manually that the stack on AWS has been stopped indeed, and you are no longer\nbeing billed for it.\n\n\n\n\nTo check the status, select the EC2 service on the AWS Management Console,\nand check the \nRunning Instances\n.\nIf you see instances named \ninet-Node\n or \ninet-Manager\n, in the \nrunning\n state, the Swarm is active.\n\n\nTo delete the Swarm manually (if the command-line tool does not work for some reason), go to the\n\nCloudFormation service page\n on\nthe AWS Management Console. Tick the checkbox next to the \ninet\n Stack in the table, then click\n\nActions \n Delete Stack\n. This should remove all INET-related resources.\n\n\nFeedback and Discussion\n\n\nWe are looking for any kind of feedback or suggestion you might have regarding this tutorial here:\n\nhttps://github.com/omnetpp/omnetpp-tutorials/issues/3", 
            "title": "Installation and Usage"
        }, 
        {
            "location": "/tutorials/swarm/page1/#installation", 
            "text": "Install Docker, python3 and pip3 on your local computer. We recommend that you use Linux.\nA working OMNeT++ installation, and a checked-out INET (fork) git repository is also necessary.", 
            "title": "Installation"
        }, 
        {
            "location": "/tutorials/swarm/page1/#deployment-on-aws", 
            "text": "", 
            "title": "Deployment on AWS"
        }, 
        {
            "location": "/tutorials/swarm/page1/#creating-an-aws-account", 
            "text": "To access any web service AWS offers, you must first create an AWS account at  http://aws.amazon.com .\nIf you already have one, just log in to the  AWS Management Console  with it;\notherwise, follow the instructions here:  AWS registration .  During registration, you will have to enter your credit card number. Amazon will charge it with a small amount (1 USD) to verify it.", 
            "title": "Creating an AWS Account"
        }, 
        {
            "location": "/tutorials/swarm/page1/#creating-the-access-policy", 
            "text": "On the AWS Management Console, navigate to the IAM Service configuration page, and switch to  Policies . Or just\nclick  this link , it will take you there.  Click the  Create policy  button. Switch over to the  JSON  tab.\nPaste the contents of  this file  into the entry field, replacing its entire contents.  This policy is the superset of the officially published one on the  Docker for AWS website .\nIt had to be slightly altered to make it fit into the default size limit, so it grants slightly more privileges than necessary.\nIt also adds the  ec2 : CreateKeyPair  and the  cloudwatch : PutMetricAlarm  permissions.\nThese are necessary to automate the connection to the swarm, and the shutting down of the machines after they have been idle for a while.  Click  Review policy . Enter a  Name  for the new policy, for example \"inet-docker-swarm-policy\", then click  Create policy .", 
            "title": "Creating the Access Policy"
        }, 
        {
            "location": "/tutorials/swarm/page1/#creating-the-user", 
            "text": "Switch over to  Users  and click  Add user .  Enter a  User name , for example \"inet-swarm-cli\", and tick the checkbox next to  Programmatic access . Leave the other checkbox unchecked. Click  Next: Permissions . Select  Attach existing policies directly , search for the name of the policy we just created  (by typing in a part of it, like \"inet\") , then check the checkboy next to it, and click  Next: Review  at the bottom. If everything looks alright, click  Create user .  As the final step of user creation, save the  Access key ID  and the  Secret access key , somewhere safe. It's a good idea that you do this by clicking  Download .csv . This will let you download this information into a simple text file, so you won't make any mistakes while copy-pasting them.  Also, read the notice in the green area, particularly this part: \"This is the last time these credentials will be available to download. However, you can create new credentials at any time.\" \nThis means that if you don't save the key ID and the secret key now, you will have to delete this user and create a new one.   Important  Keep these strings private, since these grant access to your account, without the need for your password or other credentials.\n(Of course, only until you delete this user, or revoke its permissions.) Treat them with similar caution as you do your passwords.", 
            "title": "Creating the User"
        }, 
        {
            "location": "/tutorials/swarm/page1/#configuring-cli-access", 
            "text": "To let your computer manage and use your AWS account, we have to configure it with the credentials of the user you just created for it.\nFirst we need to install the AWS CLI utility using  pip :  $ pip3 install --user --upgrade awscli  Then start the configuration:  $ aws configure  If at first you get  aws :   command   not   found , or  The program  aws  is currently not installed , try running  ~/.local/bin/aws configure  instead.  When asked, enter the  Access Key ID , then the  Secret Access Key . They will be recorded into an INI file, which is by default at  ~/.aws/credentials .  For  Default region , choose the one closest to you geographically. You can find the list of region codes and their locations here .\nIn my case, the Frankfurt datacenter was closest, so I entered  eu-central-1 . This setting is recorded in the  ~/.aws/config  file.\nYou can find more info about Regions and Availability Zones here , here , and here .  You can leave  Default output format  empty.   Important  Once all this information is entered correctly, any software you run on your computer has access\nto your AWS account, as permitted by the policy attached to the configured user.\nRemove (rename) the  credentials  file mentioned above to (temporarily) disable access.\nThe proper way to completely and permanently revoke this access is to delete the IAM User we just created.   From this point on in this tutorial, we won't need the AWS Management Console to initiate any actions. However, if you wish, you can use it to observe and check for yourself what the  aws_swarm_tool.py  script does.", 
            "title": "Configuring CLI Access"
        }, 
        {
            "location": "/tutorials/swarm/page1/#installing-local-client-scripts", 
            "text": "Having set up your AWS account and access to it, we can proceed by installing the client-side tools for submitting simulations.  First, install the required Python libraries:  $ pip3 install --user --upgrade boto3 psutil pymongo redis rq  Save the following files into  ~/.local/bin :   aws_swarm_tool.py  docker-compose.yml  inet_runall_swarm.py   Make the script files executable, and put  ~/.local/bin  into the PATH if it does not already contain it:  $  cd  ~/.local/bin ;  chmod +x aws_swarm_tool.py inet_runall_swarm.py\n$  export   PATH = $PATH : $HOME /.local/bin  Run the following command to set up the virtual machines and necessary infrastructure on AWS,\nand perform local configuration for accessing it:  $ aws_swarm_tool.py init", 
            "title": "Installing Local Client Scripts"
        }, 
        {
            "location": "/tutorials/swarm/page1/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/tutorials/swarm/page1/#running-simulations", 
            "text": "Change into the directory under INET (or your INET fork) that contains your simulation.  IMPORTANT: Your INET installation should be a checked-out copy of a GitHub repository with all changes pushed up to GitHub,\nbecause our tool only sends the Git URL of your project and the hash of the currently checked-out commit to AWS, not the full source code.\nAdditionally, OMNeT++ should be ready to use, with its tools (like  opp_run  or  opp_run_dbg ) accessible as commands.  $  cd  examples/inet/ber  Enter the command for running the simulations, using our  inet_runall_swarm.py  program instead of  ./run  or  inet :  $ inet_runall_swarm.py -c ber-flavour-experiment  The  inet_runall_swarm.py  tool will expand the list of simulation runs to be executed, submit them to the job queue, and wait for the jobs to finish.\nThe results will be downloaded automatically into the  results  folder.  You can monitor progress at  http://localhost:9181/  which displays the content of the job queue.", 
            "title": "Running Simulations"
        }, 
        {
            "location": "/tutorials/swarm/page1/#stopping-restarting-and-deleting-the-swarm", 
            "text": "Once you are done, you can stop the machines:  $ aws_swarm_tool.py halt  They will also shut down automatically after an hour of being idle.   Important  AWS usage is billed by uptime, i.e. you are charged for the time the Swarm is up and running,\nregardless whether you are actually using it for simulations or not. Our tool configures AWS\nto halt the Swarm after one hour of idle time, but for extra safety, double-check that the\nSwarm has been stopped after you have finished working with it. WE ARE NOT RESPONSIBLE FOR\nEXTRA COSTS CAUSED BY LEAVING AWS NODES RUNNING. Read on for instructions.   The  halt  command only shuts down the virtual machine instances, but leaves all other resources intact,\nthis way resuming the usage is faster than the initial deployment (with  init ). These additional\nresources being there inactively do not cost you anything.  To use the Swarm again, you have to start it up again:  $ aws_swarm_tool.py resume  To completely delete the entire Swarm:  $ aws_swarm_tool.py delete", 
            "title": "Stopping, Restarting and Deleting the Swarm"
        }, 
        {
            "location": "/tutorials/swarm/page1/#checking-the-status-manually", 
            "text": "Important  When you do not intend to work with AWS for an extended period of time, you may want to\ndouble-check manually that the stack on AWS has been stopped indeed, and you are no longer\nbeing billed for it.   To check the status, select the EC2 service on the AWS Management Console,\nand check the  Running Instances .\nIf you see instances named  inet-Node  or  inet-Manager , in the  running  state, the Swarm is active.  To delete the Swarm manually (if the command-line tool does not work for some reason), go to the CloudFormation service page  on\nthe AWS Management Console. Tick the checkbox next to the  inet  Stack in the table, then click Actions   Delete Stack . This should remove all INET-related resources.", 
            "title": "Checking the Status Manually"
        }, 
        {
            "location": "/tutorials/swarm/page1/#feedback-and-discussion", 
            "text": "We are looking for any kind of feedback or suggestion you might have regarding this tutorial here: https://github.com/omnetpp/omnetpp-tutorials/issues/3", 
            "title": "Feedback and Discussion"
        }, 
        {
            "location": "/tutorials/swarm/page2/", 
            "text": "This is a distributed application, built on top of \nDocker Swarm\n\n(the new, integrated \"Swarm Mode\", not the legacy docker-swarm utility).\nIt is composed of seven different Docker \nservices\n.\n\n\nServices\n\n\nThe application is made up of several Docker services:\nredis, mongo, builder, runner, visualizer, dashboard, distcc.\nLet's discuss what each of them are there for, and what they do.\n\n\nRedis\n\n\nThis service runs the official Redis image on the Manager. It is needed by RQ (Redis Queue)\nthat we use for job queueing.\n\n\nMongo\n\n\nMongo is a database that we use for temporary storage of binaries and result files.\n\n\nBuilder\n\n\nThis is one of the two services running an RQ worker. It starts a single\ncontainer on the manager, and listens on the \nbuild\n queue for jobs.\n\n\nIt uses the distcc servers from the \ndistcc\n service through the \nbuildnet\n\nnetwork to distribute the compilation tasks across all nodes.\n\n\nOnce a build is done, it submits the binaries (actually, the \nlibINET.so\n file) to the Mongo service,\nso the runner containers can access it later.\n\n\nRunner\n\n\nThe other RQ worker, running as many containers in each host, as their respecrtive number\nof cpu cores. Except the manager, because that needs some extra juice running the other services,\nlike redis and mongo. it is done by requesting a large number of containers (100), but reserving\n95% of a core for each container, so they automatically \"expand\" to \"fill\" the available number\nof (remaining) CPUs, like a liquid.\n\n\nIt gets the built libINET.so from the MongoDB server, and also submits the simulation results there.\n\n\nVisualizer\n\n\nThis service starts a single container on the manager, using\nthe official \ndocker-swarm-visualizer\n image.\nIn that container, a web server runs that lets you quickly inspect\nthe state of the swarm, including its nodes, services and containers,\njust using your web browser. It listens on port \n8080\n, so once\nyour swarm application is up and running (and you are connected to the swarm\nif it is on AWS), you can check it out at [\nhttp://localhost:8080/\n].\n\n\n\n\n\nDashboard\n\n\nSimilarly to the \nvisualizer\n, this is an auxiliary service, running a single\ncontainer on the manager, with a web server in it.\nThis one lets you see, and manage in a limited way, the RQ queues, workers, and jobs.\nSee: [\nhttp://localhost:9181/\n].\n\n\n\n\n\ndistcc\n\n\nThis service starts exactly one container on all nodes (workers and the manager alike).\nThey all run a distcc server, listening for incoming requests for compilation (completely\nindependent from RQ).\nThey are only attached to the \nbuildnet\n network, and have deterministic IP addresses.\n\n\nWhen the \nbuilder\n container starts a build in a \nbuild\n job, it will try\nto connect to the \ndistcc\n containers, and will use them to distribute\nthe compilation tasks to all nodes.\n\n\nNetworks\n\n\nThe stack also contains two virtual networks. Each service is attached to\none or both of these networks. The networks are:\n\n\n\n\ninterlink\n\n\nbuildnet\n\n\n\n\nBoth of them use the \noverlay\n driver, meaning that these are entirely virtual\nnetworks, not interfering with the underlying real one between the nodes.\n\n\nInterlink\n\n\nThis is the main network, all services except \ndistcc\n are attached to it.\n\n\nBuildnet\n\n\nThe \nbuildnet\n network connects the containers of the \ndistcc\n service\nwith the \nbuilder\n service. It operates on a fixed subnet, The \nbuilder\n\n\nThis was only necessary to give the \ndistcc\n containers deterministic and known\nIP addresses. On \ninterlink\n they didn't always get the same addresses, they\nwere randomly interleaved with the containers of all the other services.\n\n\nThis would not be necessary at all if multicast traffic worked on \noverlay\n\nnetworks between nodes, because then we could just use the built-in zeroconf\nservice discovery capabilities of distcc (the software itself). However, until\n\nthis issue\n is resolved, we\nhave to resort to this solution.\n\n\nOperation\n\n\naws_swarm_tool init:\n\n\nDeploying the official CloudFormation template supplied by Docker, called Docker for AWS.\n\n\nUsing the default settings, the script creates 1 manager and 3 workers, each of them as a \nc4.4xlarge\n type Instance.\n\n\nIt also creates an alarm and an AutoScaling policy that makes sure that all machines are shut down after 1 hour of inactivity (precisely, if the maximum CPU utilization of the manager machine was below 10 percent for 4 consecutive 15 minute periods).\nThis is to reduce the chances that they are forgotten about, and left running indefinitely, generating unexpected expenditure.\n\n\nTo be able to connect to the Swarm we are about to create, we must first create and SSH keypair.\nThe \naws_swarm_tool.py\n can do this for us.\n\n\nConnecting to the Swarm is essentially opening an SSH connection to the manager machine, and forwarding\na handful of ports through that tunnel from the local machine to the swarm.\nThere is no need to do it manually, the \naws_swarm_tool.py\n script has a command for it:\n\n\n$ aws_swarm_tool.py connect\n\n\nIn addition to bringing up the SSH connection, the script also saves the process ID (PID) of the SSH client process into a file (so called PID-file) in a temporary directory (most likely \n/tmp\n), called\n\n\nWhat is Docker Swarm?\n\n\nWith version 1.12.0, Docker introduced \nSwarm Mode\n. It makes it possible to\nconnect multiple computers (called hosts or nodes) on a network, into a cluster - called swarm.\n\n\nThis new feature enables someting called \"container orchestration\". It makes the development, deployment,\nand maintenance of distributed, multi-container applications easier. You can read\nmore about it \nhere\n.\n\n\nOne great advantage of this is that wherever a Docker Swarm is configured, any\napplication can be run, let it be on local machines, or any cloud computing platform.", 
            "title": "How it Works"
        }, 
        {
            "location": "/tutorials/swarm/page2/#services", 
            "text": "The application is made up of several Docker services:\nredis, mongo, builder, runner, visualizer, dashboard, distcc.\nLet's discuss what each of them are there for, and what they do.", 
            "title": "Services"
        }, 
        {
            "location": "/tutorials/swarm/page2/#redis", 
            "text": "This service runs the official Redis image on the Manager. It is needed by RQ (Redis Queue)\nthat we use for job queueing.", 
            "title": "Redis"
        }, 
        {
            "location": "/tutorials/swarm/page2/#mongo", 
            "text": "Mongo is a database that we use for temporary storage of binaries and result files.", 
            "title": "Mongo"
        }, 
        {
            "location": "/tutorials/swarm/page2/#builder", 
            "text": "This is one of the two services running an RQ worker. It starts a single\ncontainer on the manager, and listens on the  build  queue for jobs.  It uses the distcc servers from the  distcc  service through the  buildnet \nnetwork to distribute the compilation tasks across all nodes.  Once a build is done, it submits the binaries (actually, the  libINET.so  file) to the Mongo service,\nso the runner containers can access it later.", 
            "title": "Builder"
        }, 
        {
            "location": "/tutorials/swarm/page2/#runner", 
            "text": "The other RQ worker, running as many containers in each host, as their respecrtive number\nof cpu cores. Except the manager, because that needs some extra juice running the other services,\nlike redis and mongo. it is done by requesting a large number of containers (100), but reserving\n95% of a core for each container, so they automatically \"expand\" to \"fill\" the available number\nof (remaining) CPUs, like a liquid.  It gets the built libINET.so from the MongoDB server, and also submits the simulation results there.", 
            "title": "Runner"
        }, 
        {
            "location": "/tutorials/swarm/page2/#visualizer", 
            "text": "This service starts a single container on the manager, using\nthe official  docker-swarm-visualizer  image.\nIn that container, a web server runs that lets you quickly inspect\nthe state of the swarm, including its nodes, services and containers,\njust using your web browser. It listens on port  8080 , so once\nyour swarm application is up and running (and you are connected to the swarm\nif it is on AWS), you can check it out at [ http://localhost:8080/ ].", 
            "title": "Visualizer"
        }, 
        {
            "location": "/tutorials/swarm/page2/#dashboard", 
            "text": "Similarly to the  visualizer , this is an auxiliary service, running a single\ncontainer on the manager, with a web server in it.\nThis one lets you see, and manage in a limited way, the RQ queues, workers, and jobs.\nSee: [ http://localhost:9181/ ].", 
            "title": "Dashboard"
        }, 
        {
            "location": "/tutorials/swarm/page2/#distcc", 
            "text": "This service starts exactly one container on all nodes (workers and the manager alike).\nThey all run a distcc server, listening for incoming requests for compilation (completely\nindependent from RQ).\nThey are only attached to the  buildnet  network, and have deterministic IP addresses.  When the  builder  container starts a build in a  build  job, it will try\nto connect to the  distcc  containers, and will use them to distribute\nthe compilation tasks to all nodes.", 
            "title": "distcc"
        }, 
        {
            "location": "/tutorials/swarm/page2/#networks", 
            "text": "The stack also contains two virtual networks. Each service is attached to\none or both of these networks. The networks are:   interlink  buildnet   Both of them use the  overlay  driver, meaning that these are entirely virtual\nnetworks, not interfering with the underlying real one between the nodes.", 
            "title": "Networks"
        }, 
        {
            "location": "/tutorials/swarm/page2/#interlink", 
            "text": "This is the main network, all services except  distcc  are attached to it.", 
            "title": "Interlink"
        }, 
        {
            "location": "/tutorials/swarm/page2/#buildnet", 
            "text": "The  buildnet  network connects the containers of the  distcc  service\nwith the  builder  service. It operates on a fixed subnet, The  builder  This was only necessary to give the  distcc  containers deterministic and known\nIP addresses. On  interlink  they didn't always get the same addresses, they\nwere randomly interleaved with the containers of all the other services.  This would not be necessary at all if multicast traffic worked on  overlay \nnetworks between nodes, because then we could just use the built-in zeroconf\nservice discovery capabilities of distcc (the software itself). However, until this issue  is resolved, we\nhave to resort to this solution.", 
            "title": "Buildnet"
        }, 
        {
            "location": "/tutorials/swarm/page2/#operation", 
            "text": "aws_swarm_tool init:  Deploying the official CloudFormation template supplied by Docker, called Docker for AWS.  Using the default settings, the script creates 1 manager and 3 workers, each of them as a  c4.4xlarge  type Instance.  It also creates an alarm and an AutoScaling policy that makes sure that all machines are shut down after 1 hour of inactivity (precisely, if the maximum CPU utilization of the manager machine was below 10 percent for 4 consecutive 15 minute periods).\nThis is to reduce the chances that they are forgotten about, and left running indefinitely, generating unexpected expenditure.  To be able to connect to the Swarm we are about to create, we must first create and SSH keypair.\nThe  aws_swarm_tool.py  can do this for us.  Connecting to the Swarm is essentially opening an SSH connection to the manager machine, and forwarding\na handful of ports through that tunnel from the local machine to the swarm.\nThere is no need to do it manually, the  aws_swarm_tool.py  script has a command for it:  $ aws_swarm_tool.py connect  In addition to bringing up the SSH connection, the script also saves the process ID (PID) of the SSH client process into a file (so called PID-file) in a temporary directory (most likely  /tmp ), called", 
            "title": "Operation"
        }, 
        {
            "location": "/tutorials/swarm/page2/#what-is-docker-swarm", 
            "text": "With version 1.12.0, Docker introduced  Swarm Mode . It makes it possible to\nconnect multiple computers (called hosts or nodes) on a network, into a cluster - called swarm.  This new feature enables someting called \"container orchestration\". It makes the development, deployment,\nand maintenance of distributed, multi-container applications easier. You can read\nmore about it  here .  One great advantage of this is that wherever a Docker Swarm is configured, any\napplication can be run, let it be on local machines, or any cloud computing platform.", 
            "title": "What is Docker Swarm?"
        }, 
        {
            "location": "/showfile/", 
            "text": "", 
            "title": "_Source Browser_"
        }, 
        {
            "location": "/tutorials/", 
            "text": "OMNeT++ Tutorials\n\n\n\n\nTicToc\n. An introductory tutorial that guides you through building and working with\n  an example simulation model.\n\n\nResult Analysis with Python\n. This tutorial will walk you through the initial steps of\n  using Python for analysing simulation results, and shows how to do some of the most common tasks.\n\n\nRunning Simulation Campaigns in the Cloud\n. Concepts, ideas and a solution draft\n  for harnessing the power of computing clouds for running simulation campaigns.\n\n\nRunning INET Simulation Campaigns on AWS\n. Presents a minimal but powerful toolset \n  for running INET simulations on Amazon's cloud platform.", 
            "title": "_Tutorials_"
        }, 
        {
            "location": "/tutorials/#omnet-tutorials", 
            "text": "TicToc . An introductory tutorial that guides you through building and working with\n  an example simulation model.  Result Analysis with Python . This tutorial will walk you through the initial steps of\n  using Python for analysing simulation results, and shows how to do some of the most common tasks.  Running Simulation Campaigns in the Cloud . Concepts, ideas and a solution draft\n  for harnessing the power of computing clouds for running simulation campaigns.  Running INET Simulation Campaigns on AWS . Presents a minimal but powerful toolset \n  for running INET simulations on Amazon's cloud platform.", 
            "title": "OMNeT++ Tutorials"
        }
    ]
}