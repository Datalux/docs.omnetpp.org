{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. When to use Python?\n",
    "\n",
    "\n",
    "The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of\n",
    "simulation results. If you are doing sophisticated result analysis, you will\n",
    "notice after a while that you have outgrown the IDE. The need for customized\n",
    "charts, the necessity of multi-step computations to produce chart input, or the\n",
    "sheer volume of raw simulation results might all be causes to make you look for\n",
    "something else.\n",
    "\n",
    "If you are an R or Matlab expert, you'll probably reach for those tools, but for\n",
    "everyone else, Python with the right libraries is pretty much the best choice.\n",
    "Python has a big momentum for data science, and in addition to having excellent\n",
    "libraries for data analysis and visualization, it is also a great general-purpose\n",
    "programming language. Python is used for diverse problems ranging from building\n",
    "desktop GUIs to machine learning and AI, so the knowledge you gain by learning\n",
    "it will be convertible to other areas.\n",
    "\n",
    "This tutorial will walk you through the initial steps of using Python for\n",
    "analysing simulation results, and shows how to do some of the most common tasks.\n",
    "The tutorial assumes that you have a working knowledge of OMNeT++ with regard\n",
    "to result recording, and basic familiarity with Python.\n",
    "\n",
    "\n",
    "## 2. Setting up\n",
    "\n",
    "\n",
    "Before we can start, you need to install the necessary software.\n",
    "First, make sure you have Python, either version 2.x or 3.x (they are\n",
    "slightly incompatible.) If you have both versions available on your system,\n",
    "we recommend version 3.x. You also need OMNeT++ version 5.2 or later.\n",
    "\n",
    "We will heavily rely on three Python packages: [NumPy](http://www.numpy.org/),\n",
    "[Pandas](http://pandas.pydata.org/), and [Matplotlib](https://matplotlib.org/).\n",
    "There are also optional packages that will be useful for certain tasks:\n",
    "[SciPy](https://www.scipy.org/),\n",
    "[PivotTable.js](https://github.com/nicolaskruchten/pivottable).\n",
    "We also recommend that you install [IPython](https://ipython.org/) and\n",
    "[Jupyter](https://jupyter.org/), because they let you work much more comfortably\n",
    "than the bare Python shell.\n",
    "\n",
    "On most systems, these packages can be installed with `pip`, the Python package\n",
    "manager (if you go for Python 3, replace `pip` with `pip3` in the commands\n",
    "below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    sudo pip install ipython jupyter\n",
    "    sudo pip install numpy pandas matplotlib\n",
    "    sudo pip install scipy pivottablejs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As packages continually evolve, there might be incompatibilities between\n",
    "versions. We used the following versions when writing this tutorial:\n",
    "Pandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0.\n",
    "An easy way to determine which versions you have installed is using the `pip list`\n",
    "command. (Note that the last one is the version of the Python interface library,\n",
    "the PivotTable.js main Javascript library uses different version numbers, e.g.\n",
    "2.7.0.)\n",
    "\n",
    "\n",
    "## 3. Getting your simulation results into Python\n",
    "\n",
    "\n",
    "OMNeT++ result files have their own file format which is not directly\n",
    "digestible by Python. There are a number of ways to get your data\n",
    "inside Python:\n",
    "\n",
    "  1. Export from the IDE. The Analysis Tool can export data in a number of\n",
    "  formats, the ones that are useful here are CSV and Python-flavoured JSON.\n",
    "  In this tutorial we'll use the CSV export, and read the result into Pandas\n",
    "  using its `read_csv()` function.\n",
    "\n",
    "  2. Export using scavetool. Exporting from the IDE may become tedious\n",
    "  after a while, because you have to go through the GUI every time your\n",
    "  simulations are re-run. Luckily, you can automate the exporting with\n",
    "  OMNeT++'s scavetool program. scavetool exposes the same export\n",
    "  functionality as the IDE, and also allows filtering of the data.\n",
    "\n",
    "  3. Read the OMNeT++ result files directly from Python. Development\n",
    "  of a Python package to read these files into Pandas data frames is\n",
    "  underway, but given that these files are line-oriented text files\n",
    "  with a straightforward and well-documented structure, writing your\n",
    "  own custom reader is also a perfectly feasible option.\n",
    "\n",
    "  4. SQLite. Since version 5.1, OMNeT++ has the ability to record simulation\n",
    "  results int SQLite3 database files, which can be opened directly from\n",
    "  Python using the [sqlite](https://docs.python.org/3/library/sqlite3.html)\n",
    "  package. This lets you use SQL queries to select the input data for your\n",
    "  charts or computations, which is kind of cool! You can even use GUIs like\n",
    "  [SQLiteBrowser](http://sqlitebrowser.org/) to browse the database and\n",
    "  craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3\n",
    "  output, you'll still get `.vec` and `.sca` files as before, only their\n",
    "  format will change from textual to SQLite's binary format. When querying\n",
    "  the contents of the files, one issue  to deal with is that SQLite does not\n",
    "  allow cross-database queries, so you either need to configure OMNeT++\n",
    "  to record everything into one file (i.e. each run should append instead\n",
    "  of creating a new file), or use scavetool's export functionality to\n",
    "  merge the files into one.\n",
    "\n",
    "  5. Custom result recording. There is also the option to instrument\n",
    "  the simulation (via C++ code) or OMNeT++ (via custom result recorders)\n",
    "  to produce files that Python can directly digest, e.g. CSV.\n",
    "  However, in the light of the above options, it is rarely necessary\n",
    "  to go this far.\n",
    "\n",
    "With large-scale simulation studies, it can easily happen that the\n",
    "full set of simulation results do not fit into the memory at once.\n",
    "There are also multiple approaches to deal with this problem:\n",
    "\n",
    "  1. If you don't need all simulation results for the analysis, you can\n",
    "  configure OMNeT++ to record only a subset of them. Fine-grained control\n",
    "  is available.\n",
    "  2. Perform filtering and aggregation steps before analysis. The IDE and\n",
    "  scavetool are both capable of filtering the results before export.\n",
    "  3. When the above approaches are not enough, it can help to move\n",
    "  part of the result processing (typically, filtering and aggregation)\n",
    "  into the simulation model as dedicated result collection modules.\n",
    "  However, this solution requires significantly more work than the previous\n",
    "  two, so use with care.\n",
    "\n",
    "In this tutorial, we'll work with the contents of the `samples/resultfiles`\n",
    "directory distributed with OMNeT++. The directory contains result\n",
    "files produced by the Aloha and Routing sample simulations, both\n",
    "of which are parameter studies. We'll start by looking at the Aloha results.\n",
    "\n",
    "As the first step, we use OMNeT++'s *scavetool* to convert Aloha's scalar files\n",
    "to CSV. Run the following commands in the terminal (replace `~/omnetpp` with\n",
    "the location of your OMNeT++ installation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    cd ~/omnetpp/samples/resultfiles/aloha\n",
    "    scavetool x *.sca -o aloha.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scavetool command line, `x` means export, and the export format is\n",
    "inferred from the output file's extension. (Note that scavetool supports\n",
    "two different CSV output formats. We need *CSV Records*, or CSV-R for short,\n",
    "which is the default for the `.csv` extension.)\n",
    "\n",
    "Let us spend a minute on what the export has created. The CSV file\n",
    "has a fixed number of columns named `run`, `type`, `module`, `name`,\n",
    "`value`, etc. Each result item, i.e. scalar, statistic, histogram\n",
    "and vector, produces one row of output in the CSV. Other items such\n",
    "as run attributes, iteration variables of the parameter study and result\n",
    "attributes also generate their own rows. The content of the `type` column\n",
    "determines what type of information a given row contains. The `type`\n",
    "column also determines which other columns are in use. For example,\n",
    "the `binedges` and `binvalues` columns are only filled in for histogram\n",
    "items. The colums are:\n",
    "\n",
    "- *run*: Identifies the simulation run\n",
    "- *type*: Row type, one of the following: `scalar`, `vector`, `statistics`,\n",
    "  `histogram`, `runattr`, `itervar`, `param`, `attr`\n",
    "- *module*: Hierarchical name (a.k.a. full path) of the module that recorded the\n",
    "  result item\n",
    "- *name*: Name of the result item (scalar, statistic, histogram or vector)\n",
    "- *attrname*: Name of the run attribute or result item attribute (in the latter\n",
    "  case, the `module` and `name` columns identify the result item the attribute\n",
    "  belongs to)\n",
    "- *attrvalue*: Value of run and result item attributes, iteration variables,\n",
    "  saved ini param settings (`runattr`, `attr`, `itervar`, `param`)\n",
    "- *value*: Output scalar value\n",
    "- *count*, *sumweights*, *mean*, *min*, *max*, *stddev*: Fields of the statistics\n",
    "  or histogram\n",
    "- *binedges*, *binvalues*: Histogram bin edges and bin values, as space-separated\n",
    "  lists. *len(binedges)==len(binvalues)+1*\n",
    "- *vectime*, *vecvalue*: Output vector time and value arrays, as space-separated\n",
    "  lists\n",
    "\n",
    "When the export is done, you can start Jupyter server with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a web browser with the displayed URL to access the Jupyter GUI. Once there,\n",
    "choose *New* -> *Python3* in the top right corner to open a blank notebook.\n",
    "The notebook allows you to enter Python commands or sequences of commands,\n",
    "run them, and view the output. Note that *Enter* simply inserts a newline;\n",
    "hit *Ctrl+Enter* to execute the commands in the current cell, or *Alt+Enter*\n",
    "to execute them and also insert a new cell below.\n",
    "\n",
    "If you cannot use Jupyter for some reason, a terminal-based Python shell\n",
    "(`python` or `ipython`) will also allow you to follow the tutorial.\n",
    "\n",
    "On the Python prompt, enter the following lines to make the functionality of\n",
    "Pandas, NumpPy and Matplotlib available in the session. The last, `%matplotlib`\n",
    "line is only needed for Jupyter. (It is a \"magic command\" that arranges plots\n",
    "to be displayed within the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize the `read_csv()` function to import the contents of the\n",
    "CSV file into a data frame. The data frame is the central concept of\n",
    "Pandas. We will continue to work with this data frame throughout\n",
    "the whole tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha = pd.read_csv('aloha.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring the data frame\n",
    "\n",
    "\n",
    "You can view the contents of the data frame by simply entering the name\n",
    "of the variable (`aloha`). Alternatively, you can use the `head()` method\n",
    "of the data frame to view just the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>type</th>\n",
       "      <th>module</th>\n",
       "      <th>name</th>\n",
       "      <th>attrname</th>\n",
       "      <th>attrvalue</th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "      <th>sumweights</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>binedges</th>\n",
       "      <th>binvalues</th>\n",
       "      <th>vectime</th>\n",
       "      <th>vecvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PureAlohaExperiment-4-20170627-20:42:20-22739</td>\n",
       "      <td>runattr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>configname</td>\n",
       "      <td>PureAlohaExperiment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PureAlohaExperiment-4-20170627-20:42:20-22739</td>\n",
       "      <td>runattr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>datetime</td>\n",
       "      <td>20170627-20:42:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PureAlohaExperiment-4-20170627-20:42:20-22739</td>\n",
       "      <td>runattr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>experiment</td>\n",
       "      <td>PureAlohaExperiment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PureAlohaExperiment-4-20170627-20:42:20-22739</td>\n",
       "      <td>runattr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inifile</td>\n",
       "      <td>omnetpp.ini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PureAlohaExperiment-4-20170627-20:42:20-22739</td>\n",
       "      <td>runattr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iterationvars</td>\n",
       "      <td>numHosts=10, iaMean=3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             run     type module name  \\\n",
       "0  PureAlohaExperiment-4-20170627-20:42:20-22739  runattr    NaN  NaN   \n",
       "1  PureAlohaExperiment-4-20170627-20:42:20-22739  runattr    NaN  NaN   \n",
       "2  PureAlohaExperiment-4-20170627-20:42:20-22739  runattr    NaN  NaN   \n",
       "3  PureAlohaExperiment-4-20170627-20:42:20-22739  runattr    NaN  NaN   \n",
       "4  PureAlohaExperiment-4-20170627-20:42:20-22739  runattr    NaN  NaN   \n",
       "\n",
       "        attrname              attrvalue  value  count  sumweights  mean  \\\n",
       "0     configname    PureAlohaExperiment    NaN    NaN         NaN   NaN   \n",
       "1       datetime      20170627-20:42:20    NaN    NaN         NaN   NaN   \n",
       "2     experiment    PureAlohaExperiment    NaN    NaN         NaN   NaN   \n",
       "3        inifile            omnetpp.ini    NaN    NaN         NaN   NaN   \n",
       "4  iterationvars  numHosts=10, iaMean=3    NaN    NaN         NaN   NaN   \n",
       "\n",
       "   stddev  min  max binedges binvalues vectime vecvalue  \n",
       "0     NaN  NaN  NaN      NaN       NaN     NaN      NaN  \n",
       "1     NaN  NaN  NaN      NaN       NaN     NaN      NaN  \n",
       "2     NaN  NaN  NaN      NaN       NaN     NaN      NaN  \n",
       "3     NaN  NaN  NaN      NaN       NaN     NaN      NaN  \n",
       "4     NaN  NaN  NaN      NaN       NaN     NaN      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the structure of the data frame, i.e. rows and columns,\n",
    "directly corresponds to the contents of the CSV file. Column names have\n",
    "been taken from the first line of the CSV file. Missing values are\n",
    "represented with NaNs (not-a-number).\n",
    "\n",
    "The complementary `tail()` method shows the last few lines. There is also\n",
    "an `iloc` method that we use at places in this tutorial to show rows\n",
    "from the middle of the data frame. It accepts a range: `aloha.iloc[20:30]`\n",
    "selects 10 lines from line 20, `aloha.iloc[:5]` is like `head()`, and\n",
    "`aloha.iloc[-5:]` is like `tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>type</th>\n",
       "      <th>module</th>\n",
       "      <th>name</th>\n",
       "      <th>attrname</th>\n",
       "      <th>attrvalue</th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "      <th>sumweights</th>\n",
       "      <th>mean</th>\n",
       "      <th>stddev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>binedges</th>\n",
       "      <th>binvalues</th>\n",
       "      <th>vectime</th>\n",
       "      <th>vecvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>PureAlohaExperiment-1-20170627-20:42:17-22739</td>\n",
       "      <td>scalar</td>\n",
       "      <td>Aloha.server</td>\n",
       "      <td>collidedFrames:last</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40692.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>PureAlohaExperiment-1-20170627-20:42:17-22739</td>\n",
       "      <td>attr</td>\n",
       "      <td>Aloha.server</td>\n",
       "      <td>collidedFrames:last</td>\n",
       "      <td>source</td>\n",
       "      <td>sum(collision)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>PureAlohaExperiment-1-20170627-20:42:17-22739</td>\n",
       "      <td>attr</td>\n",
       "      <td>Aloha.server</td>\n",
       "      <td>collidedFrames:last</td>\n",
       "      <td>title</td>\n",
       "      <td>collided frames, last</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>PureAlohaExperiment-1-20170627-20:42:17-22739</td>\n",
       "      <td>scalar</td>\n",
       "      <td>Aloha.server</td>\n",
       "      <td>channelUtilization:last</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>PureAlohaExperiment-1-20170627-20:42:17-22739</td>\n",
       "      <td>attr</td>\n",
       "      <td>Aloha.server</td>\n",
       "      <td>channelUtilization:last</td>\n",
       "      <td>interpolationmode</td>\n",
       "      <td>linear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                run    type        module  \\\n",
       "1200  PureAlohaExperiment-1-20170627-20:42:17-22739  scalar  Aloha.server   \n",
       "1201  PureAlohaExperiment-1-20170627-20:42:17-22739    attr  Aloha.server   \n",
       "1202  PureAlohaExperiment-1-20170627-20:42:17-22739    attr  Aloha.server   \n",
       "1203  PureAlohaExperiment-1-20170627-20:42:17-22739  scalar  Aloha.server   \n",
       "1204  PureAlohaExperiment-1-20170627-20:42:17-22739    attr  Aloha.server   \n",
       "\n",
       "                         name           attrname              attrvalue  \\\n",
       "1200      collidedFrames:last                NaN                    NaN   \n",
       "1201      collidedFrames:last             source         sum(collision)   \n",
       "1202      collidedFrames:last              title  collided frames, last   \n",
       "1203  channelUtilization:last                NaN                    NaN   \n",
       "1204  channelUtilization:last  interpolationmode                 linear   \n",
       "\n",
       "             value  count  sumweights  mean  stddev  min  max binedges  \\\n",
       "1200  40692.000000    NaN         NaN   NaN     NaN  NaN  NaN      NaN   \n",
       "1201           NaN    NaN         NaN   NaN     NaN  NaN  NaN      NaN   \n",
       "1202           NaN    NaN         NaN   NaN     NaN  NaN  NaN      NaN   \n",
       "1203      0.156176    NaN         NaN   NaN     NaN  NaN  NaN      NaN   \n",
       "1204           NaN    NaN         NaN   NaN     NaN  NaN  NaN      NaN   \n",
       "\n",
       "     binvalues vectime vecvalue  \n",
       "1200       NaN     NaN      NaN  \n",
       "1201       NaN     NaN      NaN  \n",
       "1202       NaN     NaN      NaN  \n",
       "1203       NaN     NaN      NaN  \n",
       "1204       NaN     NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha.iloc[1200:1205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: If you are in the terminal and you find that the data frame printout does\n",
    "not make use of the whole width of the terminal, you can increase the display\n",
    "width for better readability with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 180)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not looked at any Pandas tutorial yet, now is a very good\n",
    "time to read one. (See References at the bottom of this page for hints.)\n",
    "Until you finish, here are some basics for your short-term survival.\n",
    "\n",
    "You can refer to a column as a whole with the array index syntax: `aloha['run']`.\n",
    "Alternatively, the more convenient member access syntax (`aloha.run`) can\n",
    "also be used, with restrictions. (E.g. the column name must be valid as a Python\n",
    "identifier, and should not collide with existing methods of the data frame.\n",
    "Names that are known to cause trouble include `name`, `min`, `max`, `mean`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.run.head()  # .head() is for limiting the output to 5 lines here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting multiple columns is also possible, one just needs to use a list of\n",
    "column names as index. The result will be another data frame. (The double\n",
    "brackets in the command are due to the fact that both the array indexing and\n",
    "the list syntax use square brackets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = aloha[['run', 'attrname', 'attrvalue']]\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method can be used to get an idea about the contents of a\n",
    "column. When applied to a non-numeric column, it prints the number of\n",
    "non-null elements in it (`count`), the number of unique values (`unique`),\n",
    "the most frequently occurring value (`top`) and its multiplicity (`freq`),\n",
    "and the inferred data type (more about that later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.module.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of the unique values using the `unique()` method. For example,\n",
    "the following command lists the names of modules that have recorded any statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.module.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you apply `describe()` to a numeric column, you get a statistical summary\n",
    "with things like mean, standard deviation, minimum, maximum, and various\n",
    "quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.value.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying `describe()` to the whole data frame creates a similar report about\n",
    "all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's spend a minute on data types and column data types. Every column has a\n",
    "data type (abbreviated *dtype*) that determines what type of values it may\n",
    "contain. Column dtypes can be printed with `dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most commonly used dtypes are *float64* and *object*. A *float64* column\n",
    "contains floating-point numbers, and missing values are represented with NaNs.\n",
    "An *object* column may contain basically anything -- usually strings, but we'll\n",
    "also have NumPy arrays (`np.ndarray`) as elements in this tutorial.\n",
    "Numeric values and booleans may also occur in an *object* column. Missing values\n",
    "in an *object* column are usually represented with `None`, but Pandas also\n",
    "interprets the floating-point NaN like that.\n",
    "Some degree of confusion arises from fact that some Pandas functions check\n",
    "the column's dtype, while others are already happy if the contained elements\n",
    "are of the required type. To clarify: applying `describe()` to a column\n",
    "prints a type inferred from the individual elements, *not* the column dtype.\n",
    "The column dtype type can be changed with the `astype()` method; we'll see an\n",
    "example for using it later in this tutorial.\n",
    "\n",
    "The column dtype can be accessed as the `dtype` property of a column, for example\n",
    "`aloha.stddev.dtype` yields `dtype('float64')`. There are also convenience\n",
    "functions such as `is_numeric_dtype()` and `is_string_dtype()` for checking\n",
    "column dtype. (They need to be imported from the `pandas.api.types` package\n",
    "though.)\n",
    "\n",
    "Another vital thing to know, especially due of the existence of the *type*\n",
    "column in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly,\n",
    "the array index syntax can be used here as well. For example, the following expression\n",
    "selects the rows that contain iteration variables: `aloha[aloha.type == 'itervar']`.\n",
    "With a healthy degree of sloppiness, here's how it works: `aloha.type` yields\n",
    "the values in the `type` column as an array-like data structure;\n",
    "`aloha.type=='itervar'` performs element-wise comparison and produces an array\n",
    "of booleans containing `True` where the condition holds and `False` where not;\n",
    "and indexing a data frame with an array of booleans returns the rows that\n",
    "correspond to `True` values in the array.\n",
    "\n",
    "Conditions can be combined with AND/OR using the \"`&`\" and \"`|`\" operators, but\n",
    "you need parentheses because of operator precedence. The following command\n",
    "selects the rows that contain scalars with a certain name and owner module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = aloha[(aloha.type=='scalar') & (aloha.module=='Aloha.server') & (aloha.name=='channelUtilization:last')]\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need to know how to add a new column to the data frame. Now that is\n",
    "a bit controversial topic, because at the time of writing, there is a \"convenient\"\n",
    "syntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple\n",
    "assignment, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha['qname'] = aloha.module + \".\" + aloha.name\n",
    "aloha[aloha.type=='scalar'].head()  # print excerpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks nice and natural, but it is not entirely correct. It often results in\n",
    "a warning: *SettingWithCopyWarning: A value is trying to be set on a copy of a\n",
    "slice from a DataFrame...*. The message essentially says that the operation\n",
    "(here, adding the new column) might have been applied to a temporary object\n",
    "instead of the original data frame, and thus might have been ineffective.\n",
    "Luckily, that is not the case most of the time (the operation *does* take\n",
    "effect). Nevertheless, for production code, i.e. scripts, the \"official\"\n",
    "solution, the `assign()` method of the data frame is recommended, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha = aloha.assign(qname = aloha.module + \".\" + aloha.name)\n",
    "aloha[aloha.type=='scalar'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, one can remove a column from a data frame using either the\n",
    "`del` operator or the `drop()` method of the data frame. Here we show the former\n",
    "(also to remove the column we added above, as we won't need it for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aloha['qname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Revisiting CSV loading\n",
    "\n",
    "\n",
    "The way we have read the CSV file has one small deficiency: all data in the\n",
    "`attrvalue` column are represented as strings, event though many of them\n",
    "are really numbers, for example the values of the `iaMean` and `numHosts`\n",
    "iteration variables. You can verify that by printing the unique values (\n",
    "`aloha.attrvalue.unique()` -- it will print all values with quotes), or using\n",
    "the `type()` operator on an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( aloha[aloha.type=='scalar'].iloc[0].value )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that `read_csv()` infers data types of columns from the data\n",
    "it finds in them. Since the `attrvalue` column is shared by run attributes,\n",
    "result item attributes, iteration variables and some other types of rows,\n",
    "there are many non-numeric strings in it, and `read_csv()` decides that it is\n",
    "a string column.\n",
    "\n",
    "A similar issue arises with the `binedges`, `binvalues`, `vectime`, `vecvalue`\n",
    "columns. These columns contain lists of numbers separated by spaces, so they\n",
    "are read into strings as well. However, we would like to store them as NumPy\n",
    "arrays (`ndarray`) inside the data frame, because that's the form we can use\n",
    "in plots or as computation input.\n",
    "\n",
    "Luckily, `read_csv()` allows us to specify conversion functions for each column.\n",
    "So, armed with the following two short functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_if_number(s):\n",
    "    try: return float(s)\n",
    "    except: return True if s==\"true\" else False if s==\"false\" else s if s else None\n",
    "\n",
    "def parse_ndarray(s):\n",
    "    return np.fromstring(s, sep=' ') if s else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can read the CSV file again, this time with the correct conversions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha = pd.read_csv('aloha.csv', converters = {\n",
    "    'attrvalue': parse_if_number,\n",
    "    'binedges': parse_ndarray,\n",
    "    'binvalues': parse_ndarray,\n",
    "    'vectime': parse_ndarray,\n",
    "    'vecvalue': parse_ndarray})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the result e.g. by printing the unique values again.\n",
    "\n",
    "\n",
    "## 6. Load-time filtering\n",
    "\n",
    "\n",
    "If the CSV file is large, you may want to skip certain columns or rows when\n",
    "reading it into memory. (File size is about the only valid reason for using\n",
    "load-time filtering, because you can also filter out or drop rows/columns\n",
    "from the data frame when it is already loaded.)\n",
    "\n",
    "To filter out columns, you need to specify in the `usecols` parameter\n",
    "the list of columns to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_csv('aloha.csv', usecols=['run', 'type', 'module', 'name', 'value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no such direct support for filtering out rows based on their content,\n",
    "but we can implement it using the iterator API that reads the CSV file\n",
    "in chunks. We can filter each chunk before storing and finally concatenating\n",
    "them into a single data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = pd.read_csv('aloha.csv', iterator=True, chunksize=100)\n",
    "chunks = [ chunk[chunk['type']!='histogram'] for chunk in iter ]  # discards type=='histogram' lines\n",
    "tmp = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting scalars\n",
    "\n",
    "\n",
    "Scalars can serve as input for many different kinds of plots. Here we'll show\n",
    "how one can create a \"throughput versus offered load\" type plot. We will plot\n",
    "the channel utilization in the Aloha model in the function of the packet\n",
    "generation frequency. Channel utilization is also affected by the number of\n",
    "hosts in the network -- we want results belonging to the same number of hosts\n",
    "to form iso lines. Packet generation frequency and the number of hosts are\n",
    "present in the results as iteration variables named `iaMean` and `numHosts`;\n",
    "channel utilization values are the `channelUtilization:last` scalars saved\n",
    "by the `Aloha.server` module. The data contains the results from two simulation\n",
    "runs for each *(iaMean, numHosts)* pair done with different seeds; we want\n",
    "to average them for the plot.\n",
    "\n",
    "The first few steps are fairly straightforward. We only need the scalars and the\n",
    "iteration variables from the data frame, so we filter out the rest. Then we\n",
    "create a `qname` column from other columns to hold the names of our variables:\n",
    "the names of scalars are in the `module` and `name` columns (we want to join them\n",
    "with a dot), and the names of iteration variables are in the `attrname` column.\n",
    "Since `attrname` is not filled in for scalar rows, we can take `attrname` as`qname`\n",
    "first, then fill in the holes with *module.name*. We use the `combine_first()`\n",
    "method for that: `a.combine_first(b)` fills the holes in `a` using the\n",
    "corresponding values from `b`.\n",
    "\n",
    "The similar issue arises with values: values of output scalars are in the `value`\n",
    "column, while that of iteration variables are in the `attrvalue` column.\n",
    "Since `attrvalue` is unfilled for scalar rows, we can again utilize\n",
    "`combine_first()` to merge two. There is one more catch: we need to change\n",
    "the dtype of the `attrvalue` to `float64`, otherwise the resulting `value`\n",
    "column also becomes `object` dtype. (Luckily, all our iteration variables are\n",
    "numeric, so the dtype conversion is possible. In other simulations that contain\n",
    "non-numeric itervars, one needs to filter those out, force them into numeric\n",
    "values somehow, or find some other trick to make things work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = aloha[(aloha.type=='scalar') | (aloha.type=='itervar')]  # filter rows\n",
    "scalars = scalars.assign(qname = scalars.attrname.combine_first(scalars.module + '.' + scalars.name))  # add qname column\n",
    "scalars.value = scalars.value.combine_first(scalars.attrvalue.astype('float64'))  # merge value columns\n",
    "scalars[['run', 'type', 'qname', 'value', 'module', 'name', 'attrname']].iloc[80:90]  # print an excerpt of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work further, it would be very convenient if we had a format where each\n",
    "simulation run corresponds to one row, and all variables produced by that\n",
    "run had their own columns. We can call it the *wide* format, and it can be\n",
    "produced using the `pivot()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_wide = scalars.pivot('run', columns='qname', values='value')\n",
    "scalars_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in only three columns for our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars_wide[['numHosts', 'iaMean', 'Aloha.server.channelUtilization:last']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have our *x* and *y* data in separate columns now, we can utilize the\n",
    "scatter plot feature of the data frame for plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the default image resolution and size\n",
    "plt.rcParams['figure.figsize'] = [8.0, 3.0]\n",
    "plt.rcParams['figure.dpi'] = 144\n",
    "# create a scatter plot\n",
    "scalars_wide.plot.scatter('iaMean', 'Aloha.server.channelUtilization:last')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Although `plt.show()` is not needed in Jupyter (`%matplotlib inline`\n",
    "turns on immediate display), we'll continue to include it in further code\n",
    "fragments, so that they work without change when you use another Python shell.\n",
    "\n",
    "The resulting chart looks quite good as the first attempt. However, it has some\n",
    "shortcomings:\n",
    "\n",
    "- Dots are not connected. The dots that have the same `numHosts` value should\n",
    "  be connected with iso lines.\n",
    "- As the result of having two simulation runs for each *(iaMean,numHosts)* pair,\n",
    "  the dots appear in pairs. We'd like to see their averages instead.\n",
    "\n",
    "Unfortunately, scatter plot can only take us this far, we need to look for\n",
    "another way.\n",
    "\n",
    "What we really need as chart input is a table where rows correspond to different\n",
    "`iaMean` values, columns correspond to different `numHosts` values, and cells\n",
    "contain channel utilization values (the average of the repetitions).\n",
    "Such table can be produced from the \"wide format\" with another pivoting\n",
    "operation. We use `pivot_table()`, a cousin of the `pivot()` method we've seen above.\n",
    "The difference between them is that `pivot()` is a reshaping operation (it just\n",
    "rearranges elements), while `pivot_table()` is more of a spreadsheet-style\n",
    "pivot table creation operation, and primarily intended for numerical data.\n",
    "`pivot_table()` accepts an aggregation function with the default being *mean*,\n",
    "which is quite convenient for us now (we want to average channel utilization\n",
    "over repetitions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha_pivot = scalars_wide.pivot_table(index='iaMean', columns='numHosts', values='Aloha.server.channelUtilization:last')  # note: aggregation function = mean (that's the default)\n",
    "aloha_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that rows correspond to various `iaMean` values (`iaMean` serves as index);\n",
    "there is one column for each value of `numHosts`; and that data in the table\n",
    "are the averages of the channel utilizations produced by the simulations\n",
    "performed with the respective `iaMean` and `numHosts` values.\n",
    "\n",
    "For the plot, every column should generate a separate line (with the *x* values\n",
    "coming from the index column, `iaMean`) labelled with the column name.\n",
    "The basic Matplotlib interface cannot create such plot in one step. However,\n",
    "the Pandas data frame itself has a plotting interface which knows how to\n",
    "interpret the data, and produces the correct plot without much convincing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha_pivot.plot.line()\n",
    "plt.ylabel('channel utilization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive pivot tables\n",
    "\n",
    "\n",
    "Getting the pivot table right is not always easy, so having a GUI where\n",
    "one can drag columns around and immediately see the result is definitely\n",
    "a blessing. Pivottable.js presents such a GUI inside a browser, and\n",
    "although the bulk of the code is Javascript, it has a Python frond-end\n",
    "that integrates nicely with Jupyter. Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pivottablejs as pj\n",
    "pj.pivot_ui(scalars_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interactive panel containing the pivot table will appear. Here is how\n",
    "you can reproduce the above \"Channel utilization vs iaMean\" plot in it:\n",
    "\n",
    "1. Drag `numHosts` to the \"rows\" area of the pivot table.\n",
    "   The table itself is the area on the left that initially only displays \"Totals | 42\",\n",
    "   and the \"rows\" area is the empty rectangle directly of left it.\n",
    "   The table should show have two columns (*numHosts* and *Totals*) and\n",
    "   five rows in total after dragging.\n",
    "2. Drag `iaMean` to the \"columns\" area (above the table). Columns for each value\n",
    "   of `iaMean` should appear in the table.\n",
    "3. Near the top-left corner of the table, select *Average* from the combo box\n",
    "   that originally displays *Count*, and select `ChannelUtilization:last`\n",
    "   from the combo box that appears below it.\n",
    "4. In the top-left corner of the panel, select *Line Chart* from the combo box\n",
    "   that originally displays *Table*.\n",
    "\n",
    "If you can't get to see it, the following command will programmatically\n",
    "configure the pivot table in the appropriate way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj.pivot_ui(scalars_wide, rows=['numHosts'], cols=['iaMean'], vals=['Aloha.server.channelUtilization:last'], aggregatorName='Average', rendererName='Line Chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want experiment with Excel's or LibreOffice's built-in pivot table\n",
    "functionality, the data frame's `to_clipboard()` and `to_csv()` methods\n",
    "will help you transfer the data. For example, you can issue the\n",
    "`scalars_wide.to_clipboard()` command to put the data on the clipboard, then\n",
    "paste it into the spreadsheet. Alternatively, type `print(scalars_wide.to_csv())`\n",
    "to print the data in CSV format that you can select and then copy/paste.\n",
    "Or, use `scalars_wide.to_csv(\"scalars.csv\")` to save the data into a file\n",
    "which you can import.\n",
    "\n",
    "\n",
    "## 9. Plotting histograms\n",
    "\n",
    "\n",
    "In this section we explore how to plot histograms recorded by the simulation.\n",
    "Histograms are in rows that have `\"histogram\"` in the `type` column.\n",
    "Histogram bin edges and bin values (counts) are in the `binedges` and\n",
    "`binvalues` columns as NumPy array objects (`ndarray`).\n",
    "\n",
    "Let us begin by selecting the histograms into a new data frame for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = aloha[aloha.type=='histogram']\n",
    "len(histograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 84 histograms. It makes no sense to plot so many histograms on one chart,\n",
    "so let's just take one on them, and examine its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = histograms.iloc[0]  # the first histogram\n",
    "hist.binedges, hist.binvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to plot the histogram from these two arrays is to look at it\n",
    "as a step function, and create a line plot with the appropriate drawing style.\n",
    "The only caveat is that we need to add an extra `0` element to draw the right\n",
    "side of the last histogram bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.binedges, np.append(hist.binvalues, 0), drawstyle='steps-post')   # or maybe steps-mid, for integers\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to plot a recorded histogram is Matplotlib's `hist()` method,\n",
    "although that is a bit tricky. Instead of taking histogram data, `hist()`\n",
    "insists on computing the histogram itself from an array of values -- but we only\n",
    "have the histogram, and not the data it was originally computed from.\n",
    "Fortunately, `hist()` can accept a bin edges array, and another array as weights\n",
    "for the values. Thus, we can trick it into doing what we want by passing\n",
    "in our `binedges` array twice, once as bin edges and once as values, and\n",
    "specifying `binvalues` as weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bins=hist.binedges, x=hist.binedges[:-1], weights=hist.binvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hist()` has some interesting options. For example, we can change the plotting\n",
    "style to be similar to a line plot by setting `histtype='step'`. To plot the\n",
    "normalized version of the histogram, specify `normed=True` or `density=True`\n",
    "(they work differently; see the Matplotlib documentation for details).\n",
    "To draw the cumulative density function, also specify `cumulative=True`.\n",
    "The following plot shows the effect of some of these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bins=hist.binedges, x=hist.binedges[:-1], weights=hist.binvalues, histtype='step', normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot several histograms, we can iterate over the histograms and draw them\n",
    "one by one on the same plot. The following code does that, and also adds a\n",
    "legend and adjusts the bounds of the x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5]\n",
    "for row in somehistograms.itertuples():\n",
    "    plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post')\n",
    "plt.legend(somehistograms.module + \".\" + somehistograms.name)\n",
    "plt.xlim(0, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, that the legend contains the same string for all histograms,\n",
    "which is not very meaningful. We could improve that by including some\n",
    "characteristics of the simulation that generated them, i.e. the number of hosts\n",
    "(`numHosts` iteration variable) and frame interarrival times (`iaTime` iteration\n",
    "variable). We'll see in the next section how that can be achieved.\n",
    "\n",
    "\n",
    "## 10. Adding iteration variables as columns\n",
    "\n",
    "\n",
    "In this step, we add the iteration variables associated with the simulation\n",
    "run to the data frame as columns. There are several reasons why this is a\n",
    "good idea: they are very useful for generating the legends for plots of\n",
    "e.g. histograms and vectors (e.g. \"collision multiplicity histogram for\n",
    "numHosts=20 and iaMean=2s\"), and often needed as chart input as well.\n",
    "\n",
    "First, we select the iteration variables vars as a smaller data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itervars_df = aloha.loc[aloha.type=='itervar', ['run', 'attrname', 'attrvalue']]\n",
    "itervars_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the result by using the `pivot()` method. The following statement\n",
    "will convert unique values in the `attrname` column into separate columns:\n",
    "`iaMean` and `numHosts`. The new data frame will be indexed with the run id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itervarspivot_df = itervars_df.pivot(index='run', columns='attrname', values='attrvalue')\n",
    "itervarspivot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to add the new columns back into the original dataframe, using\n",
    "`merge()`. This operation is not quite unlike an SQL join of two tables on the\n",
    "`run` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha2 = aloha.merge(itervarspivot_df, left_on='run', right_index=True, how='outer')\n",
    "aloha2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plot legends, it is also useful to have a single `iterationvars` column with\n",
    "string values like `numHosts=10, iaMean=2`. This is easier than the above: we\n",
    "can just select the rows containing the run attribute named `iterationvars`\n",
    "(it contains exactly the string we need), take only the `run` and `attrvalue`\n",
    "columns, rename the `attrvalue` column to `iterationvars`, and then merge back the\n",
    "result into the original data frame in a way we did above.\n",
    "\n",
    "The selection and renaming step can be done as follows. (Note: we need\n",
    "`.astype(str)` in the condition so that rows where `attrname` is not filled in\n",
    "do not cause trouble.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itervarscol_df = aloha.loc[(aloha.type=='runattr') & (aloha.attrname.astype(str)=='iterationvars'), ['run', 'attrvalue']]\n",
    "itervarscol_df = itervarscol_df.rename(columns={'attrvalue': 'iterationvars'})\n",
    "itervarscol_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the merging step, we join the two tables (I mean, data frames) on the `run`\n",
    "column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha3 = aloha2.merge(itervarscol_df, left_on='run', right_on='run', how='outer')\n",
    "aloha3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the result of our work, let's try plotting the same histograms again,\n",
    "this time with a proper legend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms = aloha3[aloha3.type=='histogram']\n",
    "somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5]\n",
    "for row in somehistograms.itertuples():\n",
    "    plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post')\n",
    "plt.title('collisionLength:histogram')\n",
    "plt.legend(somehistograms.iterationvars)\n",
    "plt.xlim(0, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plotting vectors\n",
    "\n",
    "\n",
    "This section deals with basic plotting of output vectors. Output vectors\n",
    "are basically time series data, but values have timestamps instead\n",
    "of being evenly spaced. Vectors are in rows that have `\"vector\"`\n",
    "in the `type` column. The values and their timestamps are in the\n",
    "`vecvalue` and `vectime` columns as NumPy array objects (`ndarray`).\n",
    "\n",
    "We'll use a different data set for exploring output vector plotting, one from\n",
    "the *routing* example simulation. There are pre-recorded result files in the\n",
    "`samples/resultfiles/routing` directory; change into it in the terminal, and\n",
    "issue the following command to convert them to CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    scavetool x *.sca *.vec -o routing.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read the the CSV file into a data frame in the same way we saw with the\n",
    "*aloha* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing = pd.read_csv('routing.csv', converters = {\n",
    "    'attrvalue': parse_if_number,\n",
    "    'binedges': parse_ndarray,\n",
    "    'binvalues': parse_ndarray,\n",
    "    'vectime': parse_ndarray,\n",
    "    'vecvalue': parse_ndarray})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by selecting the vectors into a new data frame for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = routing[routing.type=='vector']\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data frame contains results from one run. To get some idea what vectors\n",
    "we have, let's print the list unique vector names and module names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.name.unique(), vectors.module.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vector can be plotted on a line chart by simply passing the `vectime` and\n",
    "`vecvalue` arrays to `plt.plot()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectors[vectors.name == 'qlen:vector'].iloc[4]  # take some vector\n",
    "plt.plot(vec.vectime, vec.vecvalue, drawstyle='steps-post')\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When several vectors need to be placed on the same plot, one can simply\n",
    "use a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somevectors = vectors[vectors.name == 'qlen:vector'][:5]\n",
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime, row.vecvalue, drawstyle='steps-post')\n",
    "plt.title(somevectors.name.values[0])\n",
    "plt.legend(somevectors.module)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Vector Filtering\n",
    "\n",
    "\n",
    "Plotting vectors \"as is\" is often not practical, as the result will be a crowded\n",
    "plot that's difficult to draw conclusions from. To remedy that, one can apply\n",
    "some kind of filtering before plotting, or plot a derived quantity such as the\n",
    "integral, sum or running average instead of the original. Such things can easily\n",
    "be achieved with the help of NumPy.\n",
    "\n",
    "Vector time and value are already stored in the data frame as NumPy arrays\n",
    "(`ndarray`), so we can apply NumPy functions to them. For example, let's\n",
    "try `np.cumsum()` which computes cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([8, 2, 1, 5, 7])\n",
    "np.cumsum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime, np.cumsum(row.vecvalue))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting cumulative sum against time might be useful e.g. for an output\n",
    "vector where the simulation emits the packet length for each packet\n",
    "that has arrived at its destination. There, the sum would represent\n",
    "\"total bytes received\".\n",
    "\n",
    "Plotting the count against time for the same output vector would\n",
    "represent \"number of packets received\". For such a plot, we can utilize\n",
    "`np.arange(1,n)` which simply returns the numbers 1, 2, .., n-1\n",
    "as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime, np.arange(1, row.vecvalue.size+1), '.-', drawstyle='steps-post')\n",
    "plt.xlim(0,5); plt.ylim(0,20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we changed the plotting style to \"steps-post\", so\n",
    "that for any *t* time the plot accurately represents the number\n",
    "of values whose timestamp is less than or equal to *t*.\n",
    "\n",
    "As another warm-up exercise, let's plot the time interval\n",
    "that elapses between adjacent values; that is, for each element\n",
    "we want to plot the time difference between the that element\n",
    "and the previous one.\n",
    "This can be achieved by computing `t[1:] - t[:-1]`, which is the\n",
    "elementwise subtraction of the `t` array and its shifted version.\n",
    "Array indexing starts at 0, so `t[1:]` means \"drop the first element\".\n",
    "Negative indices count from the end of the array, so `t[:-1]` means\n",
    "\"without the last element\". The latter is necessary because the\n",
    "sizes of the two arrays must match. or convenience, we encapsulate\n",
    "the formula into a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(t):\n",
    "    return t[1:] - t[:-1]\n",
    "\n",
    "# example\n",
    "t = np.array([0.1, 1.5, 1.6, 2.0, 3.1])\n",
    "diff(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot it. Note that as `diff()` makes the array one element\n",
    "shorter, we need to write `row.vectime[1:]` to drop the first element\n",
    "(it has no preceding element, so `diff()` cannot be computed for it.)\n",
    "Also, we use dots for plotting instead of lines, as it makes more\n",
    "sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime[1:], diff(row.vectime), 'o')\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know enough NumPy to be able to write a function that computes\n",
    "running average (a.k.a. \"mean filter\"). Let's try it out in a plot\n",
    "immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_avg(x):\n",
    "    return np.cumsum(x) / np.arange(1, x.size + 1)\n",
    "\n",
    "# example plot:\n",
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime, running_avg(row.vecvalue))\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain quantities such as queue length or on-off status,\n",
    "weighted average (with time intervals used as weights) makes\n",
    "more sense. Here is a function that computes running time-average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_timeavg(t,x):\n",
    "    dt = t[1:] - t[:-1]\n",
    "    return np.cumsum(x[:-1] * dt) / t[1:]\n",
    "\n",
    "# example plot:\n",
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime[1:], running_timeavg(row.vectime, row.vecvalue))\n",
    "plt.xlim(0,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the integral of the vector as a step function is very similar\n",
    "to the `running_timeavg()` function. (Note: Computing integral in other\n",
    "ways is part of NumPy and SciPy, if you ever need it. For example,\n",
    "`np.trapz(y,x)` computes integral using the trapezoidal rule.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_steps(t,x):\n",
    "    dt = t[1:] - t[:-1]\n",
    "    return np.cumsum(x[:-1] * dt)\n",
    "\n",
    "# example plot:\n",
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime[1:], integrate_steps(row.vectime, row.vecvalue))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last example in this section, here is a function that computes\n",
    "moving window average. It relies on the clever trick of subtracting\n",
    "the cumulative sum of the original vector from its shifted version\n",
    "to get the sum of values in every *N*-sized window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winavg(x, N):\n",
    "    xpad = np.concatenate((np.zeros(N), x)) # pad with zeroes\n",
    "    s = np.cumsum(xpad)\n",
    "    ss = s[N:] - s[:-N]\n",
    "    ss[N-1:] /= N\n",
    "    ss[:N-1] /= np.arange(1, min(N-1,ss.size)+1)\n",
    "    return ss\n",
    "\n",
    "# example:\n",
    "for row in somevectors.itertuples():\n",
    "    plt.plot(row.vectime, winavg(row.vecvalue, 10))\n",
    "plt.xlim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find further hints for smoothing the plot of an output vector\n",
    "in the signal processing chapter of the SciPy Cookbook (see References).\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "\n",
    "The primary and authentic source of information on Pandas, Matplotlib and other\n",
    "libraries is their official documentation. I do not link them here because they\n",
    "are trivial to find via Google. Instead, here is a random collection of other\n",
    "resources that I found useful while writing this tutorial (not counting all the\n",
    "StackOverflow pages I visited.)\n",
    "\n",
    "- Pandas tutorial from Greg Reda:\n",
    "  http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/\n",
    "- On reshaping data frames:\n",
    "  https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping\n",
    "- Matplotlib tutorial of Nicolas P. Rougier:\n",
    "  https://www.labri.fr/perso/nrougier/teaching/matplotlib/\n",
    "- Creating boxplots with Matplotlib, from Bharat Bhole:\n",
    "  http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/\n",
    "- SciPy Cookbook on signal smoothing:\n",
    "  http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html\n",
    "- Visual Guide on Pandas (video):\n",
    "  https://www.youtube.com/watch?v=9d5-Ti6onew\n",
    "- Python Pandas Cookbook (videos):\n",
    "  https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "\n",
    "The author, Andras Varga would like to thank the participants of the\n",
    "2016 OMNeT++ Summit for the valuable feedback, and especially\n",
    "Dr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
